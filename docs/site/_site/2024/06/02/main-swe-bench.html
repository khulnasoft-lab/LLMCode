

<!DOCTYPE html>

<html lang="en-US">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge">

  <link rel="stylesheet" href="/assets/css/just-the-docs-default.css">

  <link rel="stylesheet" href="/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet">

  <style id="jtd-nav-activation">
  
.site-nav ul li a {
  background-image: none;
}

  </style>

  

  
    <script src="/assets/js/vendor/lunr.min.js"></script>
  

  <script src="/assets/js/just-the-docs.js"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1">

  



  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Llmcode is SOTA for both SWE Bench and SWE Bench Lite | llmcode</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Llmcode is SOTA for both SWE Bench and SWE Bench Lite" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Llmcode sets SOTA for the main SWE Bench, after recently setting SOTA for the Lite version." />
<meta property="og:description" content="Llmcode sets SOTA for the main SWE Bench, after recently setting SOTA for the Lite version." />
<link rel="canonical" href="http://0.0.0.0:4000/2024/06/02/main-swe-bench.html" />
<meta property="og:url" content="http://0.0.0.0:4000/2024/06/02/main-swe-bench.html" />
<meta property="og:site_name" content="llmcode" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2024-06-02T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Llmcode is SOTA for both SWE Bench and SWE Bench Lite" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2024-06-02T00:00:00+00:00","datePublished":"2024-06-02T00:00:00+00:00","description":"Llmcode sets SOTA for the main SWE Bench, after recently setting SOTA for the Lite version.","headline":"Llmcode is SOTA for both SWE Bench and SWE Bench Lite","mainEntityOfPage":{"@type":"WebPage","@id":"http://0.0.0.0:4000/2024/06/02/main-swe-bench.html"},"url":"http://0.0.0.0:4000/2024/06/02/main-swe-bench.html"}</script>
<!-- End Jekyll SEO tag -->


  
<meta property="og:image" content="http://0.0.0.0:4000/assets/swe_bench.jpg">
<meta property="twitter:image" content="http://0.0.0.0:4000/assets/swe_bench.jpg">

<link rel="alternate" type="application/rss+xml" title="RSS Feed" href="http://0.0.0.0:4000/feed.xml">
<link rel="preconnect" href="https://fonts.gstatic.com">
<link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style" type="text/css" crossorigin>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="theme-color" content="#157878">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/favicon-16x16.png">
<link rel="apple-touch-icon" sizes="180x180" href="/assets/icons/apple-touch-icon.png">
<link rel="manifest" href="/assets/icons/site.webmanifest">
<link rel="mask-icon" href="/assets/icons/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">




</head>

<body>
  <a class="skip-to-main" href="#main-content">Skip to main content</a>
  <svg xmlns="http://www.w3.org/2000/svg" class="d-none">
  <symbol id="svg-link" viewBox="0 0 24 24">
  <title>Link</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link">
    <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
  </svg>
</symbol>

  <symbol id="svg-menu" viewBox="0 0 24 24">
  <title>Menu</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu">
    <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line>
  </svg>
</symbol>

  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
  <title>Expand</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right">
    <polyline points="9 18 15 12 9 6"></polyline>
  </svg>
</symbol>

  <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE -->
<symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link">
  <title id="svg-external-link-title">(external link)</title>
  <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line>
</symbol>

  
    <symbol id="svg-doc" viewBox="0 0 24 24">
  <title>Document</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file">
    <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline>
  </svg>
</symbol>

    <symbol id="svg-search" viewBox="0 0 24 24">
  <title>Search</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search">
    <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line>
  </svg>
</symbol>

  
  
    <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md -->
<symbol id="svg-copy" viewBox="0 0 16 16">
  <title>Copy</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16">
    <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/>
    <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/>
  </svg>
</symbol>
<symbol id="svg-copied" viewBox="0 0 16 16">
  <title>Copied</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16">
    <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/>
    <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/>
  </svg>
</symbol>

  
</svg>

  <div class="side-bar">
  <div class="site-header" role="banner">
    <a href="/" class="site-title lh-tight">
  llmcode

</a>
    <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false">
      <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg>
    </button>
  </div>

  <nav aria-label="Main" id="site-nav" class="site-nav">
  
  
    <ul class="nav-list"><li class="nav-list-item"><a href="/" class="nav-list-link">Home</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Installation category" aria-pressed="false">
      <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg>
    </button><a href="/docs/install.html" class="nav-list-link">Installation</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/install/optional.html" class="nav-list-link">Optional steps</a></li><li class="nav-list-item"><a href="/docs/install/docker.html" class="nav-list-link">Llmcode with docker</a></li><li class="nav-list-item"><a href="/docs/install/codespaces.html" class="nav-list-link">GitHub Codespaces</a></li><li class="nav-list-item"><a href="/docs/install/replit.html" class="nav-list-link">Replit</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Usage category" aria-pressed="false">
      <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg>
    </button><a href="/docs/usage.html" class="nav-list-link">Usage</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/usage/tips.html" class="nav-list-link">Tips</a></li><li class="nav-list-item"><a href="/docs/usage/commands.html" class="nav-list-link">In-chat commands</a></li><li class="nav-list-item"><a href="/docs/usage/modes.html" class="nav-list-link">Chat modes</a></li><li class="nav-list-item"><a href="/docs/usage/tutorials.html" class="nav-list-link">Tutorial videos</a></li><li class="nav-list-item"><a href="/docs/usage/voice.html" class="nav-list-link">Voice-to-code with llmcode</a></li><li class="nav-list-item"><a href="/docs/usage/images-urls.html" class="nav-list-link">Images &amp; web pages</a></li><li class="nav-list-item"><a href="/docs/usage/caching.html" class="nav-list-link">Prompt caching</a></li><li class="nav-list-item"><a href="/docs/usage/watch.html" class="nav-list-link">Llmcode in your IDE</a></li><li class="nav-list-item"><a href="/docs/usage/browser.html" class="nav-list-link">Llmcode in your browser</a></li><li class="nav-list-item"><a href="/docs/usage/conventions.html" class="nav-list-link">Specifying coding conventions</a></li><li class="nav-list-item"><a href="/docs/usage/copypaste.html" class="nav-list-link">Copy/paste with web chat</a></li><li class="nav-list-item"><a href="/docs/usage/lint-test.html" class="nav-list-link">Linting and testing</a></li><li class="nav-list-item"><a href="/docs/usage/not-code.html" class="nav-list-link">Editing config &amp; text files</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Connecting to LLMs category" aria-pressed="false">
      <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg>
    </button><a href="/docs/llms.html" class="nav-list-link">Connecting to LLMs</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/llms/openai.html" class="nav-list-link">OpenAI</a></li><li class="nav-list-item"><a href="/docs/llms/anthropic.html" class="nav-list-link">Anthropic</a></li><li class="nav-list-item"><a href="/docs/llms/gemini.html" class="nav-list-link">Gemini</a></li><li class="nav-list-item"><a href="/docs/llms/groq.html" class="nav-list-link">GROQ</a></li><li class="nav-list-item"><a href="/docs/llms/lm-studio.html" class="nav-list-link">LM Studio</a></li><li class="nav-list-item"><a href="/docs/llms/xai.html" class="nav-list-link">xAI</a></li><li class="nav-list-item"><a href="/docs/llms/azure.html" class="nav-list-link">Azure</a></li><li class="nav-list-item"><a href="/docs/llms/cohere.html" class="nav-list-link">Cohere</a></li><li class="nav-list-item"><a href="/docs/llms/deepseek.html" class="nav-list-link">DeepSeek</a></li><li class="nav-list-item"><a href="/docs/llms/ollama.html" class="nav-list-link">Ollama</a></li><li class="nav-list-item"><a href="/docs/llms/openai-compat.html" class="nav-list-link">OpenAI compatible APIs</a></li><li class="nav-list-item"><a href="/docs/llms/openrouter.html" class="nav-list-link">OpenRouter</a></li><li class="nav-list-item"><a href="/docs/llms/vertex.html" class="nav-list-link">Vertex AI</a></li><li class="nav-list-item"><a href="/docs/llms/bedrock.html" class="nav-list-link">Amazon Bedrock</a></li><li class="nav-list-item"><a href="/docs/llms/other.html" class="nav-list-link">Other LLMs</a></li><li class="nav-list-item"><a href="/docs/llms/warnings.html" class="nav-list-link">Model warnings</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Configuration category" aria-pressed="false">
      <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg>
    </button><a href="/docs/config.html" class="nav-list-link">Configuration</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/config/api-keys.html" class="nav-list-link">API Keys</a></li><li class="nav-list-item"><a href="/docs/config/options.html" class="nav-list-link">Options reference</a></li><li class="nav-list-item"><a href="/docs/config/llmcode_conf.html" class="nav-list-link">YAML config file</a></li><li class="nav-list-item"><a href="/docs/config/dotenv.html" class="nav-list-link">Config with .env</a></li><li class="nav-list-item"><a href="/docs/config/editor.html" class="nav-list-link">Editor configuration</a></li><li class="nav-list-item"><a href="/docs/config/reasoning.html" class="nav-list-link">Reasoning models</a></li><li class="nav-list-item"><a href="/docs/config/adv-model-settings.html" class="nav-list-link">Advanced model settings</a></li><li class="nav-list-item"><a href="/docs/config/model-aliases.html" class="nav-list-link">Model Aliases</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Troubleshooting category" aria-pressed="false">
      <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg>
    </button><a href="/docs/troubleshooting.html" class="nav-list-link">Troubleshooting</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/troubleshooting/edit-errors.html" class="nav-list-link">File editing problems</a></li><li class="nav-list-item"><a href="/docs/troubleshooting/warnings.html" class="nav-list-link">Model warnings</a></li><li class="nav-list-item"><a href="/docs/troubleshooting/token-limits.html" class="nav-list-link">Token limits</a></li><li class="nav-list-item"><a href="/docs/troubleshooting/imports.html" class="nav-list-link">Dependency versions</a></li><li class="nav-list-item"><a href="/docs/troubleshooting/llmcode-not-found.html" class="nav-list-link">Llmcode not found</a></li><li class="nav-list-item"><a href="/docs/troubleshooting/models-and-keys.html" class="nav-list-link">Models and API keys</a></li><li class="nav-list-item"><a href="/docs/troubleshooting/support.html" class="nav-list-link">Using /help</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Example chat transcripts category" aria-pressed="false">
      <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg>
    </button><a href="/examples/README.html" class="nav-list-link">Example chat transcripts</a><ul class="nav-list"><li class="nav-list-item"><a href="/examples/hello-world-flask.html" class="nav-list-link">Create a simple flask app with llmcode</a></li><li class="nav-list-item"><a href="/examples/2048-game.html" class="nav-list-link">Modify an open source 2048 game with llmcode</a></li><li class="nav-list-item"><a href="/examples/complex-change.html" class="nav-list-link">A complex multi-file change, with debugging</a></li><li class="nav-list-item"><a href="/examples/add-test.html" class="nav-list-link">Create a “black box” test case</a></li><li class="nav-list-item"><a href="/examples/update-docs.html" class="nav-list-link">Automatically update docs with llmcode</a></li><li class="nav-list-item"><a href="/examples/pong.html" class="nav-list-link">Build pong with llmcode and pygame.</a></li><li class="nav-list-item"><a href="/examples/css-exercises.html" class="nav-list-link">Complete a css exercise with llmcode</a></li><li class="nav-list-item"><a href="/examples/census.html" class="nav-list-link">Download, analyze and plot US Census data</a></li><li class="nav-list-item"><a href="/examples/asciinema.html" class="nav-list-link">Editing an asciinema cast file with llmcode</a></li><li class="nav-list-item"><a href="/examples/hello.html" class="nav-list-link">Hello llmcode!</a></li><li class="nav-list-item"><a href="/examples/no-color.html" class="nav-list-link">Honor the NO_COLOR environment variable</a></li><li class="nav-list-item"><a href="/examples/chat-transcript-css.html" class="nav-list-link">Improve css styling of chat transcripts</a></li><li class="nav-list-item"><a href="/examples/semantic-search-replace.html" class="nav-list-link">Semantic search &amp; replace code with llmcode</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in More info category" aria-pressed="false">
      <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg>
    </button><a href="/docs/more-info.html" class="nav-list-link">More info</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/git.html" class="nav-list-link">Git integration</a></li><li class="nav-list-item"><a href="/docs/languages.html" class="nav-list-link">Supported languages</a></li><li class="nav-list-item"><a href="/docs/repomap.html" class="nav-list-link">Repository map</a></li><li class="nav-list-item"><a href="/docs/scripting.html" class="nav-list-link">Scripting llmcode</a></li><li class="nav-list-item"><a href="/docs/more/infinite-output.html" class="nav-list-link">Infinite output</a></li><li class="nav-list-item"><a href="/docs/more/edit-formats.html" class="nav-list-link">Edit formats</a></li><li class="nav-list-item"><a href="/docs/more/analytics.html" class="nav-list-link">Analytics</a></li><li class="nav-list-item"><a href="/docs/legal/privacy.html" class="nav-list-link">Privacy policy</a></li></ul></li><li class="nav-list-item"><a href="/docs/faq.html" class="nav-list-link">FAQ</a></li><li class="nav-list-item"><a href="/HISTORY.html" class="nav-list-link">Release history</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Llmcode LLM Leaderboards category" aria-pressed="false">
      <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg>
    </button><a href="/docs/leaderboards/" class="nav-list-link">Llmcode LLM Leaderboards</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/leaderboards/edit.html" class="nav-list-link">Code editing leaderboard</a></li><li class="nav-list-item"><a href="/docs/leaderboards/refactor.html" class="nav-list-link">Refactoring leaderboard</a></li><li class="nav-list-item"><a href="/docs/leaderboards/by-release-date.html" class="nav-list-link">Scores by release date</a></li><li class="nav-list-item"><a href="/docs/leaderboards/notes.html" class="nav-list-link">Benchmark notes</a></li><li class="nav-list-item"><a href="/docs/leaderboards/contrib.html" class="nav-list-link">Contributing results</a></li></ul></li><li class="nav-list-item"><a href="/blog/" class="nav-list-link">Llmcode blog</a></li></ul>

  <ul class="nav-list"><li class="nav-list-item external">
          <a href="https://github.com/KhulnaSoft/llmcode" class="nav-list-link external"
            
          >
            GitHub
            <svg viewBox="0 0 24 24" aria-labelledby="svg-external-link-title"><use xlink:href="#svg-external-link"></use></svg>
          </a>
        </li><li class="nav-list-item external">
          <a href="https://discord.gg/Tv2uQnR88V" class="nav-list-link external"
            
          >
            Discord
            <svg viewBox="0 0 24 24" aria-labelledby="svg-external-link-title"><use xlink:href="#svg-external-link"></use></svg>
          </a>
        </li></ul>
</nav>


  
  
    <footer class="site-footer">
    Llmcode is AI pair programming in your terminal.
    Llmcode is on
    <a href="https://github.com/KhulnaSoft/llmcode">GitHub</a>
    and
    <a href="https://discord.gg/Tv2uQnR88V">Discord</a>.
</footer>

  
</div>

  <div class="main" id="top">
    <div id="main-header" class="main-header">
  
    

<div class="search" role="search">
  <div class="search-input-wrap">
    <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search llmcode" aria-label="Search llmcode" autocomplete="off">
    <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label>
  </div>
  <div id="search-results" class="search-results"></div>
</div>

  
  
  
    <nav aria-label="Auxiliary" class="aux-nav">
  <ul class="aux-nav-list">
    
      <li class="aux-nav-list-item">
        <a href="https://github.com/KhulnaSoft/llmcode" class="site-button"
          
        >
          GitHub
        </a>
      </li>
    
      <li class="aux-nav-list-item">
        <a href="https://discord.gg/Tv2uQnR88V" class="site-button"
          
        >
          Discord
        </a>
      </li>
    
      <li class="aux-nav-list-item">
        <a href="/blog/" class="site-button"
          
        >
          Blog
        </a>
      </li>
    
  </ul>
</nav>

  
</div>

    <div class="main-content-wrap">
      
      <div id="main-content" class="main-content">
        <main>
          
            <p class="post-date">June 02, 2024</p>
<h1 id="llmcode-is-sota-for-both-swe-bench-and-swe-bench-lite">
  
  
    <a href="#llmcode-is-sota-for-both-swe-bench-and-swe-bench-lite" class="anchor-heading" aria-labelledby="llmcode-is-sota-for-both-swe-bench-and-swe-bench-lite"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode is SOTA for both SWE Bench and SWE Bench Lite
  
  
</h1>
    

<p>Llmcode scored 18.9%
on the main
<a href="https://www.swebench.com">SWE Bench benchmark</a>,
achieving a state-of-the-art result. 
The current top leaderboard entry is 13.8%
from Amazon Q Developer Agent.
The best result reported elsewhere seems to be
<a href="https://www.cognition.ai/post/swe-bench-technical-report">13.9% from Devin</a>.</p>

<p>This result on the main SWE Bench builds on
<a href="https://llmcode.khulnasoft.com/2024/05/22/swe-bench-lite.html">llmcode’s recent SOTA result on the easier SWE Bench Lite</a>.</p>

<p><a href="https://llmcode.khulnasoft.com/assets/swe_bench.svg"><img src="/assets/swe_bench.svg" alt="SWE Bench results" /></a></p>

<p><strong>All of llmcode’s results reported here are pass@1 results,
obtained without using the SWE Bench <code class="language-plaintext highlighter-rouge">hints_text</code>.</strong>
Llmcode was benchmarked on the same
<a href="https://github.com/CognitionAI/devin-swebench-results/tree/main/output_diffs">570 randomly selected SWE Bench problems</a>
that were used in the
<a href="https://www.cognition.ai/post/swe-bench-technical-report">Devin evaluation</a>.
See the <a href="#references">references</a>
for more details on the data presented in this chart.</p>
<h2 id="interactive-not-agentic">
  
  
    <a href="#interactive-not-agentic" class="anchor-heading" aria-labelledby="interactive-not-agentic"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Interactive, not agentic
  
  
</h2>
    

<p>Llmcode achieved this result mainly through its existing features that focus on static
code analysis, reliable LLM code editing, and pragmatic UX for automatically
fixing linting and testing errors.
Llmcode intentionally has quite limited and narrow “agentic behavior”
to avoid long delays, high token costs
and the need for users to repeatedly code review incorrect solutions.
It’s also worth noting that llmcode currently does not use
RAG, vector search, tools or give the LLM access to search the web
or unilaterally execute code.</p>

<p>Llmcode is first and foremost an interactive tool for engineers to get real work done in
real code bases using a chat interface.
Llmcode provides a pair programming UX where users can ask for a change 
and see code edits performed in real-time.
Llmcode can also offer additional help like fixing lint or test errors,
but the user is always in full interactive control.
This allows them to quickly steer misunderstandings back on course and
avoid wasting time and token costs.</p>
<h2 id="benchmark-methodology">
  
  
    <a href="#benchmark-methodology" class="anchor-heading" aria-labelledby="benchmark-methodology"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Benchmark methodology
  
  
</h2>
    

<p>Benchmarking was conducted as follows:</p>

<ul>
  <li>Llmcode with GPT-4o was launched in each problem’s git repository
with the problem statement
submitted as the opening chat message from “the user”.</li>
  <li>After that llmcode ran as normal, except all of llmcode’s
suggestions were always accepted without user approval.</li>
  <li>A <a href="https://github.com/KhulnaSoft/llmcode-swe-bench#the-llmcode-agent">simple harness</a> was used to retry the SWE Bench problem if llmcode produced code that wasn’t <em>plausibly correct</em>.
Plausibly correct means that llmcode reported that it had successfully edited the repo
without causing syntax errors or breaking any <em>pre-existing</em> tests.</li>
  <li>If the solution from llmcode with GPT-4o wasn’t plausible, the harness launched llmcode to try again from scratch using Claude 3 Opus.</li>
  <li>If no plausible solution was found after those two tries, the harness picked the “most plausible” solution with the fewest edit/lint/test problems.</li>
</ul>

<p>It’s important to be clear that
<em>llmcode and the benchmark harness
only had access to the pre-existing tests in each problem’s repo</em>.
The held out “acceptance tests” were <em>only</em> used
after benchmarking to compute statistics on which problems llmcode
correctly resolved.</p>

<p>This is the same approach
that was used for
<a href="https://llmcode.khulnasoft.com/2024/05/22/swe-bench-lite.html">llmcode’s recent SOTA result on SWE Bench Lite</a>.
For the Lite benchmark,
llmcode alternated between GPT-4o and Opus for up to six total attempts.
To manage the cost of running the main SWE Bench benchmark,
llmcode was limited to two total attempts:
one with GPT-4o and one with Opus.</p>

<p>For a detailed discussion of the benchmark
methodology, see the
<a href="https://llmcode.khulnasoft.com/2024/05/22/swe-bench-lite.html">article about llmcode’s SWE Bench Lite results</a>.
Also, the
<a href="https://github.com/KhulnaSoft/llmcode-swe-bench">llmcode SWE Bench repository on GitHub</a>
contains the harness and statistics code used for the benchmarks.</p>

<p>The benchmarking process was similar to how a developer might use llmcode to
resolve a GitHub issue:</p>

<ul>
  <li>They could launch llmcode in their repo with the command below, which
tells llmcode they want to accept every suggestion
and to use pytest to run tests.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">llmcode --yes --test-cmd pytest</code></li>
    </ul>
  </li>
  <li>They could start the chat by pasting in the URL or text of a GitHub issue.
Llmcode will pull in the URL’s content and then try and resolve the issue.</li>
  <li>If llmcode doesn’t produce code that lints and tests clean, the user might decide to
<a href="https://llmcode.khulnasoft.com/docs/git.html">use git to revert the changes</a>,
and try again with <code class="language-plaintext highlighter-rouge">llmcode --opus</code>.</li>
</ul>
<h2 id="llmcode-with-gpt-4o-alone-was-sota">
  
  
    <a href="#llmcode-with-gpt-4o-alone-was-sota" class="anchor-heading" aria-labelledby="llmcode-with-gpt-4o-alone-was-sota"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode with GPT-4o alone was SOTA
  
  
</h2>
    

<p>Using llmcode with GPT-4o to make a single attempt at resolving each problem
achieved a score of 17.0%.
This was itself a state-of-the-art result, before being surpassed by the main
result being reported here
that used llmcode with both GPT-4o &amp; Opus.</p>
<h2 id="llmcode-with-gpt-4o--opus">
  
  
    <a href="#llmcode-with-gpt-4o--opus" class="anchor-heading" aria-labelledby="llmcode-with-gpt-4o--opus"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode with GPT-4o &amp; Opus
  
  
</h2>
    

<p>The benchmark harness started by using llmcode with GPT-4o to try
and resolve each problem.
For problems where this didn’t produce a plausible solution,
the harness tried again using llmcode with Opus.
So at most, two attempts were made for each problem.</p>

<p>The table below breaks down the proposed solutions that
were found from each attempt at the 570 problems.
A proposed solution is either:</p>

<ul>
  <li>A plausible solution where
llmcode reported no outstanding errors from editing, linting and testing.</li>
  <li>Or, the “most plausible” solution generated by either attempt, with the
<a href="https://llmcode.khulnasoft.com/2024/05/22/swe-bench-lite.html#finding-a-plausible-solution">fewest outstanding editing, linting or testing errors</a>.</li>
</ul>

<p>The table also provides details on the 108 solutions that were ultimately
verified as correctly resolving their issue.</p>

<div class="table-wrapper"><table>
  <thead>
    <tr>
      <th style="text-align: center">Attempt</th>
      <th>Agent</th>
      <th style="text-align: right">Number of<br />proposed<br />solutions</th>
      <th style="text-align: right">Percent of<br />proposed<br />solutions</th>
      <th style="text-align: right">Number of<br />correctly<br />resolved<br />solutions</th>
      <th style="text-align: right">Percent of<br />correctly<br />resolved<br />solutions</th>
      <th style="text-align: right">Score on<br />SWE Bench<br />Lite</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">1</td>
      <td>Llmcode with GPT-4o</td>
      <td style="text-align: right">419</td>
      <td style="text-align: right">73.5%</td>
      <td style="text-align: right">87</td>
      <td style="text-align: right">80.6%</td>
      <td style="text-align: right">15.3%</td>
    </tr>
    <tr>
      <td style="text-align: center">2</td>
      <td>Llmcode with Opus</td>
      <td style="text-align: right">151</td>
      <td style="text-align: right">26.5%</td>
      <td style="text-align: right">21</td>
      <td style="text-align: right">19.4%</td>
      <td style="text-align: right">3.7%</td>
    </tr>
    <tr>
      <td style="text-align: center"><strong>Total</strong></td>
      <td> </td>
      <td style="text-align: right"><strong>570</strong></td>
      <td style="text-align: right"><strong>100%</strong></td>
      <td style="text-align: right"><strong>108</strong></td>
      <td style="text-align: right"><strong>100%</strong></td>
      <td style="text-align: right"><strong>18.9%</strong></td>
    </tr>
  </tbody>
</table></div>
<h2 id="non-plausible-but-correct-solutions">
  
  
    <a href="#non-plausible-but-correct-solutions" class="anchor-heading" aria-labelledby="non-plausible-but-correct-solutions"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Non-plausible but correct solutions?
  
  
</h2>
    

<p>A solution doesn’t actually have to be plausible in order to correctly resolve the issue.
Recall that plausible is simply defined as llmcode
reporting that it successfully completed all file edits,
repaired and resolved any linting errors
and resolved any test failures.
But there are many reasons why llmcode might fail to do those things
and yet still produce a solution that will pass
acceptance testing:</p>

<ul>
  <li>There may have been pre-existing failing tests in the repo,
before llmcode even started working on the SWE Bench problem.
Llmcode may not have resolved such issues, and yet they may not be
relevant to the acceptance testing.
The SWE Bench acceptance testing just confirms that tests pass or fail
in the same pattern as the “gold patch” developed by a human to resolve the
problem.
Some tests may fail during acceptance testing,
and that’s ok as long as they failed for the gold
patch too.</li>
  <li>There may have been pre-existing linting problems in the repo.
If lingering linting issues affected code paths that are not well tested,
they may not impact acceptance testing.</li>
  <li>Llmcode may have reported file editing errors because it thought the LLM
specified edits that it wasn’t able to successfully apply.
This can only happen when the LLM specified edits in
a way that doesn’t comply with the editing instructions in the system prompt.
Given that the LLM isn’t complying with the system prompt,
it may have become confused and
asked for redundant or otherwise irrelevant edits.
Such outstanding edit errors might not be fatal for acceptance testing.</li>
  <li>Etc.</li>
</ul>

<p>Keeping all this in mind, we can understand why
GPT-4o accounts for 15.3% of the benchmark score in the table above,
but benchmarking with just one attempt of llmcode with GPT-4o scored 17.0%.
When an Opus attempt is allowed after GPT-4o,
it may propose some <em>incorrect</em> solutions which
are “more plausible” than some of GPT-4o’s non-plausible solutions.
These more plausible, incorrect solutions can
eclipse some of
the earlier non-plausible correct solutions that GPT-4o generated.
This is why GPT-4o’s score in the table 
showing the combined GPT-4o &amp; Opus results (15.3%)
is lower than the result from just one try using llmcode with GPT-4o (17.0%).</p>

<p>For these reasons, adding additional attempts is not guaranteed to monotonically
increase the number of resolved problems.
New solutions may resolve some new problems but they may also
eclipse and discard some of the previous non-plausible correct solutions.</p>

<p>Luckily, the net effect of additional attempts
usually increases or at least maintains the
number of resolved solutions.
This was the case for all the attempts made in both this main SWE Bench result and the
earlier Lite result.</p>
<h2 id="computing-the-benchmark-score">
  
  
    <a href="#computing-the-benchmark-score" class="anchor-heading" aria-labelledby="computing-the-benchmark-score"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Computing the benchmark score
  
  
</h2>
    

<p>The benchmark harness produced one proposed solution for each of
the 570 SWE Bench problems.</p>

<p>A separate evaluation script was used to
test each of these solutions with the full test suite,
including the held out acceptance tests.
For this final acceptance testing, any edits that llmcode made to tests
were discarded.
This ensured that the correct,
unmodified test suite was used for acceptance testing.
The evaluation script compared each proposed solution’s test results
with results from testing
the “gold” patch that was developed by a human to correctly resolve the issue.
If they matched, the proposed solution correctly resolved the issue.</p>

<p>These acceptance tests were only ever run outside of llmcode
and the benchmark harness, and only to compute statistics about the
correctly resolved instances.
They were never run, used, or even visible during llmcode’s attempts to resolve the problems.</p>

<p>Llmcode correctly resolved 108 out of 570 SWE Bench instances that were benchmarked,
or 18.9%.</p>
<h2 id="acknowledgments">
  
  
    <a href="#acknowledgments" class="anchor-heading" aria-labelledby="acknowledgments"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Acknowledgments
  
  
</h2>
    

<p>Much thanks to the team behind the
<a href="https://www.swebench.com">SWE Bench</a>
family of AI coding benchmarks.
Also thanks to Albert Örwall who has
<a href="https://github.com/aorwall/SWE-bench-docker">dockerized the SWE Bench evaluation scripts</a>
making it faster, easier, and more reliable to run the acceptance tests.</p>
<h2 id="references">
  
  
    <a href="#references" class="anchor-heading" aria-labelledby="references"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> References
  
  
</h2>
    

<p>All of llmcode’s results reported here are pass@1 results,
obtained without using the SWE Bench <code class="language-plaintext highlighter-rouge">hints_text</code>.</p>

<p>The “llmcode agent” internally makes multiple “attempts” at solving the problem,
but it picks and returns one single candidate solution.
Only that one candidate solution is evaluated with the acceptance tests
and contributes to the benchmark score.
Thus it is a pass@1 result.</p>

<p>This is contrast to a pass@N result for N&gt;1, where N attempts are made
and all N solutions are evaluated by the acceptance tests.
If <em>any</em> of the N solution pass, that counts as a pass@N success.</p>

<p>Below are the references for the other pass@1 unhinted SWE-Bench results
displayed in the graph at the beginning of this article.</p>

<ul>
  <li><a href="https://www.cognition.ai/post/swe-bench-technical-report">13.9% Devin, benchmarked on 570 instances.</a></li>
  <li><a href="https://www.swebench.com">13.8% Amazon Q Developer Agent, benchmarked on 2,294 instances.</a></li>
  <li><a href="https://www.swebench.com">12.5% SWE- Agent + GPT-4, benchmarked on 2,294 instances.</a></li>
  <li><a href="https://arxiv.org/pdf/2404.05427v2">10.6% AutoCode Rover, benchmarked on 2,294 instances.</a></li>
  <li><a href="https://www.swebench.com">10.5% SWE- Agent + Opus, benchmarked on 2,294 instances.</a></li>
</ul>

<p>The graph contains average pass@1 results for AutoCodeRover.
The <a href="https://github.com/nus-apr/auto-code-rover">AutoCodeRover GitHub page</a>
features their pass@3 results
without being clearly labeled.
Table 2 of their
<a href="https://arxiv.org/pdf/2404.05427v2">paper</a>
reports an <code class="language-plaintext highlighter-rouge">ACR-avg</code> result of 10.59% which is an average pass@1 result.</p>

          

          
        </main>
        


      </div>
    </div>
    
      

<div class="search-overlay"></div>

    
  </div>

  
</body>
</html>

