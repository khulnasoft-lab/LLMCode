{"0": {
    "doc": "Modify an open source 2048 game with llmcode",
    "title": "Modify an open source 2048 game with llmcode",
    "content": "In this chat transcript, the user explores and modifies an open source javascript 2048 game repo. Notice that the LLM asks to see a particular source file, and llmcode automatically includes it in the chat session after getting permission from the user. $ git clone https://github.com/gabrielecirulli/2048 $ cd 2048 $ llmcode . what is this repo? . Based on the file names you provided, it seems like this repository is for a web-based game, possibly a 2048 game clone. The files include JavaScript files for game logic, HTML and CSS files for the user interface, and various meta files for icons and touch startup images. If you need help with any specific part of the code or want to add new features, please let me know, and I’ll be happy to assist you. how does the scoring work? . To help you with the scoring mechanism, I need to see the relevant code. Please provide the contents of the js/game_manager.js file, as it is likely to contain the scoring logic. js/game_manager.js Add these files to the chat? y . Apologies for the oversight. I see the contents of the js/game_manager.js file now. The scoring in this 2048 game works as follows: . | When two tiles with the same value merge, they create a new tile with a value equal to the sum of the merged tiles’ values. | The score is updated by adding the value of the new merged tile. | . Here’s the relevant code snippet from the js/game_manager.js file: . if (next &amp;&amp; next.value === tile.value &amp;&amp; !next.mergedFrom) { var merged = new Tile(positions.next, tile.value * 2); merged.mergedFrom = [tile, next]; self.grid.insertTile(merged); self.grid.removeTile(tile); // Converge the two tiles' positions tile.updatePosition(positions.next); // Update the score self.score += merged.value; // The mighty 2048 tile if (merged.value === 2048) self.won = true; } . In this code snippet, when two tiles with the same value (tile and next) are found, they are merged into a new tile (merged). The value of the new tile is added to the current score (self.score += merged.value). If the merged tile’s value is 2048, the game is considered won (self.won = true). can we add some randomness? maybe 10% of the time give the user a 10x bonus? . Sure, we can add a 10% chance of giving the user a 10x bonus when two tiles merge. To implement this, we’ll modify the scoring logic in the js/game_manager.js file. Here’s the step-by-step plan: . | Generate a random number between 0 and 1. | Check if the random number is less than or equal to 0.1 (10% chance). | If the condition is met, multiply the merged tile’s value by 10 before adding it to the score. | . Here’s the code change for the js/game_manager.js file: . js/game_manager.js &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL // Update the score self.score += merged.value; ======= // Update the score with a 10% chance of 10x bonus var bonus = Math.random() &lt;= 0.1 ? 10 : 1; self.score += merged.value * bonus; &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . This change will give the user a 10x bonus 10% of the time when two tiles merge. Applied edit to js/game_manager.js Commit eb9d882 llmcode: Added a 10% chance of giving the user a 10x bonus when two tiles merge. ",
    "url": "/examples/2048-game.html",
    
    "relUrl": "/examples/2048-game.html"
  },"1": {
    "doc": "Release history",
    "title": "Release history",
    "content": "The above stats are based on the git commit history of the llmcode repo. ",
    "url": "/HISTORY.html",
    
    "relUrl": "/HISTORY.html"
  },"2": {
    "doc": "Release history",
    "title": "Release notes",
    "content": "main branch . | Added support for Claude 3.7 Sonnet models on OpenRouter, Bedrock and Vertex AI. | Llmcode wrote 47% of the code in this release. | . Llmcode v0.75.1 . | Added support for openrouter/anthropic/claude-3.7-sonnet | . Llmcode v0.75.0 . | Basic support for Claude 3.7 Sonnet . | Use --model sonnet to use the new 3.7 | Thinking support coming soon. | . | Bugfix to /editor command. | Llmcode wrote 46% of the code in this release. | . Llmcode v0.74.3 . | Downgrade streamlit dependency to avoid threading bug. | Added support for tree-sitter language pack. | Added openrouter/o3-mini-high model configuration. | Added build.gradle.kts to special files for Kotlin project support, by Lucas Shadler. | . Llmcode v0.74.2 . | Prevent more than one cache warming thread from becoming active. | Fixed continuation prompt “. “ for multiline input. | Added HCL (Terraform) syntax support, by Warren Krewenki. | . Llmcode v0.74.1 . | Have o1 &amp; o3-mini generate markdown by sending the magic “Formatting re-enabled.” string. | Bugfix for multi-line inputs, which should not include the “. “ continuation prompt. | . Llmcode v0.74.0 . | Dynamically changes the Ollama context window to hold the current chat. | Better support for o3-mini, DeepSeek V3 &amp; R1, o1-mini, o1 especially via third-party API providers. | Remove &lt;think&gt; tags from R1 responses for commit messages (and other weak model uses). | Can now specify use_temperature: &lt;float&gt; in model settings, not just true/false. | The full docker container now includes boto3 for Bedrock. | Docker containers now set HOME=/app which is the normal project mount-point, to persist ~/.llmcode. | Bugfix to prevent creating incorrect filenames like python, php, etc. | Bugfix for --timeout | Bugfix so that /model now correctly reports that the weak model is not changed. | Bugfix so that multi-line mode persists through ^C at confirmation prompts. | Watch files now fully ignores top-level directories named in ignore files, to reduce the chance of hitting OS watch limits. Helpful to ignore giant subtrees like node_modules. | Fast startup with more providers and when model metadata provided in local files. | Improved .gitignore handling: . | Honor ignores already in effect regardless of how they’ve been configured. | Check for .env only when the file exists. | . | Yes/No prompts now accept All/Skip as alias for Y/N even when not processing a group of confirmations. | Llmcode wrote 77% of the code in this release. | . Llmcode v0.73.0 . | Full support for o3-mini: llmcode --model o3-mini | New --reasoning-effort argument: low, medium, high. | Improved handling of context window size limits, with better messaging and Ollama-specific guidance. | Added support for removing model-specific reasoning tags from responses with remove_reasoning: tagname model setting. | Auto-create parent directories when creating new files, by xqyz. | Support for R1 free on OpenRouter: --model openrouter/deepseek/deepseek-r1:free | Llmcode wrote 69% of the code in this release. | . Llmcode v0.72.3 . | Enforce user/assistant turn order to avoid R1 errors, by miradnanali. | Case-insensitive model name matching while preserving original case. | . Llmcode v0.72.2 . | Harden against user/assistant turn order problems which cause R1 errors. | . Llmcode v0.72.1 . | Fix model metadata for openrouter/deepseek/deepseek-r1 | . Llmcode v0.72.0 . | Support for DeepSeek R1. | Use shortcut: --model r1 | Also via OpenRouter: --model openrouter/deepseek/deepseek-r1 | . | Added Kotlin syntax support to repo map, by Paul Walker. | Added --line-endings for file writing, by Titusz Pan. | Added examples_as_sys_msg=True for GPT-4o models, improves benchmark scores. | Bumped all dependencies, to pick up litellm support for o1 system messages. | Bugfix for turn taking when reflecting lint/test errors. | Llmcode wrote 52% of the code in this release. | . Llmcode v0.71.1 . | Fix permissions issue in Docker images. | Added read-only file announcements. | Bugfix: ASCII fallback for unicode errors. | Bugfix: integer indices for list slicing in repomap calculations. | . Llmcode v0.71.0 . | Prompts to help DeepSeek work better when alternating between /ask and /code. | Streaming pretty LLM responses is smoother and faster for long replies. | Streaming automatically turns of for model that don’t support it . | Can now switch to/from /model o1 and a streaming model | . | Pretty output remains enabled even when editing files with triple-backtick fences | Bare /ask, /code and /architect commands now switch the chat mode. | Increased default size of the repomap. | Increased max chat history tokens limit from 4k to 8k. | Turn off fancy input and watch files if terminal is dumb. | Added support for custom voice format and input device settings. | Disabled Streamlit email prompt, by apaz-cli. | Docker container runs as non-root user. | Fixed lint command handling of nested spaced strings, by Aaron Weisberg. | Added token count feedback when adding command output to chat. | Improved error handling for large audio files with automatic format conversion. | Improved handling of git repo index errors, by Krazer. | Improved unicode handling in console output with ASCII fallback. | Added AssertionError, AttributeError to git error handling. | Llmcode wrote 60% of the code in this release. | . Llmcode v0.70.0 . | Full support for o1 models. | Watch files now honors --subtree-only, and only watches that subtree. | Improved prompting for watch files, to work more reliably with more models. | New install methods via uv, including one-liners. | Support for openrouter/deepseek/deepseek-chat model. | Better error handling when interactive commands are attempted via /load or --load. | Display read-only files with abs path if its shorter than rel path. | Ask 10% of users to opt-in to analytics. | Bugfix for auto-suggest. | Gracefully handle unicode errors in git path names. | Llmcode wrote 74% of the code in this release. | . Llmcode v0.69.1 . | Fix for gemini model names in model metadata. | Show hints about AI! and AI? when user makes AI comments. | Support for running without git installed. | Improved environment variable setup messages on Windows. | . Llmcode v0.69.0 . | Watch files improvements: . | Use # ... AI? comments to trigger llmcode and ask questions about your code. | Now watches all files, not just certain source files. | Use # AI comments, // AI comments, or -- AI comments to give llmcode instructions in any text file. | . | Full support for Gemini Flash 2.0 Exp: . | llmcode --model flash or llmcode --model gemini/gemini-2.0-flash-exp | . | New --multiline flag and /multiline-mode command makes ENTER a soft newline and META-ENTER send the message, by @miradnanali. | /copy-context &lt;instructions&gt; now takes optional “instructions” when copying code context to the clipboard. | Improved clipboard error handling with helpful requirements install info. | Ask 5% of users if they want to opt-in to analytics. | /voice now lets you edit the transcribed text before sending. | Disabled auto-complete in Y/N prompts. | Llmcode wrote 68% of the code in this release. | . Llmcode v0.68.0 . | Llmcode works with LLM web chat UIs. | New --copy-paste mode. | New /copy-context command. | . | Set API keys and other environment variables for all providers from command line or yaml conf file. | New --api-key provider=key setting. | New --set-env VAR=value setting. | . | Added bash and zsh support to --watch-files. | Better error messages when missing dependencies for Gemini and Bedrock models. | Control-D now properly exits the program. | Don’t count token costs when API provider returns a hard error. | Bugfix so watch files works with files that don’t have tree-sitter support. | Bugfix so o1 models can be used as weak model. | Updated shell command prompt. | Added docstrings for all Coders. | Reorganized command line arguments with improved help messages and grouping. | Use the exact sys.python for self-upgrades. | Added experimental Gemini models. | Llmcode wrote 71% of the code in this release. | . Llmcode v0.67.0 . | Use llmcode in your IDE or editor. | Run llmcode --watch-files and it will watch for instructions you add to your source files. | One-liner # ... or // ... comments that start or end with “AI” are instructions to llmcode. | When llmcode sees “AI!” it reads and follows all the instructions in AI comments. | . | Support for new Amazon Bedrock Nova models. | When /run or /test have non-zero exit codes, pre-fill “Fix that” into the next message prompt. | /diff now invokes git diff to use your preferred diff tool. | Added Ctrl-Z support for process suspension. | Spinner now falls back to ASCII art if fancy symbols throw unicode errors. | --read now expands ~ home dirs. | Enabled exception capture in analytics. | Llmcode wrote 61% of the code in this release. | . Llmcode v0.66.0 . | PDF support for Sonnet and Gemini models. | Added --voice-input-device to select audio input device for voice recording, by @preynal. | Added --timeout option to configure API call timeouts. | Set cwd to repo root when running shell commands. | Added Ctrl-Up/Down keyboard shortcuts for per-message history navigation. | Improved error handling for failed .gitignore file operations. | Improved error handling for input history file permissions. | Improved error handling for analytics file access. | Removed spurious warning about disabling pretty in VSCode. | Removed broken support for Dart. | Bugfix when scraping URLs found in chat messages. | Better handling of version import errors. | Improved /drop command to support substring matching for non-glob patterns. | Llmcode wrote 82% of the code in this release. | . Llmcode v0.65.1 . | Bugfix to --alias. | . Llmcode v0.65.0 . | Added --alias config to define custom model aliases. | Added --[no-]detect-urls flag to disable detecting and offering to scrape URLs found in the chat. | Ollama models now default to an 8k context window. | Added RepoMap support for Dart language by @malkoG. | Ask 2.5% of users if they want to opt-in to analytics. | Skip suggesting files that share names with files already in chat. | /editor returns and prefill the file content into the prompt, so you can use /editor to compose messages that start with /commands, etc. | Enhanced error handling for analytics. | Improved handling of UnknownEditFormat exceptions with helpful documentation links. | Bumped dependencies to pick up grep-ast 0.4.0 for Dart language support. | Llmcode wrote 81% of the code in this release. | . Llmcode v0.64.1 . | Disable streaming for o1 on OpenRouter. | . Llmcode v0.64.0 . | Added /editor command to open system editor for writing prompts, by @thehunmonkgroup. | Full support for gpt-4o-2024-11-20. | Stream o1 models by default. | /run and suggested shell commands are less mysterious and now confirm that they “Added XX lines of output to the chat.” | Ask 1% of users if they want to opt-in to analytics. | Added support for optional multiline input tags with matching closing tags. | Improved model settings configuration with support for global extra_params for litellm.completion(). | Architect mode now asks to add files suggested by the LLM. | Fixed bug in fuzzy model name matching. | Added Timeout exception to handle API provider timeouts. | Added --show-release-notes to control release notes display on first run of new version. | Save empty dict to cache file on model metadata download failure, to delay retry. | Improved error handling and code formatting. | Llmcode wrote 74% of the code in this release. | . Llmcode v0.63.2 . | Fixed bug in fuzzy model name matching when litellm provider info is missing. | Modified model metadata file loading to allow override of resource file. | Allow recursive loading of dirs using --read. | Updated dependency versions to pick up litellm fix for ollama models. | Added exponential backoff retry when writing files to handle editor file locks. | Updated Qwen 2.5 Coder 32B model configuration. | . Llmcode v0.63.1 . | Fixed bug in git ignored file handling. | Improved error handling for git operations. | . Llmcode v0.63.0 . | Support for Qwen 2.5 Coder 32B. | /web command just adds the page to the chat, without triggering an LLM response. | Improved prompting for the user’s preferred chat language. | Improved handling of LiteLLM exceptions. | Bugfix for double-counting tokens when reporting cache stats. | Bugfix for the LLM creating new files. | Other small bug fixes. | Llmcode wrote 55% of the code in this release. | . Llmcode v0.62.0 . | Full support for Claude 3.5 Haiku . | Scored 75% on llmcode’s code editing leaderboard. | Almost as good as Sonnet at much lower cost. | Launch with --haiku to use it. | . | Easily apply file edits from ChatGPT, Claude or other web apps . | Chat with ChatGPT or Claude via their web app. | Give it your source files and ask for the changes you want. | Use the web app’s “copy response” button to copy the entire reply from the LLM. | Run llmcode --apply-clipboard-edits file-to-edit.js. | Llmcode will edit your file with the LLM’s changes. | . | Bugfix for creating new files. | Llmcode wrote 84% of the code in this release. | . Llmcode v0.61.0 . | Load and save llmcode slash-commands to files: . | /save &lt;fname&gt; command will make a file of /add and /read-only commands that recreate the current file context in the chat. | /load &lt;fname&gt; will replay the commands in the file. | You can use /load to run any arbitrary set of slash-commands, not just /add and /read-only. | Use --load &lt;fname&gt; to run a list of commands on launch, before the interactive chat begins. | . | Anonymous, opt-in analytics with no personal data sharing. | Llmcode follows litellm’s supports_vision attribute to enable image support for models. | Bugfix for when diff mode flexibly handles the model using the wrong filename. | Displays filenames in sorted order for /add and /read-only. | New --no-fancy-input switch disables prompt toolkit input, now still available with --no-pretty. | Override browser config with --no-browser or --no-gui. | Offer to open documentation URLs when errors occur. | Properly support all o1 models, regardless of provider. | Improved layout of filenames above input prompt. | Better handle corrupted repomap tags cache. | Improved handling of API errors, especially when accessing the weak model. | Llmcode wrote 68% of the code in this release. | . Llmcode v0.60.1 . | Enable image support for Sonnet 10/22. | Display filenames in sorted order. | . Llmcode v0.60.0 . | Full support for Sonnet 10/22, the new SOTA model on llmcode’s code editing benchmark. | Llmcode uses Sonnet 10/22 by default. | . | Improved formatting of added and read-only files above chat prompt, by @jbellis. | Improved support for o1 models by more flexibly parsing their nonconforming code edit replies. | Corrected diff edit format prompt that only the first match is replaced. | Stronger whole edit format prompt asking for clean file names. | Now offers to add .env to the .gitignore file. | Ships with a small model metadata json file to handle models not yet updated in litellm. | Model settings for o1 models on azure. | Bugfix to properly include URLs in /help RAG results. | Llmcode wrote 49% of the code in this release. | . Llmcode v0.59.1 . | Check for obsolete yes: true in yaml config, show helpful error. | Model settings for openrouter/anthropic/claude-3.5-sonnet:beta | . Llmcode v0.59.0 . | Improvements to /read-only: . | Now supports shell-style auto-complete of the full file system. | Still auto-completes the full paths of the repo files like /add. | Now supports globs like src/**/*.py | . | Renamed --yes to --yes-always. | Now uses LLMCODE_YES_ALWAYS env var and yes-always: yaml key. | Existing YAML and .env files will need to be updated. | Can still abbreviate to --yes on the command line. | . | Config file now uses standard YAML list syntax with ` - list entries`, one per line. | /settings now includes the same announcement lines that would print at launch. | Sanity checks the --editor-model on launch now, same as main and weak models. | Added --skip-sanity-check-repo switch to speedup launch in large repos. | Bugfix so architect mode handles Control-C properly. | Repo-map is deterministic now, with improved caching logic. | Improved commit message prompt. | Llmcode wrote 77% of the code in this release. | . Llmcode v0.58.1 . | Fixed bug where cache warming pings caused subsequent user messages to trigger a tight loop of LLM requests. | . Llmcode v0.58.0 . | Use a pair of Architect/Editor models for improved coding . | Use a strong reasoning model like o1-preview as your Architect. | Use a cheaper, faster model like gpt-4o as your Editor. | . | New --o1-preview and --o1-mini shortcuts. | Support for new Gemini 002 models. | Better support for Qwen 2.5 models. | Many confirmation questions can be skipped for the rest of the session with “(D)on’t ask again” response. | Autocomplete for /read-only supports the entire filesystem. | New settings for completion menu colors. | New /copy command to copy the last LLM response to the clipboard. | Renamed /clipboard to /paste. | Will now follow HTTP redirects when scraping urls. | New --voice-format switch to send voice audio as wav/mp3/webm, by @mbailey. | ModelSettings takes extra_params dict to specify any extras to pass to litellm.completion(). | Support for cursor shapes when in vim mode. | Numerous bug fixes. | Llmcode wrote 53% of the code in this release. | . Llmcode v0.57.1 . | Fixed dependency conflict between llmcode[help] and [playwright]. | . Llmcode v0.57.0 . | Support for OpenAI o1 models: . | o1-preview now works well with diff edit format. | o1-preview with diff now matches SOTA leaderboard result with whole edit format. | llmcode --model o1-mini | llmcode --model o1-preview | . | On Windows, /run correctly uses PowerShell or cmd.exe. | Support for new 08-2024 Cohere models, by @jalammar. | Can now recursively add directories with /read-only. | User input prompts now fall back to simple input() if --no-pretty or a Windows console is not available. | Improved sanity check of git repo on startup. | Improvements to prompt cache chunking strategy. | Removed “No changes made to git tracked files”. | Numerous bug fixes for corner case crashes. | Updated all dependency versions. | Llmcode wrote 70% of the code in this release. | . Llmcode v0.56.0 . | Enables prompt caching for Sonnet via OpenRouter by @fry69 | Enables 8k output tokens for Sonnet via VertexAI and DeepSeek V2.5. | New /report command to open your browser with a pre-populated GitHub Issue. | New --chat-language switch to set the spoken language. | Now --[no-]suggest-shell-commands controls both prompting for and offering to execute shell commands. | Check key imports on launch, provide helpful error message if dependencies aren’t available. | Renamed --models to --list-models by @fry69. | Numerous bug fixes for corner case crashes. | Llmcode wrote 56% of the code in this release. | . Llmcode v0.55.0 . | Only print the pip command when self updating on Windows, without running it. | Converted many error messages to warning messages. | Added --tool-warning-color setting. | Blanket catch and handle git errors in any /command. | Catch and handle glob errors in /add, errors writing files. | Disabled built in linter for typescript. | Catch and handle terminals which don’t support pretty output. | Catch and handle playwright and pandoc errors. | Catch /voice transcription exceptions, show the WAV file so the user can recover it. | Llmcode wrote 53% of the code in this release. | . Llmcode v0.54.12 . | Switched to vX.Y.Z.dev version naming. | . Llmcode v0.54.11 . | Improved printed pip command output on Windows. | . Llmcode v0.54.10 . | Bugfix to test command in platform info. | . Llmcode v0.54.9 . | Include important devops files in the repomap. | Print quoted pip install commands to the user. | Adopt setuptools_scm to provide dev versions with git hashes. | Share active test and lint commands with the LLM. | Catch and handle most errors creating new files, reading existing files. | Catch and handle most git errors. | Added –verbose debug output for shell commands. | . Llmcode v0.54.8 . | Startup QOL improvements: . | Sanity check the git repo and exit gracefully on problems. | Pause for confirmation after model sanity check to allow user to review warnings. | . | Bug fix for shell commands on Windows. | Do not fuzzy match filenames when LLM is creating a new file, by @ozapinq | Numerous corner case bug fixes submitted via new crash report -&gt; GitHub Issue feature. | Crash reports now include python version, OS, etc. | . Llmcode v0.54.7 . | Offer to submit a GitHub issue pre-filled with uncaught exception info. | Bugfix for infinite output. | . Llmcode v0.54.6 . | New /settings command to show active settings. | Only show cache warming status update if --verbose. | . Llmcode v0.54.5 . | Bugfix for shell commands on Windows. | Refuse to make git repo in $HOME, warn user. | Don’t ask again in current session about a file the user has said not to add to the chat. | Added --update as an alias for --upgrade. | . Llmcode v0.54.4 . | Bugfix to completions for /model command. | Bugfix: revert home dir special case. | . Llmcode v0.54.3 . | Dependency watchdog&lt;5 for docker image. | . Llmcode v0.54.2 . | When users launch llmcode in their home dir, help them find/create a repo in a subdir. | Added missing pexpect dependency. | . Llmcode v0.54.0 . | Added model settings for gemini/gemini-1.5-pro-exp-0827 and gemini/gemini-1.5-flash-exp-0827. | Shell and /run commands can now be interactive in environments where a pty is available. | Optionally share output of suggested shell commands back to the LLM. | New --[no-]suggest-shell-commands switch to configure shell commands. | Performance improvements for autocomplete in large/mono repos. | New --upgrade switch to install latest version of llmcode from pypi. | Bugfix to --show-prompt. | Disabled automatic reply to the LLM on /undo for all models. | Removed pager from /web output. | Llmcode wrote 64% of the code in this release. | . Llmcode v0.53.0 . | Keep your prompt cache from expiring with --cache-keepalive-pings. | Pings the API every 5min to keep the cache warm. | . | You can now bulk accept/reject a series of add url and run shell confirmations. | Improved matching of filenames from S/R blocks with files in chat. | Stronger prompting for Sonnet to make edits in code chat mode. | Stronger prompting for the LLM to specify full file paths. | Improved shell command prompting. | Weak model now uses extra_headers, to support Anthropic beta features. | New --install-main-branch to update to the latest dev version of llmcode. | Improved error messages on attempt to add not-git subdir to chat. | Show model metadata info on --verbose. | Improved warnings when LLMs env variables aren’t set. | Bugfix to windows filenames which contain \\_. | Llmcode wrote 59% of the code in this release. | . Llmcode v0.52.1 . | Bugfix for NameError when applying edits. | . Llmcode v0.52.0 . | Llmcode now offers to run shell commands: . | Launch a browser to view updated html/css/js. | Install new dependencies. | Run DB migrations. | Run the program to exercise changes. | Run new test cases. | . | /read and /drop now expand ~ to the home dir. | Show the active chat mode at llmcode prompt. | New /reset command to /drop files and /clear chat history. | New --map-multiplier-no-files to control repo map size multiplier when no files are in the chat. | Reduced default multiplier to 2. | . | Bugfixes and improvements to auto commit sequencing. | Improved formatting of token reports and confirmation dialogs. | Default OpenAI model is now gpt-4o-2024-08-06. | Bumped dependencies to pickup litellm bugfixes. | Llmcode wrote 68% of the code in this release. | . Llmcode v0.51.0 . | Prompt caching for Anthropic models with --cache-prompts. | Caches the system prompt, repo map and /read-only files. | . | Repo map recomputes less often in large/mono repos or when caching enabled. | Use --map-refresh &lt;always|files|manual|auto&gt; to configure. | . | Improved cost estimate logic for caching. | Improved editing performance on Jupyter Notebook .ipynb files. | Show which config yaml file is loaded with --verbose. | Bumped dependency versions. | Bugfix: properly load .llmcode.models.metadata.json data. | Bugfix: Using --msg /ask ... caused an exception. | Bugfix: litellm tokenizer bug for images. | Llmcode wrote 56% of the code in this release. | . Llmcode v0.50.1 . | Bugfix for provider API exceptions. | . Llmcode v0.50.0 . | Infinite output for DeepSeek Coder, Mistral models in addition to Anthropic’s models. | New --deepseek switch to use DeepSeek Coder. | DeepSeek Coder uses 8k token output. | New --chat-mode &lt;mode&gt; switch to launch in ask/help/code modes. | New /code &lt;message&gt; command request a code edit while in ask mode. | Web scraper is more robust if page never idles. | Improved token and cost reporting for infinite output. | Improvements and bug fixes for /read only files. | Switched from setup.py to pyproject.toml, by @branchvincent. | Bug fix to persist files added during /ask. | Bug fix for chat history size in /tokens. | Llmcode wrote 66% of the code in this release. | . Llmcode v0.49.1 . | Bugfix to /help. | . Llmcode v0.49.0 . | Add read-only files to the chat context with /read and --read, including from outside the git repo. | /diff now shows diffs of all changes resulting from your request, including lint and test fixes. | New /clipboard command to paste images or text from the clipboard, replaces /add-clipboard-image. | Now shows the markdown scraped when you add a url with /web. | When scripting llmcode messages can now contain in-chat / commands. | Llmcode in docker image now suggests the correct command to update to latest version. | Improved retries on API errors (was easy to test during Sonnet outage). | Added --mini for gpt-4o-mini. | Bugfix to keep session cost accurate when using /ask and /help. | Performance improvements for repo map calculation. | /tokens now shows the active model. | Enhanced commit message attribution options: . | New --attribute-commit-message-author to prefix commit messages with ‘llmcode: ‘ if llmcode authored the changes, replaces --attribute-commit-message. | New --attribute-commit-message-committer to prefix all commit messages with ‘llmcode: ‘. | . | Llmcode wrote 61% of the code in this release. | . Llmcode v0.48.1 . | Added openai/gpt-4o-2024-08-06. | Worked around litellm bug that removes OpenRouter app headers when using extra_headers. | Improved progress indication during repo map processing. | Corrected instructions for upgrading the docker container to latest llmcode version. | Removed obsolete 16k token limit on commit diffs, use per-model limits. | . Llmcode v0.48.0 . | Performance improvements for large/mono repos. | Added --subtree-only to limit llmcode to current directory subtree. | Should help with large/mono repo performance. | . | New /add-clipboard-image to add images to the chat from your clipboard. | Use --map-tokens 1024 to use repo map with any model. | Support for Sonnet’s 8k output window. | Llmcode already supported infinite output from Sonnet. | . | Workaround litellm bug for retrying API server errors. | Upgraded dependencies, to pick up litellm bug fixes. | Llmcode wrote 44% of the code in this release. | . Llmcode v0.47.1 . | Improvements to conventional commits prompting. | . Llmcode v0.47.0 . | Commit message improvements: . | Added Conventional Commits guidelines to commit message prompt. | Added --commit-prompt to customize the commit message prompt. | Added strong model as a fallback for commit messages (and chat summaries). | . | Linting improvements: . | Ask before fixing lint errors. | Improved performance of --lint on all dirty files in repo. | Improved lint flow, now doing code edit auto-commit before linting. | Bugfix to properly handle subprocess encodings (also for /run). | . | Improved docker support: . | Resolved permission issues when using docker run --user xxx. | New khulnasoft/llmcode-full docker image, which includes all extras. | . | Switching to code and ask mode no longer summarizes the chat history. | Added graph of llmcode’s contribution to each release. | Generic auto-completions are provided for /commands without a completion override. | Fixed broken OCaml tags file. | Bugfix in /run add to chat approval logic. | Llmcode wrote 58% of the code in this release. | . Llmcode v0.46.1 . | Downgraded stray numpy dependency back to 1.26.4. | . Llmcode v0.46.0 . | New /ask &lt;question&gt; command to ask about your code, without making any edits. | New /chat-mode &lt;mode&gt; command to switch chat modes: . | ask: Ask questions about your code without making any changes. | code: Ask for changes to your code (using the best edit format). | help: Get help about using llmcode (usage, config, troubleshoot). | . | Add file: CONVENTIONS.md to .llmcode.conf.yml to always load a specific file. | Or file: [file1, file2, file3] to always load multiple files. | . | Enhanced token usage and cost reporting. Now works when streaming too. | Filename auto-complete for /add and /drop is now case-insensitive. | Commit message improvements: . | Updated commit message prompt to use imperative tense. | Fall back to main model if weak model is unable to generate a commit message. | . | Stop llmcode from asking to add the same url to the chat multiple times. | Updates and fixes to --no-verify-ssl: . | Fixed regression that broke it in v0.42.0. | Disables SSL certificate verification when /web scrapes websites. | . | Improved error handling and reporting in /web scraping functionality | Fixed syntax error in Elm’s tree-sitter scm file (by @cjoach). | Handle UnicodeEncodeError when streaming text to the terminal. | Updated dependencies to latest versions. | Llmcode wrote 45% of the code in this release. | . Llmcode v0.45.1 . | Use 4o-mini as the weak model wherever 3.5-turbo was used. | . Llmcode v0.45.0 . | GPT-4o mini scores similar to the original GPT 3.5, using whole edit format. | Llmcode is better at offering to add files to the chat on Windows. | Bugfix corner cases for /undo with new files or new repos. | Now shows last 4 characters of API keys in --verbose output. | Bugfix to precedence of multiple .env files. | Bugfix to gracefully handle HTTP errors when installing pandoc. | Llmcode wrote 42% of the code in this release. | . Llmcode v0.44.0 . | Default pip install size reduced by 3-12x. | Added 3 package extras, which llmcode will offer to install when needed: . | llmcode[help] | llmcode[browser] | llmcode[playwright] | . | Improved regex for detecting URLs in user chat messages. | Bugfix to globbing logic when absolute paths are included in /add. | Simplified output of --models. | The --check-update switch was renamed to --just-check-updated. | The --skip-check-update switch was renamed to --[no-]check-update. | Llmcode wrote 29% of the code in this release (157/547 lines). | . Llmcode v0.43.4 . | Added scipy back to main requirements.txt. | . Llmcode v0.43.3 . | Added build-essentials back to main Dockerfile. | . Llmcode v0.43.2 . | Moved HuggingFace embeddings deps into [hf-embed] extra. | Added [dev] extra. | . Llmcode v0.43.1 . | Replace the torch requirement with the CPU only version, because the GPU versions are huge. | . Llmcode v0.43.0 . | Use /help &lt;question&gt; to ask for help about using llmcode, customizing settings, troubleshooting, using LLMs, etc. | Allow multiple use of /undo. | All config/env/yml/json files now load from home, git root, cwd and named command line switch. | New $HOME/.llmcode/caches dir for app-wide expendable caches. | Default --model-settings-file is now .llmcode.model.settings.yml. | Default --model-metadata-file is now .llmcode.model.metadata.json. | Bugfix affecting launch with --no-git. | Llmcode wrote 9% of the 424 lines edited in this release. | . Llmcode v0.42.0 . | Performance release: . | 5X faster launch! | Faster auto-complete in large git repos (users report ~100X speedup)! | . | . Llmcode v0.41.0 . | Allow Claude 3.5 Sonnet to stream back &gt;4k tokens! . | It is the first model capable of writing such large coherent, useful code edits. | Do large refactors or generate multiple files of new code in one go. | . | Llmcode now uses claude-3-5-sonnet-20240620 by default if ANTHROPIC_API_KEY is set in the environment. | Enabled image support for 3.5 Sonnet and for GPT-4o &amp; 3.5 Sonnet via OpenRouter (by @yamitzky). | Added --attribute-commit-message to prefix llmcode’s commit messages with “llmcode:”. | Fixed regression in quality of one-line commit messages. | Automatically retry on Anthropic overloaded_error. | Bumped dependency versions. | . Llmcode v0.40.6 . | Fixed /undo so it works regardless of --attribute settings. | . Llmcode v0.40.5 . | Bump versions to pickup latest litellm to fix streaming issue with Gemini . | https://github.com/BerriAI/litellm/issues/4408 | . | . Llmcode v0.40.1 . | Improved context awareness of repomap. | Restored proper --help functionality. | . Llmcode v0.40.0 . | Improved prompting to discourage Sonnet from wasting tokens emitting unchanging code (#705). | Improved error info for token limit errors. | Options to suppress adding “(llmcode)” to the git author and committer names. | Use --model-settings-file to customize per-model settings, like use of repo-map (by @caseymcc). | Improved invocation of flake8 linter for python code. | . Llmcode v0.39.0 . | Use --sonnet for Claude 3.5 Sonnet, which is the top model on llmcode’s LLM code editing leaderboard. | All LLMCODE_xxx environment variables can now be set in .env (by @jpshack-at-palomar). | Use --llm-history-file to log raw messages sent to the LLM (by @daniel-vainsencher). | Commit messages are no longer prefixed with “llmcode:”. Instead the git author and committer names have “(llmcode)” added. | . Llmcode v0.38.0 . | Use --vim for vim keybindings in the chat. | Add LLM metadata via .llmcode.models.json file (by @caseymcc). | More detailed error messages on token limit errors. | Single line commit messages, without the recent chat messages. | Ensure --commit --dry-run does nothing. | Have playwright wait for idle network to better scrape js sites. | Documentation updates, moved into website/ subdir. | Moved tests/ into llmcode/tests/. | . Llmcode v0.37.0 . | Repo map is now optimized based on text of chat history as well as files added to chat. | Improved prompts when no files have been added to chat to solicit LLM file suggestions. | Llmcode will notice if you paste a URL into the chat, and offer to scrape it. | Performance improvements the repo map, especially in large repos. | Llmcode will not offer to add bare filenames like make or run which may just be words. | Properly override GIT_EDITOR env for commits if it is already set. | Detect supported audio sample rates for /voice. | Other small bug fixes. | . Llmcode v0.36.0 . | Llmcode can now lint your code and fix any errors. | Llmcode automatically lints and fixes after every LLM edit. | You can manually lint-and-fix files with /lint in the chat or --lint on the command line. | Llmcode includes built in basic linters for all supported tree-sitter languages. | You can also configure llmcode to use your preferred linter with --lint-cmd. | . | Llmcode has additional support for running tests and fixing problems. | Configure your testing command with --test-cmd. | Run tests with /test or from the command line with --test. | Llmcode will automatically attempt to fix any test failures. | . | . Llmcode v0.35.0 . | Llmcode now uses GPT-4o by default. | GPT-4o tops the llmcode LLM code editing leaderboard at 72.9%, versus 68.4% for Opus. | GPT-4o takes second on llmcode’s refactoring leaderboard with 62.9%, versus Opus at 72.3%. | . | Added --restore-chat-history to restore prior chat history on launch, so you can continue the last conversation. | Improved reflection feedback to LLMs using the diff edit format. | Improved retries on httpx errors. | . Llmcode v0.34.0 . | Updated prompting to use more natural phrasing about files, the git repo, etc. Removed reliance on read-write/read-only terminology. | Refactored prompting to unify some phrasing across edit formats. | Enhanced the canned assistant responses used in prompts. | Added explicit model settings for openrouter/anthropic/claude-3-opus, gpt-3.5-turbo | Added --show-prompts debug switch. | Bugfix: catch and retry on all litellm exceptions. | . Llmcode v0.33.0 . | Added native support for Deepseek models using DEEPSEEK_API_KEY and deepseek/deepseek-chat, etc rather than as a generic OpenAI compatible API. | . Llmcode v0.32.0 . | Llmcode LLM code editing leaderboards that rank popular models according to their ability to edit code. | Leaderboards include GPT-3.5/4 Turbo, Opus, Sonnet, Gemini 1.5 Pro, Llama 3, Deepseek Coder &amp; Command-R+. | . | Gemini 1.5 Pro now defaults to a new diff-style edit format (diff-fenced), enabling it to work better with larger code bases. | Support for Deepseek-V2, via more a flexible config of system messages in the diff edit format. | Improved retry handling on errors from model APIs. | Benchmark outputs results in YAML, compatible with leaderboard. | . Llmcode v0.31.0 . | Llmcode is now also AI pair programming in your browser! Use the --browser switch to launch an experimental browser based version of llmcode. | Switch models during the chat with /model &lt;name&gt; and search the list of available models with /models &lt;query&gt;. | . Llmcode v0.30.1 . | Adding missing google-generativeai dependency | . Llmcode v0.30.0 . | Added Gemini 1.5 Pro as a recommended free model. | Allow repo map for “whole” edit format. | Added --models &lt;MODEL-NAME&gt; to search the available models. | Added --no-show-model-warnings to silence model warnings. | . Llmcode v0.29.2 . | Improved model warnings for unknown or unfamiliar models | . Llmcode v0.29.1 . | Added better support for groq/llama3-70b-8192 | . Llmcode v0.29.0 . | Added support for directly connecting to Anthropic, Cohere, Gemini and many other LLM providers. | Added --weak-model &lt;model-name&gt; which allows you to specify which model to use for commit messages and chat history summarization. | New command line switches for working with popular models: . | --4-turbo-vision | --opus | --sonnet | --anthropic-api-key | . | Improved “whole” and “diff” backends to better support Cohere’s free to use Command-R+ model. | Allow /add of images from anywhere in the filesystem. | Fixed crash when operating in a repo in a detached HEAD state. | Fix: Use the same default model in CLI and python scripting. | . Llmcode v0.28.0 . | Added support for new gpt-4-turbo-2024-04-09 and gpt-4-turbo models. | Benchmarked at 61.7% on Exercism benchmark, comparable to gpt-4-0613 and worse than the gpt-4-preview-XXXX models. See recent Exercism benchmark results. | Benchmarked at 34.1% on the refactoring/laziness benchmark, significantly worse than the gpt-4-preview-XXXX models. See recent refactor bencmark results. | Llmcode continues to default to gpt-4-1106-preview as it performs best on both benchmarks, and significantly better on the refactoring/laziness benchmark. | . | . Llmcode v0.27.0 . | Improved repomap support for typescript, by @ryanfreckleton. | Bugfix: Only /undo the files which were part of the last commit, don’t stomp other dirty files | Bugfix: Show clear error message when OpenAI API key is not set. | Bugfix: Catch error for obscure languages without tags.scm file. | . Llmcode v0.26.1 . | Fixed bug affecting parsing of git config in some environments. | . Llmcode v0.26.0 . | Use GPT-4 Turbo by default. | Added -3 and -4 switches to use GPT 3.5 or GPT-4 (non-Turbo). | Bug fix to avoid reflecting local git errors back to GPT. | Improved logic for opening git repo on launch. | . Llmcode v0.25.0 . | Issue a warning if user adds too much code to the chat. | https://llmcode.khulnasoft.com/docs/faq.html#how-can-i-add-all-the-files-to-the-chat | . | Vocally refuse to add files to the chat that match .llmcodeignore . | Prevents bug where subsequent git commit of those files will fail. | . | Added --openai-organization-id argument. | Show the user a FAQ link if edits fail to apply. | Made past articles part of https://llmcode.khulnasoft.com/blog/ | . Llmcode v0.24.1 . | Fixed bug with cost computations when –no-steam in effect | . Llmcode v0.24.0 . | New /web &lt;url&gt; command which scrapes the url, turns it into fairly clean markdown and adds it to the chat. | Updated all OpenAI model names, pricing info | Default GPT 3.5 model is now gpt-3.5-turbo-0125. | Bugfix to the ! alias for /run. | . Llmcode v0.23.0 . | Added support for --model gpt-4-0125-preview and OpenAI’s alias --model gpt-4-turbo-preview. The --4turbo switch remains an alias for --model gpt-4-1106-preview at this time. | New /test command that runs a command and adds the output to the chat on non-zero exit status. | Improved streaming of markdown to the terminal. | Added /quit as alias for /exit. | Added --skip-check-update to skip checking for the update on launch. | Added --openrouter as a shortcut for --openai-api-base https://openrouter.ai/api/v1 | Fixed bug preventing use of env vars OPENAI_API_BASE, OPENAI_API_TYPE, OPENAI_API_VERSION, OPENAI_API_DEPLOYMENT_ID. | . Llmcode v0.22.0 . | Improvements for unified diff editing format. | Added ! as an alias for /run. | Autocomplete for /add and /drop now properly quotes filenames with spaces. | The /undo command asks GPT not to just retry reverted edit. | . Llmcode v0.21.1 . | Bugfix for unified diff editing format. | Added –4turbo and –4 aliases for –4-turbo. | . Llmcode v0.21.0 . | Support for python 3.12. | Improvements to unified diff editing format. | New --check-update arg to check if updates are available and exit with status code. | . Llmcode v0.20.0 . | Add images to the chat to automatically use GPT-4 Vision, by @joshuavial . | Bugfixes: . | Improved unicode encoding for /run command output, by @ctoth | Prevent false auto-commits on Windows, by @ctoth | . | . Llmcode v0.19.1 . | Removed stray debug output. | . Llmcode v0.19.0 . | Significantly reduced “lazy” coding from GPT-4 Turbo due to new unified diff edit format . | Score improves from 20% to 61% on new “laziness benchmark”. | Llmcode now uses unified diffs by default for gpt-4-1106-preview. | . | New --4-turbo command line switch as a shortcut for --model gpt-4-1106-preview. | . Llmcode v0.18.1 . | Upgraded to new openai python client v1.3.7. | . Llmcode v0.18.0 . | Improved prompting for both GPT-4 and GPT-4 Turbo. | Far fewer edit errors from GPT-4 Turbo (gpt-4-1106-preview). | Significantly better benchmark results from the June GPT-4 (gpt-4-0613). Performance leaps from 47%/64% up to 51%/71%. | . | Fixed bug where in-chat files were marked as both read-only and ready-write, sometimes confusing GPT. | Fixed bug to properly handle repos with submodules. | . Llmcode v0.17.0 . | Support for OpenAI’s new 11/06 models: . | gpt-4-1106-preview with 128k context window | gpt-3.5-turbo-1106 with 16k context window | . | Benchmarks for OpenAI’s new 11/06 models | Streamlined API for scripting llmcode, added docs | Ask for more concise SEARCH/REPLACE blocks. Benchmarked at 63.9%, no regression. | Improved repo-map support for elisp. | Fixed crash bug when /add used on file matching .gitignore | Fixed misc bugs to catch and handle unicode decoding errors. | . Llmcode v0.16.3 . | Fixed repo-map support for C#. | . Llmcode v0.16.2 . | Fixed docker image. | . Llmcode v0.16.1 . | Updated tree-sitter dependencies to streamline the pip install process | . Llmcode v0.16.0 . | Improved repository map using tree-sitter | Switched from “edit block” to “search/replace block”, which reduced malformed edit blocks. Benchmarked at 66.2%, no regression. | Improved handling of malformed edit blocks targeting multiple edits to the same file. Benchmarked at 65.4%, no regression. | Bugfix to properly handle malformed /add wildcards. | . Llmcode v0.15.0 . | Added support for .llmcodeignore file, which instructs llmcode to ignore parts of the git repo. | New --commit cmd line arg, which just commits all pending changes with a sensible commit message generated by gpt-3.5. | Added universal ctags and multiple architectures to the llmcode docker image | /run and /git now accept full shell commands, like: /run (cd subdir; ls) | Restored missing --encoding cmd line switch. | . Llmcode v0.14.2 . | Easily run llmcode from a docker image | Fixed bug with chat history summarization. | Fixed bug if soundfile package not available. | . Llmcode v0.14.1 . | /add and /drop handle absolute filenames and quoted filenames | /add checks to be sure files are within the git repo (or root) | If needed, warn users that in-chat file paths are all relative to the git repo | Fixed /add bug in when llmcode launched in repo subdir | Show models supported by api/key if requested model isn’t available | . Llmcode v0.14.0 . | Support for Claude2 and other LLMs via OpenRouter by @joshuavial | Documentation for running the llmcode benchmarking suite | Llmcode now requires Python &gt;= 3.9 | . Llmcode v0.13.0 . | Only git commit dirty files that GPT tries to edit | Send chat history as prompt/context for Whisper voice transcription | Added --voice-language switch to constrain /voice to transcribe to a specific language | Late-bind importing sounddevice, as it was slowing down llmcode startup | Improved –foo/–no-foo switch handling for command line and yml config settings | . Llmcode v0.12.0 . | Voice-to-code support, which allows you to code with your voice. | Fixed bug where /diff was causing crash. | Improved prompting for gpt-4, refactor of editblock coder. | Benchmarked at 63.2% for gpt-4/diff, no regression. | . Llmcode v0.11.1 . | Added a progress bar when initially creating a repo map. | Fixed bad commit message when adding new file to empty repo. | Fixed corner case of pending chat history summarization when dirty committing. | Fixed corner case of undefined text when using --no-pretty. | Fixed /commit bug from repo refactor, added test coverage. | Benchmarked at 53.4% for gpt-3.5/whole (no regression). | . Llmcode v0.11.0 . | Automatically summarize chat history to avoid exhausting context window. | More detail on dollar costs when running with --no-stream | Stronger GPT-3.5 prompt against skipping/eliding code in replies (51.9% benchmark, no regression) | Defend against GPT-3.5 or non-OpenAI models suggesting filenames surrounded by asterisks. | Refactored GitRepo code out of the Coder class. | . Llmcode v0.10.1 . | /add and /drop always use paths relative to the git root | Encourage GPT to use language like “add files to the chat” to ask users for permission to edit them. | . Llmcode v0.10.0 . | Added /git command to run git from inside llmcode chats. | Use Meta-ENTER (Esc+ENTER in some environments) to enter multiline chat messages. | Create a .gitignore with .llmcode* to prevent users from accidentally adding llmcode files to git. | Check pypi for newer versions and notify user. | Updated keyboard interrupt logic so that 2 ^C in 2 seconds always forces llmcode to exit. | Provide GPT with detailed error if it makes a bad edit block, ask for a retry. | Force --no-pretty if llmcode detects it is running inside a VSCode terminal. | Benchmarked at 64.7% for gpt-4/diff (no regression) | . Llmcode v0.9.0 . | Support for the OpenAI models in Azure | Added --show-repo-map | Improved output when retrying connections to the OpenAI API | Redacted api key from --verbose output | Bugfix: recognize and add files in subdirectories mentioned by user or GPT | Benchmarked at 53.8% for gpt-3.5-turbo/whole (no regression) | . Llmcode v0.8.3 . | Added --dark-mode and --light-mode to select colors optimized for terminal background | Install docs link to NeoVim plugin by @joshuavial | Reorganized the --help output | Bugfix/improvement to whole edit format, may improve coding editing for GPT-3.5 | Bugfix and tests around git filenames with unicode characters | Bugfix so that llmcode throws an exception when OpenAI returns InvalidRequest | Bugfix/improvement to /add and /drop to recurse selected directories | Bugfix for live diff output when using “whole” edit format | . Llmcode v0.8.2 . | Disabled general availability of gpt-4 (it’s rolling out, not 100% available yet) | . Llmcode v0.8.1 . | Ask to create a git repo if none found, to better track GPT’s code changes | Glob wildcards are now supported in /add and /drop commands | Pass --encoding into ctags, require it to return utf-8 | More robust handling of filepaths, to avoid 8.3 windows filenames | Added FAQ | Marked GPT-4 as generally available | Bugfix for live diffs of whole coder with missing filenames | Bugfix for chats with multiple files | Bugfix in editblock coder prompt | . Llmcode v0.8.0 . | Benchmark comparing code editing in GPT-3.5 and GPT-4 | Improved Windows support: . | Fixed bugs related to path separators in Windows | Added a CI step to run all tests on Windows | . | Improved handling of Unicode encoding/decoding . | Explicitly read/write text files with utf-8 encoding by default (mainly benefits Windows) | Added --encoding switch to specify another encoding | Gracefully handle decoding errors | . | Added --code-theme switch to control the pygments styling of code blocks (by @kwmiebach) | Better status messages explaining the reason when ctags is disabled | . Llmcode v0.7.2: . | Fixed a bug to allow llmcode to edit files that contain triple backtick fences. | . Llmcode v0.7.1: . | Fixed a bug in the display of streaming diffs in GPT-3.5 chats | . Llmcode v0.7.0: . | Graceful handling of context window exhaustion, including helpful tips. | Added --message to give GPT that one instruction and then exit after it replies and any edits are performed. | Added --no-stream to disable streaming GPT responses. | Non-streaming responses include token usage info. | Enables display of cost info based on OpenAI advertised pricing. | . | Coding competence benchmarking tool against suite of programming tasks based on Execism’s python repo. | https://github.com/exercism/python | . | Major refactor in preparation for supporting new function calls api. | Initial implementation of a function based code editing backend for 3.5. | Initial experiments show that using functions makes 3.5 less competent at coding. | . | Limit automatic retries when GPT returns a malformed edit response. | . Llmcode v0.6.2 . | Support for gpt-3.5-turbo-16k, and all OpenAI chat models | Improved ability to correct when gpt-4 omits leading whitespace in code edits | Added --openai-api-base to support API proxies, etc. | . Llmcode v0.5.0 . | Added support for gpt-3.5-turbo and gpt-4-32k. | Added --map-tokens to set a token budget for the repo map, along with a PageRank based algorithm for prioritizing which files and identifiers to include in the map. | Added in-chat command /tokens to report on context window token usage. | Added in-chat command /clear to clear the conversation history. | . ",
    "url": "/HISTORY.html#release-notes",
    
    "relUrl": "/HISTORY.html#release-notes"
  },"3": {
    "doc": "Example chat transcripts",
    "title": "Example chat transcripts",
    "content": "Below are some chat transcripts showing what it’s like to code with llmcode. In the chats, you’ll see a variety of coding tasks like generating new code, editing existing code, debugging, exploring unfamiliar code, etc. | Hello World Flask App: Start from scratch and have llmcode create a simple Flask app with various endpoints, such as adding two numbers and calculating the Fibonacci sequence. | Javascript Game Modification: Dive into an existing open-source repo, and get llmcode’s help to understand it and make modifications. | Complex Multi-file Change with Debugging: Llmcode makes a complex code change that is coordinated across multiple source files, and resolves bugs by reviewing error output and doc snippets. | Create a Black Box Test Case: Llmcode creates a “black box” test case without access to the source of the method being tested, using only a high level map of the repository based on ctags. | Honor the NO_COLOR env var: The user pastes the NO_COLOR spec from no-color.org into the chat, and llmcode modifies the application to conform. | Download, analyze and plot US Census data: Llmcode downloads census data, suggests some hypotheses to test, tests one and then summarizes and plots a graph of the results. | Semantic Search &amp; Replace: Updating a collection of function calls, which requires dealing with various formatting and semantic differences in the various function call sites. | Pong Game with Pygame: Creating a simple Pong game using the Pygame library, with customizations for paddle size and color, and ball speed adjustments. | CSS Exercise: Animation Dropdown Menu: A small CSS exercise involving adding animation to a dropdown menu. | Automatically Update Docs: Automatically updating documentation based on the latest version of the main() function. | Editing an Asciinema Cast File: Editing escape sequences in an asciinema screencast file. | . ",
    "url": "/examples/README.html",
    
    "relUrl": "/examples/README.html"
  },"4": {
    "doc": "Example chat transcripts",
    "title": "What’s happening in these chats?",
    "content": "To better understand the chat transcripts, it’s worth knowing that: . | Each time the LLM suggests a code change, llmcode automatically applies it to the source files. | After applying the edits, llmcode commits them to git with a descriptive commit message. | The LLM can only see and edit files which have been “added to the chat session”. The user adds files either via the command line or the in-chat /add command. If the LLM asks to see specific files, llmcode asks the user for permission to add them to the chat. The transcripts contain notifications from llmcode whenever a file is added or dropped from the session. | . ",
    "url": "/examples/README.html#whats-happening-in-these-chats",
    
    "relUrl": "/examples/README.html#whats-happening-in-these-chats"
  },"5": {
    "doc": "Example chat transcripts",
    "title": "Transcript formatting",
    "content": "This is output from the llmcode tool. These are chat messages written by the user. Chat responses from the LLM are in a blue font like this, and often include colorized “edit blocks” that specify edits to the code. Here’s a sample edit block that switches from printing “hello” to “goodbye”: . hello.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL print(\"hello\") ======= print(\"goodbye\") &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . ",
    "url": "/examples/README.html#transcript-formatting",
    
    "relUrl": "/examples/README.html#transcript-formatting"
  },"6": {
    "doc": "Create a “black box” test case",
    "title": "Create a “black box” test case",
    "content": "This transcript shows llmcode creating a black box test case, without being given access to the source code of the function being tested or any of the other code in the repo. Instead, llmcode is operating entirely off a high level map of the repository based on ctags, which provides details on all the symbols and function signatures in the repo. Using only the meta-data in the map, the LLM is able to figure out how to call the method to be tested, as well as how to instantiate multiple class objects that are required to prepare for the test. In detail, llmcode is able to use the repo map to: . | Find the function signature of the cmd_add() function which the user wants a test case for. | Determine that it is a method of the Command class, so the test case will need to instantiate an instance to conduct the test. | Identify that creating a Command instance requires passing in InputOutput and Coder instances. | Figure out the arguments required to instantiate the InputOutput instance. | Decide that the Coder class looks complex enough to use a MagickMock. | . The cmd_add() method is defined without type annotations, so llmcode makes a reasonable but incorrect guess that it accepts a list of files. The user invokes the /run command to run the new unit test, which throws an error based on this misunderstanding. The LLM reviews the error message and fixes the call to cmd_add() to correctly pass a str with a space-separated list of filenames instead. The test then passes on the next /run. $ llmcode tests/test_commands.py Creating empty file tests/test_commands.py Added tests/test_commands.py to the chat Files not tracked in .git: . | tests/test_commands.py | . Add them? y Added tests/test_commands.py to the git repo Commit b9cea2c Added new files to the git repo: tests/test_commands.py Using git repo: .git . add a test for cmd_add that passes in ‘foo.txt’ and ‘bar.txt’ and ensures they have both been created after the test. run the test in a tmp dir. I will add a new test method called test_cmd_add in the tests/test_commands.py file. This test will create a temporary directory, change the current working directory to the temporary directory, and then call the cmd_add method with ‘foo.txt’ and ‘bar.txt’ as arguments. After that, it will check if both files have been created in the temporary directory. Here’s the code change: . tests/test_commands.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL ======= import os import tempfile from unittest import TestCase from unittest.mock import MagicMock from llmcode.commands import Commands from llmcode.io import InputOutput class TestCommands(TestCase): def test_cmd_add(self): # Create a temporary directory and change the current working directory with tempfile.TemporaryDirectory() as tmpdir: os.chdir(tmpdir) # Initialize the Commands and InputOutput objects io = InputOutput(pretty=False, yes=True) coder = MagicMock() commands = Commands(io, coder) # Call the cmd_add method with 'foo.txt' and 'bar.txt' commands.cmd_add([\"foo.txt\", \"bar.txt\"]) # Check if both files have been created in the temporary directory self.assertTrue(os.path.exists(\"foo.txt\")) self.assertTrue(os.path.exists(\"bar.txt\")) &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . This test will ensure that the cmd_add method creates the specified files in the current working directory. Applied edit to tests/test_commands.py Commit e2d5212 llmcode: Added a test method test_cmd_add that creates a temporary directory, calls the cmd_add method with ‘foo.txt’ and ‘bar.txt’, and checks if both files have been created in the temporary directory. /run pytest tests/test_commands.py . ============================= test session starts ============================== platform darwin -- Python 3.11.3, pytest-7.3.1, pluggy-1.0.0 rootdir: /Users/gauthier/Projects/llmcode collected 1 item tests/test_commands.py F [100%] =================================== FAILURES =================================== __________________________ TestCommands.test_cmd_add ___________________________ self = &lt;tests.test_commands.TestCommands testMethod=test_cmd_add&gt; def test_cmd_add(self): # Create a temporary directory and change the current working directory with tempfile.TemporaryDirectory() as tmpdir: os.chdir(tmpdir) # Initialize the Commands and InputOutput objects io = InputOutput(pretty=False, yes=True) coder = MagicMock() commands = Commands(io, coder) # Call the cmd_add method with 'foo.txt' and 'bar.txt' &gt; commands.cmd_add([\"foo.txt\", \"bar.txt\"]) /Users/gauthier/Projects/llmcode/tests/test_commands.py:20: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ self = &lt;llmcode.commands.Commands object at 0x10b412b50&gt; args = ['foo.txt', 'bar.txt'] def cmd_add(self, args): \"Add matching files to the chat session\" added_fnames = [] files = self.coder.get_all_relative_files() &gt; for word in args.split(): E AttributeError: 'list' object has no attribute 'split' /Users/gauthier/Projects/llmcode/llmcode/commands.py:161: AttributeError =========================== short test summary info ============================ FAILED tests/test_commands.py::TestCommands::test_cmd_add - AttributeError: '... ============================== 1 failed in 0.34s =============================== . Add the output to the chat? [y/n] (y): y . It seems that the cmd_add method in llmcode/commands.py is expecting a string as input, but in the test, we are passing a list. I will update the test to pass a string instead of a list. Here’s the code change: . tests/test_commands.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL # Call the cmd_add method with 'foo.txt' and 'bar.txt' commands.cmd_add([\"foo.txt\", \"bar.txt\"]) ======= # Call the cmd_add method with 'foo.txt' and 'bar.txt' as a single string commands.cmd_add(\"foo.txt bar.txt\") &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . Now, run the test again with the updated test method: . pytest tests/test_commands.py . Applied edit to tests/test_commands.py Commit d1551b4 llmcode: Updated test to pass a string instead of a list when calling cmd_add method. /run pytest tests/test_commands.py . ============================= test session starts ============================== platform darwin -- Python 3.11.3, pytest-7.3.1, pluggy-1.0.0 rootdir: /Users/gauthier/Projects/llmcode collected 1 item tests/test_commands.py . [100%] ============================== 1 passed in 0.38s =============================== . ",
    "url": "/examples/add-test.html",
    
    "relUrl": "/examples/add-test.html"
  },"7": {
    "doc": "Advanced model settings",
    "title": "Advanced model settings",
    "content": " ",
    "url": "/docs/config/adv-model-settings.html",
    
    "relUrl": "/docs/config/adv-model-settings.html"
  },"8": {
    "doc": "Advanced model settings",
    "title": "Context window size and token costs",
    "content": "In most cases, you can safely ignore llmcode’s warning about unknown context window size and model costs. Llmcode never enforces token limits, it only reports token limit errors from the API provider. You probably don’t need to configure llmcode with the proper token limits for unusual models. But, you can register context window limits and costs for models that aren’t known to llmcode. Create a .llmcode.model.metadata.json file in one of these locations: . | Your home directory. | The root if your git repo. | The current directory where you launch llmcode. | Or specify a specific file with the --model-metadata-file &lt;filename&gt; switch. | . If the files above exist, they will be loaded in that order. Files loaded last will take priority. The json file should be a dictionary with an entry for each model, as follows: . { \"deepseek/deepseek-chat\": { \"max_tokens\": 4096, \"max_input_tokens\": 32000, \"max_output_tokens\": 4096, \"input_cost_per_token\": 0.00000014, \"output_cost_per_token\": 0.00000028, \"litellm_provider\": \"deepseek\", \"mode\": \"chat\" } } . Use a fully qualified model name with a provider/ at the front in the .llmcode.model.metadata.json file. For example, use deepseek/deepseek-chat, not just deepseek-chat. That prefix should match the litellm_provider field. Contribute model metadata . Llmcode relies on litellm’s model_prices_and_context_window.json file for model metadata. Consider submitting a PR to that file to add missing models. ",
    "url": "/docs/config/adv-model-settings.html#context-window-size-and-token-costs",
    
    "relUrl": "/docs/config/adv-model-settings.html#context-window-size-and-token-costs"
  },"9": {
    "doc": "Advanced model settings",
    "title": "Model settings",
    "content": "Llmcode has a number of settings that control how it works with different models. These model settings are pre-configured for most popular models. But it can sometimes be helpful to override them or add settings for a model that llmcode doesn’t know about. Configuration file locations . You can override or add settings for any model by creating a .llmcode.model.settings.yml file in one of these locations: . | Your home directory. | The root of your git repo. | The current directory where you launch llmcode. | Or specify a specific file with the --model-settings-file &lt;filename&gt; switch. | . If the files above exist, they will be loaded in that order. Files loaded last will take priority. The yaml file should be a list of dictionary objects for each model. Global extra params . You can use the special model name llmcode/extra_params to define extra_params that will be passed to litellm.completion() for all models. Only the extra_params dict is used from this special model name. For example: . - name: llmcode/extra_params extra_params: extra_headers: Custom-Header: value max_tokens: 8192 . These settings will be merged with any model-specific settings, with the llmcode/extra_params settings taking precedence for any direct conflicts. Controlling o1 reasoning effort . You need this chunk of yaml: . extra_params: extra_body: reasoning_effort: high . This is a full entry for o1 with that setting, obtained by finding the default entry in the list below and adding the above extra_params entry: . - name: o1 edit_format: diff weak_model_name: gpt-4o-mini use_repo_map: true send_undo_reply: false lazy: false reminder: user examples_as_sys_msg: false cache_control: false caches_by_default: false use_system_prompt: true use_temperature: false streaming: false editor_model_name: gpt-4o editor_edit_format: editor-diff extra_params: extra_body: reasoning_effort: high . Default model settings . Below are all the pre-configured model settings to give a sense for the settings which are supported. You can also look at the ModelSettings class in models.py file for more details about all of the model setting that llmcode supports. The first entry shows all the settings, with their default values. For a real model, you just need to include whichever fields that you want to override the defaults. - name: (default values) edit_format: whole weak_model_name: null use_repo_map: false send_undo_reply: false lazy: false reminder: user examples_as_sys_msg: false extra_params: null cache_control: false caches_by_default: false use_system_prompt: true use_temperature: true streaming: true editor_model_name: null editor_edit_format: null remove_reasoning: null system_prompt_prefix: null - name: anthropic/claude-3-5-haiku-20241022 edit_format: diff weak_model_name: anthropic/claude-3-5-haiku-20241022 use_repo_map: true extra_params: extra_headers: anthropic-beta: prompt-caching-2024-07-31,pdfs-2024-09-25 cache_control: true - name: anthropic/claude-3-5-sonnet-20240620 edit_format: diff weak_model_name: anthropic/claude-3-5-haiku-20241022 use_repo_map: true examples_as_sys_msg: true extra_params: extra_headers: anthropic-beta: prompt-caching-2024-07-31,pdfs-2024-09-25 max_tokens: 8192 cache_control: true editor_model_name: anthropic/claude-3-5-sonnet-20240620 editor_edit_format: editor-diff - name: anthropic/claude-3-5-sonnet-20241022 edit_format: diff weak_model_name: anthropic/claude-3-5-haiku-20241022 use_repo_map: true examples_as_sys_msg: true extra_params: extra_headers: anthropic-beta: prompt-caching-2024-07-31,pdfs-2024-09-25 max_tokens: 8192 cache_control: true editor_model_name: anthropic/claude-3-5-sonnet-20241022 editor_edit_format: editor-diff - name: anthropic/claude-3-5-sonnet-latest edit_format: diff weak_model_name: anthropic/claude-3-5-haiku-20241022 use_repo_map: true examples_as_sys_msg: true extra_params: extra_headers: anthropic-beta: prompt-caching-2024-07-31,pdfs-2024-09-25 max_tokens: 8192 cache_control: true editor_model_name: anthropic/claude-3-5-sonnet-20241022 editor_edit_format: editor-diff - name: anthropic/claude-3-7-sonnet-20250219 edit_format: diff weak_model_name: anthropic/claude-3-5-haiku-20241022 use_repo_map: true examples_as_sys_msg: true extra_params: extra_headers: anthropic-beta: prompt-caching-2024-07-31,pdfs-2024-09-25,output-128k-2025-02-19 max_tokens: 64000 cache_control: true editor_model_name: anthropic/claude-3-7-sonnet-20250219 editor_edit_format: editor-diff - name: anthropic/claude-3-7-sonnet-latest edit_format: diff weak_model_name: anthropic/claude-3-5-haiku-20241022 use_repo_map: true examples_as_sys_msg: true extra_params: extra_headers: anthropic-beta: prompt-caching-2024-07-31,pdfs-2024-09-25,output-128k-2025-02-19 max_tokens: 64000 cache_control: true editor_model_name: anthropic/claude-3-7-sonnet-latest editor_edit_format: editor-diff - name: anthropic/claude-3-haiku-20240307 weak_model_name: anthropic/claude-3-haiku-20240307 examples_as_sys_msg: true extra_params: extra_headers: anthropic-beta: prompt-caching-2024-07-31,pdfs-2024-09-25 cache_control: true - name: azure/o1 edit_format: diff weak_model_name: azure/gpt-4o-mini use_repo_map: true use_temperature: false streaming: false editor_model_name: azure/gpt-4o editor_edit_format: editor-diff - name: azure/o1-mini weak_model_name: azure/gpt-4o-mini use_repo_map: true use_system_prompt: false use_temperature: false editor_model_name: azure/gpt-4o editor_edit_format: editor-diff - name: azure/o1-preview edit_format: diff weak_model_name: azure/gpt-4o-mini use_repo_map: true use_system_prompt: false use_temperature: false editor_model_name: azure/gpt-4o editor_edit_format: editor-diff - name: azure/o3-mini edit_format: diff weak_model_name: azure/gpt-4o-mini use_repo_map: true use_temperature: false editor_model_name: azure/gpt-4o editor_edit_format: editor-diff system_prompt_prefix: 'Formatting re-enabled. ' - name: bedrock/anthropic.claude-3-5-haiku-20241022-v1:0 edit_format: diff weak_model_name: bedrock/anthropic.claude-3-5-haiku-20241022-v1:0 use_repo_map: true extra_params: extra_headers: anthropic-beta: prompt-caching-2024-07-31,pdfs-2024-09-25 cache_control: true - name: bedrock/anthropic.claude-3-5-sonnet-20241022-v2:0 edit_format: diff weak_model_name: bedrock/anthropic.claude-3-5-haiku-20241022-v1:0 use_repo_map: true examples_as_sys_msg: true extra_params: extra_headers: anthropic-beta: prompt-caching-2024-07-31,pdfs-2024-09-25 max_tokens: 8192 cache_control: true editor_model_name: bedrock/anthropic.claude-3-5-sonnet-20241022-v2:0 editor_edit_format: editor-diff - name: bedrock/anthropic.claude-3-7-sonnet-20250219-v1:0 edit_format: diff weak_model_name: bedrock/anthropic.claude-3-5-haiku-20241022-v1:0 use_repo_map: true examples_as_sys_msg: true extra_params: extra_headers: anthropic-beta: prompt-caching-2024-07-31,pdfs-2024-09-25,output-128k-2025-02-19 max_tokens: 64000 cache_control: true editor_model_name: bedrock/anthropic.claude-3-7-sonnet-20250219-v1:0 editor_edit_format: editor-diff - name: bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0 edit_format: diff weak_model_name: bedrock/us.anthropic.claude-3-5-haiku-20241022-v1:0 use_repo_map: true examples_as_sys_msg: true extra_params: extra_headers: anthropic-beta: prompt-caching-2024-07-31,pdfs-2024-09-25,output-128k-2025-02-19 max_tokens: 64000 cache_control: true editor_model_name: bedrock/us.anthropic.claude-3-7-sonnet-20250219-v1:0 editor_edit_format: editor-diff - name: bedrock_converse/anthropic.claude-3-7-sonnet-20250219-v1:0 edit_format: diff weak_model_name: bedrock_converse/anthropic.claude-3-5-haiku-20241022-v1:0 use_repo_map: true examples_as_sys_msg: true extra_params: extra_headers: anthropic-beta: prompt-caching-2024-07-31,pdfs-2024-09-25,output-128k-2025-02-19 max_tokens: 64000 cache_control: true editor_model_name: bedrock_converse/anthropic.claude-3-7-sonnet-20250219-v1:0 editor_edit_format: editor-diff - name: bedrock_converse/us.anthropic.claude-3-7-sonnet-20250219-v1:0 edit_format: diff weak_model_name: bedrock_converse/us.anthropic.claude-3-5-haiku-20241022-v1:0 use_repo_map: true examples_as_sys_msg: true extra_params: extra_headers: anthropic-beta: prompt-caching-2024-07-31,pdfs-2024-09-25,output-128k-2025-02-19 max_tokens: 64000 cache_control: true editor_model_name: bedrock_converse/us.anthropic.claude-3-7-sonnet-20250219-v1:0 editor_edit_format: editor-diff - name: claude-3-5-haiku-20241022 edit_format: diff weak_model_name: claude-3-5-haiku-20241022 use_repo_map: true examples_as_sys_msg: true extra_params: extra_headers: anthropic-beta: prompt-caching-2024-07-31,pdfs-2024-09-25 cache_control: true - name: claude-3-5-sonnet-20240620 edit_format: diff weak_model_name: claude-3-5-haiku-20241022 use_repo_map: true examples_as_sys_msg: true extra_params: extra_headers: anthropic-beta: prompt-caching-2024-07-31,pdfs-2024-09-25 max_tokens: 8192 cache_control: true editor_model_name: claude-3-5-sonnet-20240620 editor_edit_format: editor-diff - name: claude-3-5-sonnet-20241022 edit_format: diff weak_model_name: claude-3-5-haiku-20241022 use_repo_map: true examples_as_sys_msg: true extra_params: extra_headers: anthropic-beta: prompt-caching-2024-07-31,pdfs-2024-09-25 max_tokens: 8192 cache_control: true editor_model_name: claude-3-5-sonnet-20241022 editor_edit_format: editor-diff - name: claude-3-7-sonnet-20250219 edit_format: diff weak_model_name: claude-3-5-haiku-20241022 use_repo_map: true examples_as_sys_msg: true extra_params: extra_headers: anthropic-beta: prompt-caching-2024-07-31,pdfs-2024-09-25,output-128k-2025-02-19 max_tokens: 64000 cache_control: true editor_model_name: claude-3-7-sonnet-20250219 editor_edit_format: editor-diff - name: claude-3-7-sonnet-latest edit_format: diff weak_model_name: claude-3-5-haiku-20241022 use_repo_map: true examples_as_sys_msg: true extra_params: extra_headers: anthropic-beta: prompt-caching-2024-07-31,pdfs-2024-09-25,output-128k-2025-02-19 max_tokens: 64000 cache_control: true editor_model_name: claude-3-7-sonnet-latest editor_edit_format: editor-diff - name: claude-3-haiku-20240307 weak_model_name: claude-3-haiku-20240307 examples_as_sys_msg: true extra_params: extra_headers: anthropic-beta: prompt-caching-2024-07-31,pdfs-2024-09-25 cache_control: true - name: claude-3-opus-20240229 edit_format: diff weak_model_name: claude-3-5-haiku-20241022 use_repo_map: true - name: claude-3-sonnet-20240229 weak_model_name: claude-3-5-haiku-20241022 - name: command-r-08-2024 weak_model_name: command-r-08-2024 use_repo_map: true - name: command-r-plus weak_model_name: command-r-plus use_repo_map: true - name: command-r-plus-08-2024 weak_model_name: command-r-plus-08-2024 use_repo_map: true - name: deepseek-chat edit_format: diff use_repo_map: true reminder: sys examples_as_sys_msg: true extra_params: max_tokens: 8192 - name: deepseek-coder edit_format: diff use_repo_map: true reminder: sys examples_as_sys_msg: true extra_params: max_tokens: 8192 caches_by_default: true - name: deepseek/deepseek-chat edit_format: diff use_repo_map: true reminder: sys examples_as_sys_msg: true extra_params: max_tokens: 8192 caches_by_default: true - name: deepseek/deepseek-coder edit_format: diff use_repo_map: true reminder: sys examples_as_sys_msg: true extra_params: max_tokens: 8192 caches_by_default: true - name: deepseek/deepseek-reasoner edit_format: diff weak_model_name: deepseek/deepseek-chat use_repo_map: true examples_as_sys_msg: true extra_params: max_tokens: 8192 caches_by_default: true use_temperature: false editor_model_name: deepseek/deepseek-chat editor_edit_format: editor-diff - name: fireworks_ai/accounts/fireworks/models/deepseek-r1 edit_format: diff weak_model_name: fireworks_ai/accounts/fireworks/models/deepseek-v3 use_repo_map: true extra_params: max_tokens: 160000 use_temperature: false editor_model_name: fireworks_ai/accounts/fireworks/models/deepseek-v3 editor_edit_format: editor-diff remove_reasoning: think - name: fireworks_ai/accounts/fireworks/models/deepseek-v3 edit_format: diff use_repo_map: true reminder: sys examples_as_sys_msg: true extra_params: max_tokens: 128000 - name: gemini/gemini-1.5-flash-002 - name: gemini/gemini-1.5-flash-exp-0827 - name: gemini/gemini-1.5-pro edit_format: diff-fenced use_repo_map: true - name: gemini/gemini-1.5-pro-002 edit_format: diff use_repo_map: true - name: gemini/gemini-1.5-pro-exp-0827 edit_format: diff-fenced use_repo_map: true - name: gemini/gemini-1.5-pro-latest edit_format: diff-fenced use_repo_map: true - name: gemini/gemini-2.0-flash edit_format: diff use_repo_map: true - name: gemini/gemini-2.0-flash-exp edit_format: diff use_repo_map: true - name: gemini/gemini-exp-1114 edit_format: diff use_repo_map: true - name: gemini/gemini-exp-1121 edit_format: diff use_repo_map: true - name: gemini/gemini-exp-1206 edit_format: diff use_repo_map: true - name: gpt-3.5-turbo weak_model_name: gpt-4o-mini reminder: sys - name: gpt-3.5-turbo-0125 weak_model_name: gpt-4o-mini reminder: sys - name: gpt-3.5-turbo-0613 weak_model_name: gpt-4o-mini reminder: sys - name: gpt-3.5-turbo-1106 weak_model_name: gpt-4o-mini reminder: sys - name: gpt-3.5-turbo-16k-0613 weak_model_name: gpt-4o-mini reminder: sys - name: gpt-4-0125-preview edit_format: udiff weak_model_name: gpt-4o-mini use_repo_map: true lazy: true reminder: sys examples_as_sys_msg: true - name: gpt-4-0314 edit_format: diff weak_model_name: gpt-4o-mini use_repo_map: true reminder: sys examples_as_sys_msg: true - name: gpt-4-0613 edit_format: diff weak_model_name: gpt-4o-mini use_repo_map: true reminder: sys - name: gpt-4-1106-preview edit_format: udiff weak_model_name: gpt-4o-mini use_repo_map: true lazy: true reminder: sys - name: gpt-4-32k-0613 edit_format: diff weak_model_name: gpt-4o-mini use_repo_map: true reminder: sys - name: gpt-4-turbo edit_format: udiff weak_model_name: gpt-4o-mini use_repo_map: true lazy: true reminder: sys - name: gpt-4-turbo-2024-04-09 edit_format: udiff weak_model_name: gpt-4o-mini use_repo_map: true lazy: true reminder: sys - name: gpt-4-vision-preview edit_format: diff weak_model_name: gpt-4o-mini use_repo_map: true reminder: sys - name: gpt-4o edit_format: diff weak_model_name: gpt-4o-mini use_repo_map: true lazy: true reminder: sys examples_as_sys_msg: true editor_edit_format: editor-diff - name: gpt-4o-2024-08-06 edit_format: diff weak_model_name: gpt-4o-mini use_repo_map: true lazy: true reminder: sys examples_as_sys_msg: true - name: gpt-4o-2024-11-20 edit_format: diff weak_model_name: gpt-4o-mini use_repo_map: true lazy: true reminder: sys examples_as_sys_msg: true - name: gpt-4o-mini weak_model_name: gpt-4o-mini lazy: true reminder: sys - name: groq/llama3-70b-8192 edit_format: diff weak_model_name: groq/llama3-8b-8192 examples_as_sys_msg: true - name: o1 edit_format: diff weak_model_name: gpt-4o-mini use_repo_map: true use_temperature: false streaming: false editor_model_name: gpt-4o editor_edit_format: editor-diff system_prompt_prefix: 'Formatting re-enabled. ' - name: o1-mini weak_model_name: gpt-4o-mini use_repo_map: true use_system_prompt: false use_temperature: false editor_model_name: gpt-4o editor_edit_format: editor-diff - name: o1-preview edit_format: architect weak_model_name: gpt-4o-mini use_repo_map: true use_system_prompt: false use_temperature: false editor_model_name: gpt-4o editor_edit_format: editor-diff - name: o3-mini edit_format: diff weak_model_name: gpt-4o-mini use_repo_map: true use_temperature: false editor_model_name: gpt-4o editor_edit_format: editor-diff system_prompt_prefix: 'Formatting re-enabled. ' - name: openai/gpt-4o edit_format: diff weak_model_name: gpt-4o-mini use_repo_map: true lazy: true reminder: sys examples_as_sys_msg: true editor_edit_format: editor-diff - name: openai/gpt-4o-2024-08-06 edit_format: diff weak_model_name: gpt-4o-mini use_repo_map: true lazy: true reminder: sys examples_as_sys_msg: true - name: openai/gpt-4o-2024-11-20 edit_format: diff weak_model_name: gpt-4o-mini use_repo_map: true lazy: true reminder: sys examples_as_sys_msg: true - name: openai/gpt-4o-mini weak_model_name: openai/gpt-4o-mini lazy: true reminder: sys - name: openai/o1 edit_format: diff weak_model_name: openai/gpt-4o-mini use_repo_map: true use_temperature: false streaming: false editor_model_name: openai/gpt-4o editor_edit_format: editor-diff system_prompt_prefix: 'Formatting re-enabled. ' - name: openai/o1-mini weak_model_name: openai/gpt-4o-mini use_repo_map: true use_system_prompt: false use_temperature: false editor_model_name: openai/gpt-4o editor_edit_format: editor-diff - name: openai/o1-preview edit_format: diff weak_model_name: openai/gpt-4o-mini use_repo_map: true use_system_prompt: false use_temperature: false editor_model_name: openai/gpt-4o editor_edit_format: editor-diff - name: openai/o3-mini edit_format: diff weak_model_name: gpt-4o-mini use_repo_map: true use_temperature: false editor_model_name: gpt-4o editor_edit_format: editor-diff system_prompt_prefix: 'Formatting re-enabled. ' - name: openrouter/anthropic/claude-3-opus edit_format: diff weak_model_name: openrouter/anthropic/claude-3-5-haiku use_repo_map: true - name: openrouter/anthropic/claude-3.5-sonnet edit_format: diff weak_model_name: openrouter/anthropic/claude-3-5-haiku use_repo_map: true examples_as_sys_msg: true extra_params: max_tokens: 8192 cache_control: true editor_model_name: openrouter/anthropic/claude-3.5-sonnet editor_edit_format: editor-diff - name: openrouter/anthropic/claude-3.5-sonnet:beta edit_format: diff weak_model_name: openrouter/anthropic/claude-3-5-haiku:beta use_repo_map: true examples_as_sys_msg: true extra_params: max_tokens: 8192 cache_control: true editor_model_name: openrouter/anthropic/claude-3.5-sonnet:beta editor_edit_format: editor-diff - name: openrouter/anthropic/claude-3.7-sonnet edit_format: diff weak_model_name: openrouter/anthropic/claude-3-5-haiku use_repo_map: true examples_as_sys_msg: true extra_params: extra_headers: anthropic-beta: prompt-caching-2024-07-31,pdfs-2024-09-25,output-128k-2025-02-19 max_tokens: 64000 cache_control: true editor_model_name: openrouter/anthropic/claude-3.7-sonnet editor_edit_format: editor-diff - name: openrouter/anthropic/claude-3.7-sonnet:beta edit_format: diff weak_model_name: openrouter/anthropic/claude-3-5-haiku use_repo_map: true examples_as_sys_msg: true extra_params: extra_headers: anthropic-beta: prompt-caching-2024-07-31,pdfs-2024-09-25,output-128k-2025-02-19 max_tokens: 64000 cache_control: true editor_model_name: openrouter/anthropic/claude-3.7-sonnet editor_edit_format: editor-diff - name: openrouter/deepseek/deepseek-chat edit_format: diff use_repo_map: true reminder: sys examples_as_sys_msg: true - name: openrouter/deepseek/deepseek-coder edit_format: diff use_repo_map: true reminder: sys examples_as_sys_msg: true - name: openrouter/deepseek/deepseek-r1 edit_format: diff weak_model_name: openrouter/deepseek/deepseek-chat use_repo_map: true examples_as_sys_msg: true extra_params: max_tokens: 8192 caches_by_default: true use_temperature: false editor_model_name: openrouter/deepseek/deepseek-chat editor_edit_format: editor-diff - name: openrouter/deepseek/deepseek-r1-distill-llama-70b edit_format: diff weak_model_name: openrouter/deepseek/deepseek-chat use_repo_map: true examples_as_sys_msg: true extra_params: max_tokens: 8192 caches_by_default: true use_temperature: false editor_model_name: openrouter/deepseek/deepseek-chat editor_edit_format: editor-diff - name: openrouter/deepseek/deepseek-r1:free edit_format: diff weak_model_name: openrouter/deepseek/deepseek-r1:free use_repo_map: true examples_as_sys_msg: true extra_params: max_tokens: 8192 caches_by_default: true use_temperature: false editor_model_name: openrouter/deepseek/deepseek-r1:free editor_edit_format: editor-diff - name: openrouter/meta-llama/llama-3-70b-instruct edit_format: diff weak_model_name: openrouter/meta-llama/llama-3-70b-instruct examples_as_sys_msg: true - name: openrouter/openai/gpt-4o edit_format: diff weak_model_name: openrouter/openai/gpt-4o-mini use_repo_map: true lazy: true reminder: sys examples_as_sys_msg: true editor_edit_format: editor-diff - name: openrouter/openai/o1 edit_format: diff weak_model_name: openrouter/openai/gpt-4o-mini use_repo_map: true use_temperature: false streaming: false editor_model_name: openrouter/openai/gpt-4o editor_edit_format: editor-diff system_prompt_prefix: 'Formatting re-enabled. ' - name: openrouter/openai/o1-mini weak_model_name: openrouter/openai/gpt-4o-mini use_repo_map: true use_system_prompt: false use_temperature: false streaming: false editor_model_name: openrouter/openai/gpt-4o editor_edit_format: editor-diff - name: openrouter/openai/o1-preview edit_format: diff weak_model_name: openrouter/openai/gpt-4o-mini use_repo_map: true use_system_prompt: false use_temperature: false streaming: false editor_model_name: openrouter/openai/gpt-4o editor_edit_format: editor-diff - name: openrouter/openai/o3-mini edit_format: diff weak_model_name: openrouter/openai/gpt-4o-mini use_repo_map: true use_temperature: false editor_model_name: openrouter/openai/gpt-4o editor_edit_format: editor-diff system_prompt_prefix: 'Formatting re-enabled. ' - name: openrouter/openai/o3-mini-high edit_format: diff weak_model_name: openrouter/openai/gpt-4o-mini use_repo_map: true use_temperature: false editor_model_name: openrouter/openai/gpt-4o editor_edit_format: editor-diff system_prompt_prefix: 'Formatting re-enabled. ' - name: openrouter/qwen/qwen-2.5-coder-32b-instruct edit_format: diff weak_model_name: openrouter/qwen/qwen-2.5-coder-32b-instruct use_repo_map: true editor_model_name: openrouter/qwen/qwen-2.5-coder-32b-instruct editor_edit_format: editor-diff - name: vertex_ai-anthropic_models/vertex_ai/claude-3-7-sonnet@20250219 edit_format: diff weak_model_name: vertex_ai/claude-3-5-haiku@20241022 use_repo_map: true examples_as_sys_msg: true extra_params: max_tokens: 64000 editor_model_name: vertex_ai-anthropic_models/vertex_ai/claude-3-7-sonnet@20250219 editor_edit_format: editor-diff - name: vertex_ai/claude-3-5-haiku@20241022 edit_format: diff weak_model_name: vertex_ai/claude-3-5-haiku@20241022 use_repo_map: true extra_params: max_tokens: 4096 - name: vertex_ai/claude-3-5-sonnet-v2@20241022 edit_format: diff weak_model_name: vertex_ai/claude-3-5-haiku@20241022 use_repo_map: true examples_as_sys_msg: true extra_params: max_tokens: 8192 editor_model_name: vertex_ai/claude-3-5-sonnet-v2@20241022 editor_edit_format: editor-diff - name: vertex_ai/claude-3-5-sonnet@20240620 edit_format: diff weak_model_name: vertex_ai/claude-3-5-haiku@20241022 use_repo_map: true examples_as_sys_msg: true extra_params: max_tokens: 8192 editor_model_name: vertex_ai/claude-3-5-sonnet@20240620 editor_edit_format: editor-diff - name: vertex_ai/claude-3-7-sonnet@20250219 edit_format: diff weak_model_name: vertex_ai/claude-3-5-haiku@20241022 use_repo_map: true examples_as_sys_msg: true extra_params: max_tokens: 64000 editor_model_name: vertex_ai/claude-3-7-sonnet@20250219 editor_edit_format: editor-diff - name: vertex_ai/claude-3-opus@20240229 edit_format: diff weak_model_name: vertex_ai/claude-3-5-haiku@20241022 use_repo_map: true - name: vertex_ai/claude-3-sonnet@20240229 weak_model_name: vertex_ai/claude-3-5-haiku@20241022 - name: vertex_ai/gemini-pro-experimental edit_format: diff-fenced use_repo_map: true . ",
    "url": "/docs/config/adv-model-settings.html#model-settings",
    
    "relUrl": "/docs/config/adv-model-settings.html#model-settings"
  },"10": {
    "doc": "Analytics",
    "title": "Analytics",
    "content": "Llmcode can collect anonymous analytics to help improve llmcode’s ability to work with LLMs, edit code and complete user requests. ",
    "url": "/docs/more/analytics.html",
    
    "relUrl": "/docs/more/analytics.html"
  },"11": {
    "doc": "Analytics",
    "title": "Opt-in, anonymous, no personal info",
    "content": "Analytics are only collected if you agree and opt-in. Llmcode respects your privacy and never collects your code, chat messages, keys or personal info. Llmcode collects information on: . | which LLMs are used and with how many tokens, | which of llmcode’s edit formats are used, | how often features and commands are used, | information about exceptions and errors, | etc | . These analytics are associated with an anonymous, randomly generated UUID4 user identifier. This information helps improve llmcode by identifying which models, edit formats, features and commands are most used. It also helps uncover bugs that users are experiencing, so that they can be fixed in upcoming releases. ",
    "url": "/docs/more/analytics.html#opt-in-anonymous-no-personal-info",
    
    "relUrl": "/docs/more/analytics.html#opt-in-anonymous-no-personal-info"
  },"12": {
    "doc": "Analytics",
    "title": "Disabling analytics",
    "content": "You can opt out of analytics forever by running this command one time: . llmcode --analytics-disable . ",
    "url": "/docs/more/analytics.html#disabling-analytics",
    
    "relUrl": "/docs/more/analytics.html#disabling-analytics"
  },"13": {
    "doc": "Analytics",
    "title": "Enabling analytics",
    "content": "The --[no-]analytics switch controls whether analytics are enabled for the current session: . | --analytics will turn on analytics for the current session. This will not have any effect if you have permanently disabled analytics with --analytics-disable. If this is the first time you have enabled analytics, llmcode will confirm you wish to opt-in to analytics. | --no-analytics will turn off analytics for the current session. | By default, if you don’t provide --analytics or --no-analytics, llmcode will enable analytics for a random subset of users. This will never happen if you have permanently disabled analytics with --analytics-disable. Randomly selected users will be asked if they wish to opt-in to analytics. | . ",
    "url": "/docs/more/analytics.html#enabling-analytics",
    
    "relUrl": "/docs/more/analytics.html#enabling-analytics"
  },"14": {
    "doc": "Analytics",
    "title": "Opting in",
    "content": "The first time analytics are enabled, you will need to agree to opt-in. llmcode --analytics Llmcode respects your privacy and never collects your code, prompts, chats, keys or any personal info. For more info: https://llmcode.khulnasoft.com/docs/more/analytics.html Allow collection of anonymous analytics to help improve llmcode? (Y)es/(N)o [Yes]: . If you say “no”, analytics will be permanently disabled. ",
    "url": "/docs/more/analytics.html#opting-in",
    
    "relUrl": "/docs/more/analytics.html#opting-in"
  },"15": {
    "doc": "Analytics",
    "title": "Details about data being collected",
    "content": "Sample analytics data . To get a better sense of what type of data is collected, you can review some sample analytics logs. These are the last 1,000 analytics events from the author’s personal use of llmcode, updated regularly. Analytics code . Since llmcode is open source, all the places where llmcode collects analytics are visible in the source code. They can be viewed using GitHub search. Logging and inspecting analytics . You can get a full log of the analytics that llmcode is collecting, in case you would like to audit or inspect this data. llmcode --analytics-log filename.jsonl . If you want to just log analytics without reporting them, you can do: . llmcode --analytics-log filename.jsonl --no-analytics . ",
    "url": "/docs/more/analytics.html#details-about-data-being-collected",
    
    "relUrl": "/docs/more/analytics.html#details-about-data-being-collected"
  },"16": {
    "doc": "Analytics",
    "title": "Reporting issues",
    "content": "If you have concerns about any of the analytics that llmcode is collecting or our data practices please contact us by opening a GitHub Issue. ",
    "url": "/docs/more/analytics.html#reporting-issues",
    
    "relUrl": "/docs/more/analytics.html#reporting-issues"
  },"17": {
    "doc": "Analytics",
    "title": "Privacy policy",
    "content": "Please see llmcode’s privacy policy for more details. ",
    "url": "/docs/more/analytics.html#privacy-policy",
    
    "relUrl": "/docs/more/analytics.html#privacy-policy"
  },"18": {
    "doc": "Anthropic",
    "title": "Anthropic",
    "content": "To work with Anthropic’s models, you need to provide your Anthropic API key either in the ANTHROPIC_API_KEY environment variable or via the --anthropic-api-key command line switch. Llmcode has some built in shortcuts for the most popular Anthropic models and has been tested and benchmarked to work well with them: . python -m pip install -U llmcode export ANTHROPIC_API_KEY=&lt;key&gt; # Mac/Linux setx ANTHROPIC_API_KEY &lt;key&gt; # Windows, restart shell after setx # Llmcode uses Claude 3.5 Sonnet by default (or use --sonnet) llmcode # Claude 3 Opus llmcode --opus # List models available from Anthropic llmcode --list-models anthropic/ . Anthropic has very low rate limits. You can access all the Anthropic models via OpenRouter or Google Vertex AI with more generous rate limits. You can use llmcode --model &lt;model-name&gt; to use any other Anthropic model. For example, if you want to use a specific version of Opus you could do llmcode --model claude-3-opus-20240229. ",
    "url": "/docs/llms/anthropic.html",
    
    "relUrl": "/docs/llms/anthropic.html"
  },"19": {
    "doc": "Anthropic",
    "title": "Thinking tokens",
    "content": "Llmcode can work with Sonnet 3.7’s new thinking tokens, but does not ask Sonnet to use thinking tokens by default. Enabling thinking currently requires manual configuration. You need to add the following to your .llmcode.model.settings.yml model settings file. Adjust the budget_tokens value to change the target number of thinking tokens. - name: anthropic/claude-3-7-sonnet-20250219 edit_format: diff weak_model_name: anthropic/claude-3-5-haiku-20241022 use_repo_map: true examples_as_sys_msg: true use_temperature: false extra_params: extra_headers: anthropic-beta: prompt-caching-2024-07-31,pdfs-2024-09-25,output-128k-2025-02-19 max_tokens: 64000 thinking: type: enabled budget_tokens: 32000 # Adjust this number cache_control: true editor_model_name: anthropic/claude-3-7-sonnet-20250219 editor_edit_format: editor-diff . More streamlined support will be coming soon. ",
    "url": "/docs/llms/anthropic.html#thinking-tokens",
    
    "relUrl": "/docs/llms/anthropic.html#thinking-tokens"
  },"20": {
    "doc": "API Keys",
    "title": "API Keys",
    "content": "Llmcode lets you specify API keys in a few ways: . | On the command line | As environment variables | In a .env file | In your .llmcode.conf.yml config file | . ",
    "url": "/docs/config/api-keys.html",
    
    "relUrl": "/docs/config/api-keys.html"
  },"21": {
    "doc": "API Keys",
    "title": "OpenAI and Anthropic",
    "content": "Llmcode has special support for providing OpenAI and Anthropic API keys via dedicated switches and configuration options. Settings keys for other providers works a bit differently, see below. Command line . You can set OpenAI and Anthropic API keys via command line switches --openai-api-key and --anthropic-api-key. Environment variables or .env file . You can also store them in environment variables or a .env file, which also works for every API provider: . OPENAI_API_KEY=&lt;key&gt; ANTHROPIC_API_KEY=&lt;key&gt; . Yaml config file . You can also set those API keys via special entries in the yaml config file, like this: . openai-api-key: &lt;key&gt; anthropic-api-key: &lt;key&gt; . ",
    "url": "/docs/config/api-keys.html#openai-and-anthropic",
    
    "relUrl": "/docs/config/api-keys.html#openai-and-anthropic"
  },"22": {
    "doc": "API Keys",
    "title": "Other API providers",
    "content": "All other LLM providers can use one of these other methods to set their API keys. Command line . Use --api-key provider=&lt;key&gt; which has the effect of setting the environment variable PROVIDER_API_KEY=&lt;key&gt;. So --api-key gemini=xxx would set GEMINI_API_KEY=xxx. Environment variables or .env file . You can set API keys in environment variables. The .env file is a great place to store your API keys and other provider API environment variables: . GEMINI_API_KEY=foo OPENROUTER_API_KEY=bar DEEPSEEK_API_KEY=baz . Yaml config file . You can also set API keys in the .llmcode.conf.yml file via the api-key entry: . api-key: - gemini=foo # Sets env var GEMINI_API_KEY=foo - openrouter=bar # Sets env var OPENROUTER_API_KEY=bar - deepseek=baz # Sets env var DEEPSEEK_API_KEY=baz . ",
    "url": "/docs/config/api-keys.html#other-api-providers",
    
    "relUrl": "/docs/config/api-keys.html#other-api-providers"
  },"23": {
    "doc": "Editing an asciinema cast file with llmcode",
    "title": "Editing an asciinema cast file with llmcode",
    "content": "The user needs to make some tedious edits to the escape sequences in an asciinema screencast file. $ llmcode hello.cast Added hello.cast to the chat . when the “hello.py&gt; “ prompt is displayed, it’s including some escape codes to add a few blank lines below the prompt and then bringing the cursor back up. remove all that. just show the prompt followed by a space and leave the cursor showing right after that. hello.cast &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL [19.534013, \"o\", \"\\u001b[?2004h\\u001b[?1l\\u001b[?25l\\u001b[0m\\u001b[?7l\\u001b[0m\\u001b[J\\u001b[0;38;5;28mhello.py&gt; \\u001b[0m\\r\\r\\n\\r\\r\\n\\r\\r\\n\\u001b[0;38;5;28m \\r\\u001b[64C \\r\\u001b[3A\\u001b[10C\\u001b[?7h\\u001b[0m\\u001b[?12l\\u001b[?25h\"] ======= [19.534013, \"o\", \"\\u001b[?2004h\\u001b[?1l\\u001b[?25l\\u001b[0m\\u001b[?7l\\u001b[0m\\u001b[J\\u001b[0;38;5;28mhello.py&gt; \\u001b[0m\\u001b[?7h\\u001b[0m\\u001b[?12l\\u001b[?25h\"] &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . Applied edit to hello.cast Commit a3fa4d2 llmcode: Removed escape codes to display prompt and cursor correctly. ",
    "url": "/examples/asciinema.html",
    
    "relUrl": "/examples/asciinema.html"
  },"24": {
    "doc": "Azure",
    "title": "Azure",
    "content": "Llmcode can connect to the OpenAI models on Azure. python -m pip install -U llmcode # Mac/Linux: export AZURE_API_KEY=&lt;key&gt; export AZURE_API_VERSION=2023-05-15 export AZURE_API_BASE=https://myendpt.openai.azure.com # Windows setx AZURE_API_KEY &lt;key&gt; setx AZURE_API_VERSION 2023-05-15 setx AZURE_API_BASE https://myendpt.openai.azure.com # ... restart your shell after setx commands llmcode --model azure/&lt;your_deployment_name&gt; # List models available from Azure llmcode --list-models azure/ . Note that llmcode will also use environment variables like AZURE_OPENAI_API_xxx. ",
    "url": "/docs/llms/azure.html",
    
    "relUrl": "/docs/llms/azure.html"
  },"25": {
    "doc": "Amazon Bedrock",
    "title": "Amazon Bedrock",
    "content": "Llmcode can connect to models provided by Amazon Bedrock. You will need to have an AWS account with access to the Bedrock service. To configure Llmcode to use the Amazon Bedrock API, you need to set up your AWS credentials. This can be done using the AWS CLI or by setting environment variables. ",
    "url": "/docs/llms/bedrock.html",
    
    "relUrl": "/docs/llms/bedrock.html"
  },"26": {
    "doc": "Amazon Bedrock",
    "title": "Select a Model from Amazon Bedrock",
    "content": "Before you can use a model through Amazon Bedrock, you must “enable” the model under the Model Access screen in the AWS Management Console. To find the Model ID, open the Model Catalog area in the Bedrock console, select the model you want to use, and the find the modelId property under the “Usage” heading. Bedrock Inference Profiles . Amazon Bedrock has added support for a new feature called cross-region “inference profiles.” Some models hosted in Bedrock only support these inference profiles. If you’re using one of these models, then you will need to use the Inference Profile ID instead of the Model ID from the Model Catalog screen, in the AWS Management Console. For example, the Claude Sonnet 3.7 model, release in February 2025, exclusively supports inference through inference profiles. To use this model, you would use the us.anthropic.claude-3-7-sonnet-20250219-v1:0 Inference Profile ID. In the Amazon Bedrock console, go to Inference and Assessment ➡️ Cross-region Inference to find the Inference Profile ID value. If you attempt to use a Model ID for a model that exclusively supports the Inference Profile feature, you will receive an error message like the following: . litellm.BadRequestError: BedrockException - b’{“message”:”Invocation of model ID anthropic.claude-3-7-sonnet-20250219-v1:0 with on-demand throughput isn\\xe2\\x80\\x99t supported. Retry your request with the ID or ARN of an inference profile that contains this model.”}’ . ",
    "url": "/docs/llms/bedrock.html#select-a-model-from-amazon-bedrock",
    
    "relUrl": "/docs/llms/bedrock.html#select-a-model-from-amazon-bedrock"
  },"27": {
    "doc": "Amazon Bedrock",
    "title": "AWS CLI Configuration",
    "content": "If you haven’t already, install the AWS CLI and configure it with your credentials: . aws configure . This will prompt you to enter your AWS Access Key ID, Secret Access Key, and default region. ",
    "url": "/docs/llms/bedrock.html#aws-cli-configuration",
    
    "relUrl": "/docs/llms/bedrock.html#aws-cli-configuration"
  },"28": {
    "doc": "Amazon Bedrock",
    "title": "Environment Variables",
    "content": "Alternatively, you can set the following environment variables: . export AWS_REGION=your_preferred_region # For user authentication export AWS_ACCESS_KEY_ID=your_access_key export AWS_SECRET_ACCESS_KEY=your_secret_key # For profile authentication export AWS_PROFILE=your-profile . You can add these to your .env file. Set Environment Variables with PowerShell . If you’re using PowerShell on MacOS, Linux, or Windows, you can set the same AWS configuration environment variables with these commands. $env:AWS_ACCESS_KEY_ID = 'your_access_key' $env:AWS_SECRET_ACCESS_KEY = 'your_secret_key' $env:AWS_REGION = 'us-west-2' # Put whichever AWS region that you'd like, that the Bedrock service supports. ",
    "url": "/docs/llms/bedrock.html#environment-variables",
    
    "relUrl": "/docs/llms/bedrock.html#environment-variables"
  },"29": {
    "doc": "Amazon Bedrock",
    "title": "Install boto3",
    "content": "The AWS Bedrock provider requires the boto3 package in order to function correctly: . pip install boto3 . To use llmcode installed via pipx with AWS Bedrock, you must add the boto3 dependency to llmcode’s virtual environment by running . pipx inject llmcode boto3 . You must install boto3 dependency to llmcode’s virtual environment installed via one-liner or uv by running . uv tool run --from llmcode pip install boto3 . ",
    "url": "/docs/llms/bedrock.html#install-boto3",
    
    "relUrl": "/docs/llms/bedrock.html#install-boto3"
  },"30": {
    "doc": "Amazon Bedrock",
    "title": "Running Llmcode with Bedrock",
    "content": "Once your AWS credentials are set up, you can run Llmcode with the --model command line switch, specifying the Bedrock model you want to use: . llmcode --model bedrock/anthropic.claude-3-5-sonnet-20240620-v1:0 . Sometimes it seems to help if you prefix the model name with “us.”: . llmcode --model bedrock/us.anthropic.claude-3-5-sonnet-20240620-v1:0 . ",
    "url": "/docs/llms/bedrock.html#running-llmcode-with-bedrock",
    
    "relUrl": "/docs/llms/bedrock.html#running-llmcode-with-bedrock"
  },"31": {
    "doc": "Amazon Bedrock",
    "title": "Available Models",
    "content": "To see some models available via Bedrock, run: . llmcode --list-models bedrock/ . Make sure you have access to these models in your AWS account before attempting to use them with Llmcode. ",
    "url": "/docs/llms/bedrock.html#available-models",
    
    "relUrl": "/docs/llms/bedrock.html#available-models"
  },"32": {
    "doc": "Amazon Bedrock",
    "title": "More info",
    "content": "For more information on Amazon Bedrock and its models, refer to the official AWS documentation. Also, see the litellm docs on Bedrock. ",
    "url": "/docs/llms/bedrock.html#more-info",
    
    "relUrl": "/docs/llms/bedrock.html#more-info"
  },"33": {
    "doc": "The January GPT-4 Turbo is lazier than the last version",
    "title": "The January GPT-4 Turbo is lazier than the last version",
    "content": ". OpenAI just released a new version of GPT-4 Turbo. This new model is intended to reduce the “laziness” that has been widely observed with the previous gpt-4-1106-preview model: . Today, we are releasing an updated GPT-4 Turbo preview model, gpt-4-0125-preview. This model completes tasks like code generation more thoroughly than the previous preview model and is intended to reduce cases of “laziness” where the model doesn’t complete a task. With that in mind, I’ve been benchmarking the new model using llmcode’s existing lazy coding benchmark. ",
    "url": "/docs/benchmarks-0125.html#the-january-gpt-4-turbo-is-lazier-than-the-last-version",
    
    "relUrl": "/docs/benchmarks-0125.html#the-january-gpt-4-turbo-is-lazier-than-the-last-version"
  },"34": {
    "doc": "The January GPT-4 Turbo is lazier than the last version",
    "title": "Benchmark results",
    "content": "Overall, the new gpt-4-0125-preview model seems lazier than the November gpt-4-1106-preview model: . | It gets worse benchmark scores when using the unified diffs code editing format. | Using llmcode’s older SEARCH/REPLACE block editing format, the new January model outperforms the older November model. But it still performs worse than both models using unified diffs. | . ",
    "url": "/docs/benchmarks-0125.html#benchmark-results",
    
    "relUrl": "/docs/benchmarks-0125.html#benchmark-results"
  },"35": {
    "doc": "The January GPT-4 Turbo is lazier than the last version",
    "title": "Related reports",
    "content": "This is one in a series of reports that use the llmcode benchmarking suite to assess and compare the code editing capabilities of OpenAI’s GPT models. You can review the other reports for additional information: . | GPT code editing benchmarks evaluates the March and June versions of GPT-3.5 and GPT-4. | Code editing benchmarks for OpenAI’s “1106” models. | Llmcode’s lazy coding benchmark. | . ",
    "url": "/docs/benchmarks-0125.html#related-reports",
    
    "relUrl": "/docs/benchmarks-0125.html#related-reports"
  },"36": {
    "doc": "The January GPT-4 Turbo is lazier than the last version",
    "title": "The January GPT-4 Turbo is lazier than the last version",
    "content": " ",
    "url": "/docs/benchmarks-0125.html",
    
    "relUrl": "/docs/benchmarks-0125.html"
  },"37": {
    "doc": "Code editing benchmarks for OpenAI's \"1106\" models",
    "title": "Code editing benchmarks for OpenAI’s “1106” models",
    "content": ". OpenAI just released new versions of GPT-3.5 and GPT-4, and there’s a lot of interest about their ability to code compared to the previous versions. With that in mind, I’ve been benchmarking the new models. Llmcode is an open source command line chat tool that lets you work with GPT to edit code in your local git repo. To do this, llmcode needs to be able to reliably recognize when GPT wants to edit your source code, determine which files it wants to modify and accurately apply the changes it’s trying to make. Doing a good job on this “code editing” task requires a good LLM, good prompting and a good tool driving the interactions with the LLM. Llmcode relies on a code editing benchmark to quantitatively evaluate performance whenever one of these things changes. For example, whenever I change llmcode’s prompting or the backend which drives LLM conversations, I run the benchmark to make sure these changes produce improvements (not regressions). The benchmark uses llmcode to try and complete 133 Exercism Python coding exercises. For each exercise, Exercism provides a starting python file with stubs for the needed functions, a natural language description of the problem to solve and a test suite to evaluate whether the coder has correctly solved the problem. The benchmark gives llmcode two tries to complete the task: . | On the first try, llmcode gives GPT the stub code file to edit and the natural language instructions that describe the problem. This reflects how you code with llmcode. You add your source code files to the chat and ask for changes, which are automatically applied. | If the test suite fails after the first try, llmcode gives GPT the test error output and asks it to fix the code. Llmcode supports this sort of interaction using a command like /run pytest to run and share pytest results in the chat with GPT. You can /run whatever tests/linters/etc make sense for your language/framework/situation. | . ",
    "url": "/docs/benchmarks-1106.html#code-editing-benchmarks-for-openais-1106-models",
    
    "relUrl": "/docs/benchmarks-1106.html#code-editing-benchmarks-for-openais-1106-models"
  },"38": {
    "doc": "Code editing benchmarks for OpenAI's \"1106\" models",
    "title": "Benchmark results",
    "content": "gpt-4-1106-preview . For now, I have only benchmarked the GPT-4 models using the diff edit method. This is the edit format that llmcode uses by default with gpt-4. | The new gpt-4-1106-preview model seems 2-2.5X faster than the June GPT-4 model. | It seems better at producing correct code on the first try. It gets 53% of the coding exercises correct, without needing to see errors from the test suite. Previous models only get 46-47% of the exercises correct on the first try. | The new model seems to perform similar (~65%) to the old models (63-64%) after their second chance to correct bugs by reviewing test suite error output. | . gpt-3.5-turbo-1106 . I benchmarked the GPT-3.5 models with both the whole and diff edit format. None of the gpt-3.5 models seem able to effectively use the diff edit format, including the newest November (1106) model. The comments below only focus on comparing the whole edit format results: . | The new gpt-3.5-turbo-1106 model is completing the benchmark 3-4X faster than the earlier GPT-3.5 models. | The success rate after the first try of 42% is comparable to the previous June (0613) model. The new November and previous June models are both worse than the original March (0301) model’s 50% result on the first try. | The new model’s 56% success rate after the second try seems comparable to the original March model, and somewhat better than the June model’s 50% score. | . ",
    "url": "/docs/benchmarks-1106.html#benchmark-results",
    
    "relUrl": "/docs/benchmarks-1106.html#benchmark-results"
  },"39": {
    "doc": "Code editing benchmarks for OpenAI's \"1106\" models",
    "title": "Related reports",
    "content": "This is one in a series of reports that use the llmcode benchmarking suite to assess and compare the code editing capabilities of OpenAI’s GPT models. You can review the other reports for additional information: . | GPT code editing benchmarks evaluates the March and June versions of GPT-3.5 and GPT-4. | Code editing speed benchmarks for OpenAI’s “1106” models compares the performance of the new GPT models. | . ",
    "url": "/docs/benchmarks-1106.html#related-reports",
    
    "relUrl": "/docs/benchmarks-1106.html#related-reports"
  },"40": {
    "doc": "Code editing benchmarks for OpenAI's \"1106\" models",
    "title": "Updates",
    "content": "Last updated 11/14/23. OpenAI has relaxed rate limits so these results are no longer considered preliminary. ",
    "url": "/docs/benchmarks-1106.html#updates",
    
    "relUrl": "/docs/benchmarks-1106.html#updates"
  },"41": {
    "doc": "Code editing benchmarks for OpenAI's \"1106\" models",
    "title": "Code editing benchmarks for OpenAI's \"1106\" models",
    "content": " ",
    "url": "/docs/benchmarks-1106.html",
    
    "relUrl": "/docs/benchmarks-1106.html"
  },"42": {
    "doc": "Speed benchmarks of GPT-4 Turbo and gpt-3.5-turbo-1106",
    "title": "Speed benchmarks of GPT-4 Turbo and gpt-3.5-turbo-1106",
    "content": ". OpenAI just released new versions of GPT-3.5 and GPT-4, and there’s a lot of interest about their capabilities and performance. With that in mind, I’ve been benchmarking the new models. Llmcode is an open source command line chat tool that lets you work with GPT to edit code in your local git repo. Llmcode relies on a code editing benchmark to quantitatively evaluate performance. This is the latest in a series of reports that use the llmcode benchmarking suite to assess and compare the code editing capabilities of OpenAI’s GPT models. You can review previous reports to get more background on llmcode’s benchmark suite: . | GPT code editing benchmarks evaluates the March and June versions of GPT-3.5 and GPT-4. | Code editing skill benchmarks for OpenAI’s “1106” models compares the olders models to the November (1106) models. | . ",
    "url": "/docs/benchmarks-speed-1106.html#speed-benchmarks-of-gpt-4-turbo-and-gpt-35-turbo-1106",
    
    "relUrl": "/docs/benchmarks-speed-1106.html#speed-benchmarks-of-gpt-4-turbo-and-gpt-35-turbo-1106"
  },"43": {
    "doc": "Speed benchmarks of GPT-4 Turbo and gpt-3.5-turbo-1106",
    "title": "Speed",
    "content": "This report compares the speed of the various GPT models. Llmcode’s benchmark measures the response time of the OpenAI chat completion endpoint each time it asks GPT to solve a programming exercise in the benchmark suite. These results measure only the time spent waiting for OpenAI to respond to the prompt. So they are measuring how fast these models can generate responses which primarily consist of source code. Some observations: . | GPT-3.5 got 6-11x faster. The gpt-3.5-turbo-1106 model is 6-11x faster than the June (0613) version which has been the default gpt-3.5-turbo model. | GPT-4 Turbo is 2-2.5x faster. The new gpt-4-1106-preview model is 2-2.5x faster than the June (0613) version which has been the default gpt-4 model. | The old March (0301) version of GPT-3.5 is actually faster than the June (0613) version. This was a surprising discovery. | . ",
    "url": "/docs/benchmarks-speed-1106.html#speed",
    
    "relUrl": "/docs/benchmarks-speed-1106.html#speed"
  },"44": {
    "doc": "Speed benchmarks of GPT-4 Turbo and gpt-3.5-turbo-1106",
    "title": "Updates",
    "content": "Last updated 11/14/23. OpenAI has relaxed rate limits so these results are no longer considered preliminary. ",
    "url": "/docs/benchmarks-speed-1106.html#updates",
    
    "relUrl": "/docs/benchmarks-speed-1106.html#updates"
  },"45": {
    "doc": "Speed benchmarks of GPT-4 Turbo and gpt-3.5-turbo-1106",
    "title": "Speed benchmarks of GPT-4 Turbo and gpt-3.5-turbo-1106",
    "content": " ",
    "url": "/docs/benchmarks-speed-1106.html",
    
    "relUrl": "/docs/benchmarks-speed-1106.html"
  },"46": {
    "doc": "GPT code editing benchmarks",
    "title": "GPT code editing benchmarks",
    "content": ". Llmcode is an open source command line chat tool that lets you work with GPT to edit code in your local git repo. To do this, llmcode needs to be able to reliably recognize when GPT wants to edit local files, determine which files it wants to modify and what changes to save. Such automated code editing hinges on using the system prompt to tell GPT how to structure code edits in its responses. Llmcode currently asks GPT to use simple text based “edit formats”, but OpenAI’s new function calling API looks like a promising way to create more structured edit formats. After implementing a couple of function based edit formats, I wanted to measure the potential benefits of switching llmcode to use them by default. With this in mind, I developed a benchmark based on the Exercism python coding exercises. This benchmark evaluates how effectively llmcode and GPT can translate a natural language coding request into executable code saved into files that pass unit tests. It provides an end-to-end evaluation of not just GPT’s coding ability, but also its capacity to edit existing code and format those code edits so that llmcode can save the edits to the local source files. I ran the benchmark on all the ChatGPT models (except gpt-4-32k), using a variety of edit formats. The results were interesting: . | Plain text edit formats worked best. Asking GPT to return an updated copy of the whole file in a standard markdown fenced code block proved to be the most reliable and effective edit format across all GPT-3.5 and GPT-4 models. The results for this whole edit format are shown in solid blue in the graph. | Function calls performed worse. Using the new functions API for edits performed worse than the above whole file method, for all the models. GPT-3.5 especially produced inferior code and frequently mangled this output format. This was surprising, as the functions API was introduced to enhance the reliability of structured outputs. The results for these ...-func edit methods are shown as patterned bars in the graph (both green and blue). | The new June GPT-3.5 models did a bit worse than the old June model. The performance of the new June (0613) versions of GPT-3.5 appears to be a bit worse than the February (0301) version. This is visible if you look at the “first attempt” markers on the first three solid blue bars and also by comparing the first three solid green diff bars. | GPT-4 does better than GPT-3.5, as expected. | . The quantitative benchmark results agree with my intuitions about prompting GPT for complex tasks like coding. It’s beneficial to minimize the “cognitive overhead” of formatting the response, allowing GPT to concentrate on the coding task at hand. As a thought experiment, imagine a slack conversation with a editor developer where you ask them to write the code to add some new feature to your app. They’re going to type the response back to you by hand in the chat. Should they type out the code and wrap it in a normal markdown code block? Or should they type up a properly escaped and syntactically correct json data structure that contains the text of the new code? . Using more complex output formats with GPT seems to cause two issues: . | It makes GPT write worse code. Keeping the output format simple seems to allow GPT to devote more attention to the actual coding task. | It reduces GPT’s adherence to the output format, making it more challenging for tools like llmcode to accurately identify and apply the edits GPT is attempting to make. | . I was expecting to start using function call based edits in llmcode for both GPT-3.5 and GPT-4. But given these benchmark results, I won’t be adopting the functions API at this time. I will certainly plan to benchmark functions again with future versions of the models. More details on the benchmark, edit formats and results are discussed below. ",
    "url": "/docs/benchmarks.html#gpt-code-editing-benchmarks",
    
    "relUrl": "/docs/benchmarks.html#gpt-code-editing-benchmarks"
  },"47": {
    "doc": "GPT code editing benchmarks",
    "title": "The benchmark",
    "content": "The benchmark uses 133 practice exercises from the Exercism python repository. These exercises were designed to help individuals learn Python and hone their coding skills. Each exercise includes: . | Instructions, provided in markdown files. | Stub python code in an implementation file, specifying the functions or classes that need to be implemented. | Unit tests in a separate python file. | . The goal is for GPT to read the instructions, implement the provided function/class skeletons and pass all the unit tests. The benchmark measures what percentage of the 133 exercises are completed successfully, causing all the associated unit tests to pass. To start each exercise, llmcode sends GPT the initial contents of the implementation file, the Exercism instructions and a final instruction: . Use the above instructions to modify the supplied files: &lt;implementation file&gt; Keep and implement the existing function or class stubs, they will be called from unit tests. Only use standard python libraries, don't suggest installing any packages. Llmcode updates the implementation file based on GPT’s reply and runs the unit tests. If all tests pass, the exercise is considered complete. If some tests fail, llmcode sends GPT a second message with the test error output. It only sends the first 50 lines of test errors to try and avoid exceeding the context window of the smaller models. Llmcode also includes this final instruction: . See the testing errors above. The tests are correct. Fix the code in &lt;implementation file&gt; to resolve the errors. Requiring GPT to fix its first implementation in response to test failures is another way in which this benchmark stresses code editing skill. This second chance is also important because it gives GPT the opportunity to adjust if the instructions were imprecise with respect to the specific requirements of the unit tests. Many of the exercises have multiple paragraphs of instructions, and most human coders would likely fail some tests on their first try. The bars in the graph show the percent of exercises that were completed by each model and edit format combination. The full bar height represents the final outcome following both coding attempts. Each bar also has a horizontal mark that shows the intermediate performance after the first coding attempt, without the benefit of the second try that includes the test error output. It’s worth noting that GPT never gets to see the source code of the unit tests during the benchmark. It only sees the error output from failed tests. Of course, all of this code was probably part of its original training data! . In summary, passing an exercise means GPT was able to: . | Write the required code (possibly after reviewing test error output), | Correctly package all of the code edits into the edit format so that llmcode can process and save it to the implementation file. | . Conversely, failing an exercise only requires a breakdown in one of those steps. In practice, GPT fails at different steps in different exercises. Sometimes it simply writes the wrong code. Other times, it fails to format the code edits in a way that conforms to the edit format, resulting in the code not being saved correctly. It’s worth keeping in mind that changing the edit format often affects both aspects of GPT’s performance. Complex edit formats often lead GPT to write worse code and make it less successful at formatting the edits correctly. ",
    "url": "/docs/benchmarks.html#the-benchmark",
    
    "relUrl": "/docs/benchmarks.html#the-benchmark"
  },"48": {
    "doc": "GPT code editing benchmarks",
    "title": "Edit formats",
    "content": "I benchmarked 4 different edit formats, described below. Each description includes a sample response that GPT might provide to a user who requests: “Change the print from hello to goodbye.” . whole . The whole format asks GPT to return an updated copy of the entire file, including any changes. The file should be formatted with normal markdown triple-backtick fences, inlined with the rest of its response text. This format is very similar to how ChatGPT returns code snippets during normal chats, except with the addition of a filename right before the opening triple-backticks. Here is the updated copy of your file demo.py: demo.py ```python def main(): print(\"goodbye\") ``` . diff . The diff format also asks GPT to return edits as part of the normal response text, in a simple diff format. Each edit is a fenced code block that specifies the filename and a chunk of ORIGINAL and UPDATED code. GPT provides some original lines from the file and then a new updated set of lines. Here are the changes you requested to demo.py: ```python demo.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL print(\"hello\") ======= print(\"goodbye\") &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED ``` . whole-func . The whole-func format requests updated copies of whole files to be returned using the function call API. { \"explanation\": \"Changed hello to goodbye.\", \"files\": [ { \"path\": \"demo.py\", \"content\": \"def main():\\n print(\\\"goodbye\\\")\\n\" } } . diff-func . The diff-func format requests a list of original/updated style edits to be returned using the function call API. { \"explanation\": \"Changed hello to goodbye.\", \"edits\": [ { \"path\": \"demo.py\", \"original_lines\": [ \" print(\\\"hello\\\")\" ], \"updated_lines\": [ \" print(\\\"goodbye\\\")\" ], } ] } . ",
    "url": "/docs/benchmarks.html#edit-formats",
    
    "relUrl": "/docs/benchmarks.html#edit-formats"
  },"49": {
    "doc": "GPT code editing benchmarks",
    "title": "GPT-3.5’s performance",
    "content": "The 0613 models seem worse? . The GPT-3.5 benchmark results have me fairly convinced that the new gpt-3.5-turbo-0613 and gpt-3.5-16k-0613 models are a bit worse at code editing than the older gpt-3.5-turbo-0301 model. This is visible in the “first attempt” portion of each result, before GPT gets a second chance to edit the code. Look at the horizontal white line in the middle of the first three blue bars. Performance with the whole edit format was 46% for the February model and only 39% for the June models. But also note how much the solid green diff bars degrade between the February and June GPT-3.5 models. They drop from 30% down to about 19%. I saw other signs of this degraded performance in earlier versions of the benchmark as well. Pathological use of diff . When GPT-3.5 is able to correctly generate the diff edit format, it often uses it in a pathological manner. It places the entire original source file in the ORIGINAL block and the entire updated file in the UPDATED block. This is strictly worse than just using the whole edit format, as GPT is sending two full copies of the file. Hallucinated function calls . When GPT-3.5 uses the functions API it is prone to ignoring the JSON Schema that specifies valid functions. It often returns a completely novel and semantically invalid function_call fragment with \"name\": \"python\". The arguments attribute is supposed to be a set of key/value pairs with the arguments to the function specified in the name field. Instead, GPT-3.5 frequently just stuffs an entire python file into that field. \"function_call\": { \"name\": \"python\", \"arguments\": \"def main():\\n print(\\\"hello\\\")\\n\" }, . It seems like it might be getting confused by fine-tuning that was done for the ChatGPT code interpreter plugin? . ",
    "url": "/docs/benchmarks.html#gpt-35s-performance",
    
    "relUrl": "/docs/benchmarks.html#gpt-35s-performance"
  },"50": {
    "doc": "GPT code editing benchmarks",
    "title": "Randomness",
    "content": "The benchmark attempts to be deterministic, always sending identical requests for each exercise on repeated runs. As part of this effort, when sending test error output to GPT, it removes the wall-clock timing information that is normally included by the unittest module. The benchmark harness also logs SHA hashes of all the OpenAI API requests and replies. This makes it possible to detect randomness or nondeterminism in the benchmarking process. It turns out that the OpenAI chat APIs are not deterministic, even at temperature=0. The same identical request will produce multiple distinct responses, usually less than 5-10 variations. This suggests that OpenAI may be load balancing their API across a number of slightly different instances of the model? . For certain exercises, some of these variable responses pass the unit tests while other variants do not. Results for exercises like this, which are “on the bubble”, are therefore a bit random, depending on which variant OpenAI returns. Given that, it would be ideal to run all 133 exercises many times for each model/edit-format combination and report an average performance. This would average away the effect of the API variance. It would also significantly increase the cost of this sort of benchmarking. So I didn’t do that. Benchmarking against 133 exercises already provides some robustness, since we are measuring the performance across many exercises. But to get a sense of how much the API variance impacts the benchmark outcomes, I ran all 133 exercises 10 times each against gpt-3.5-turbo-0613 with the whole edit format. You’ll see one set of error bars in the graph, which show the range of results from those 10 runs. The OpenAI API randomness doesn’t seem to cause a large variance in the overall benchmark results. ",
    "url": "/docs/benchmarks.html#randomness",
    
    "relUrl": "/docs/benchmarks.html#randomness"
  },"51": {
    "doc": "GPT code editing benchmarks",
    "title": "Conclusions",
    "content": "Based on these benchmark results, llmcode will continue to use the whole edit format for GPT-3.5, and diff for GPT-4. GPT-4 gets comparable results with the whole and diff edit formats, but using whole significantly increases costs and latency compared to diff. The latency of streaming back the entire updated copy of each edited file is a real challenge with the whole format. The GPT-3.5 models are quite responsive, and can stream back entire files at reasonable speed. Llmcode displays a progress bar and live diffs of the files as they stream in, which helps pass the time. The GPT-4 models are much slower, and waiting for even small files to be completely “retyped” on each request is probably unacceptable. ",
    "url": "/docs/benchmarks.html#conclusions",
    
    "relUrl": "/docs/benchmarks.html#conclusions"
  },"52": {
    "doc": "GPT code editing benchmarks",
    "title": "GPT code editing benchmarks",
    "content": " ",
    "url": "/docs/benchmarks.html",
    
    "relUrl": "/docs/benchmarks.html"
  },"53": {
    "doc": "Llmcode in your browser",
    "title": "Llmcode in your browser",
    "content": "Llmcode browser UI demo video Use llmcode’s new experimental browser UI to collaborate with LLMs to edit code in your local git repo. Llmcode will directly edit the code in your local source files, and git commit the changes with sensible commit messages. You can start a new project or work with an existing git repo. Llmcode works well with GPT 3.5, GPT-4, GPT-4 Turbo with Vision, and Claude 3 Opus. It also supports connecting to almost any LLM. Use the --browser switch to launch the browser version of llmcode: . python -m pip install -U llmcode export OPENAI_API_KEY=&lt;key&gt; # Mac/Linux setx OPENAI_API_KEY &lt;key&gt; # Windows, restart shell after setx llmcode --browser . ",
    "url": "/docs/usage/browser.html#llmcode-in-your-browser",
    
    "relUrl": "/docs/usage/browser.html#llmcode-in-your-browser"
  },"54": {
    "doc": "Llmcode in your browser",
    "title": "Llmcode in your browser",
    "content": " ",
    "url": "/docs/usage/browser.html",
    
    "relUrl": "/docs/usage/browser.html"
  },"55": {
    "doc": "Scores by release date",
    "title": "LLM code editing skill by model release date",
    "content": ". ",
    "url": "/docs/leaderboards/by-release-date.html#llm-code-editing-skill-by-model-release-date",
    
    "relUrl": "/docs/leaderboards/by-release-date.html#llm-code-editing-skill-by-model-release-date"
  },"56": {
    "doc": "Scores by release date",
    "title": "Scores by release date",
    "content": " ",
    "url": "/docs/leaderboards/by-release-date.html",
    
    "relUrl": "/docs/leaderboards/by-release-date.html"
  },"57": {
    "doc": "Prompt caching",
    "title": "Prompt caching",
    "content": "Llmcode supports prompt caching for cost savings and faster coding. Currently Anthropic provides caching for Sonnet and Haiku, and DeepSeek provides caching for Chat. Llmcode organizes the chat history to try and cache: . | The system prompt. | Read only files added with --read or /read-only. | The repository map. | The editable files that have been added to the chat. | . ",
    "url": "/docs/usage/caching.html",
    
    "relUrl": "/docs/usage/caching.html"
  },"58": {
    "doc": "Prompt caching",
    "title": "Usage",
    "content": "Run llmcode with --cache-prompts or add that setting to your configuration files. Due to limitations in the provider APIs, caching statistics and costs are not available when streaming responses. To turn off streaming, use --no-stream. When caching is enabled, it will be noted for the main model when llmcode launches: . Main model: claude-3-5-sonnet-20240620 with diff edit format, prompt cache, infinite output . ",
    "url": "/docs/usage/caching.html#usage",
    
    "relUrl": "/docs/usage/caching.html#usage"
  },"59": {
    "doc": "Prompt caching",
    "title": "Preventing cache expiration",
    "content": "Llmcode can ping the provider to keep your prompt cache warm and prevent it from expiring. By default, Anthropic keeps your cache for 5 minutes. Use --cache-keepalive-pings N to tell llmcode to ping every 5 minutes to keep the cache warm. Llmcode will ping up to N times over a period of N*5 minutes after each message you send. ",
    "url": "/docs/usage/caching.html#preventing-cache-expiration",
    
    "relUrl": "/docs/usage/caching.html#preventing-cache-expiration"
  },"60": {
    "doc": "Download, analyze and plot US Census data",
    "title": "Download, analyze and plot US Census data",
    "content": "Inspired by Ethan Mollick’s recent blog article about ChatGPT plugins, I used llmcode and GPT-4 to do data analysis of some US Census data. I asked llmcode how to download census data and to suggest some hypotheses to test. I asked it to write the code to get the data, test one of the hypotheses and provide a summary and plot of the results. $ llmcode Using git repo: .git . what is a good basic us census dataset to analyze for interesting patterns? . A good basic US Census dataset to analyze for interesting patterns is the American Community Survey (ACS) dataset. The ACS is an ongoing survey conducted by the US Census Bureau that provides vital information on a yearly basis about the US population and its characteristics. The dataset includes information on demographics, social, economic, and housing characteristics. You can find the ACS dataset on the US Census Bureau’s website: https://www.census.gov/programs-surveys/acs/data.html . Some interesting patterns to analyze in the ACS dataset could include: . | Population growth and decline in different regions. | Changes in racial and ethnic composition. | Income inequality and poverty levels. | Educational attainment and its relationship with income. | Housing affordability and homeownership rates. | . Remember to consider the dataset’s documentation and guidelines to understand the data and its limitations. do you know how to download American Community Survey data? . Yes, I can guide you on how to download American Community Survey (ACS) data. You can use the Census Bureau’s API to access the data. To do this, you’ll need an API key, which you can obtain from the Census Bureau’s website. Here’s a Python example using the requests library to download ACS data: . | Install the requests library if you haven’t already: | . pip install requests . | Use the following code to download ACS data: | . import requests # Replace 'your_api_key' with your actual API key api_key = 'your_api_key' # Define the API endpoint and parameters url = 'https://api.census.gov/data/2019/acs/acs5' params = { 'get': 'NAME,B01001_001E', # Variables to retrieve (NAME: Geographic area name, B01001_001E: Total population) 'for': 'state:*', # Geographic area (all states) 'key': api_key } # Send the request and parse the response response = requests.get(url, params=params) data = response.json() # Print the data for row in data: print(row) . Replace 'your_api_key' with the API key you obtained from the Census Bureau’s website. This example retrieves the total population (variable B01001_001E) for all states. You can modify the get and for parameters to download different variables and geographic areas. For more information on available variables and geographic areas, refer to the ACS documentation: . | Variables: https://api.census.gov/data/2019/acs/acs5/variables.html | Geographic Areas: https://api.census.gov/data/2019/acs/acs5/geography.html | . how do i get an api key? . To obtain an API key from the Census Bureau, follow these steps: . | Go to the Census Bureau’s API key request page: https://api.census.gov/data/key_signup.html . | Fill out the form with your name, email address, organization (if applicable), and a brief description of your intended use of the API. | Click the “Request Key” button. | You will receive an email containing your API key. It may take a few minutes for the email to arrive. Make sure to check your spam folder if you don’t see it in your inbox. | . Once you have your API key, you can use it in your Python code to access the American Community Survey data, as shown in the previous example. Replace 'your_api_key' with the API key you received in the email. please save your download script as getdata.py . Here’s the getdata.py file with the download script: . new_file/getdata.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL ======= import requests # Replace 'your_api_key' with your actual API key api_key = 'your_api_key' # Define the API endpoint and parameters url = 'https://api.census.gov/data/2019/acs/acs5' params = { 'get': 'NAME,B01001_001E', # Variables to retrieve (NAME: Geographic area name, B01001_001E: Total population) 'for': 'state:*', # Geographic area (all states) 'key': api_key } # Send the request and parse the response response = requests.get(url, params=params) data = response.json() # Print the data for row in data: print(row) &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . Save this code in a new file named getdata.py. Remember to replace 'your_api_key' with your actual API key obtained from the Census Bureau. Allow creation of new file new_file/getdata.py? y Add new_file/getdata.py to git? y Applied edit to new_file/getdata.py Commit afcb9d1 llmcode: Added getdata.py file with download script. add my api key: 4ae31ed2a0f0dd62609e51cd95deadbeefe7774f . I will update the API key in the file. new_file/getdata.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL api_key = 'your_api_key' ======= api_key = '4ae31ed2a0f0dd62609e51cd95deadbeefe7774f' &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . Applied edit to new_file/getdata.py Commit 1d8b4a6 llmcode: Updated the API key in new_file/getdata.py. what are some interesting hypothesis that you might be able to test using ACS data? . Here are some interesting hypotheses that you can test using ACS (American Community Survey) data: . | Relationship between income and education level: Higher education levels lead to higher income. | Impact of population density on housing prices: Higher population density areas have higher housing prices. | Relationship between employment rate and crime rate: Higher employment rates lead to lower crime rates. | Effect of public transportation availability on car ownership: Areas with better public transportation have lower car ownership rates. | Relationship between age distribution and healthcare facilities: Areas with a higher percentage of elderly population have more healthcare facilities. | . These are just a few examples. ACS data provides a wealth of information on various demographic, social, economic, and housing characteristics, allowing you to explore many different hypotheses. can you download the data to test #2 Impact of population density on housing prices: Higher population density areas have higher housing prices. As an AI language model, I cannot directly download or access data. However, I can help you with the code to download the required data for testing hypothesis #2 using the ACS data. To test the hypothesis, you’ll need to obtain population density and housing price data. You can use the following ACS variables: . | B01003_001E: Total population | B25077_001E: Median value of owner-occupied housing units | . Here’s the updated code to fetch the required data: . new_file/getdata.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL 'get': 'NAME,B01001_001E', # Variables to retrieve (NAME: Geographic area name, B01001_001E: Total population) ======= 'get': 'NAME,B01003_001E,B25077_001E', # Variables to retrieve (NAME: Geographic area name, B01003_001E: Total population, B25077_001E: Median value of owner-occupied housing units) &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . After running the updated code, you’ll have the population and housing price data for each geographic area. You can then analyze the data to test the hypothesis. Applied edit to new_file/getdata.py Commit 81549ab llmcode: Updated code to fetch population and housing price data for testing hypothesis #2. instead of printing all the data, print a summary table that shows whether the data supports the hypothesis . To print a summary table, you can use the pandas library to analyze the data and calculate the correlation between population density and housing prices. Here’s how to modify the code: . | Import pandas. | Convert the JSON data to a pandas DataFrame. | Calculate the correlation between population density and housing prices. | Print the correlation as a summary table. | . Here are the changes to the code: . new_file/getdata.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL import requests ======= import requests import pandas as pd &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . new_file/getdata.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL data = response.json() ======= data = response.json() # Convert the JSON data to a pandas DataFrame df = pd.DataFrame(data[1:], columns=data[0]) df[['B01003_001E', 'B25077_001E']] = df[['B01003_001E', 'B25077_001E']].apply(pd.to_numeric) &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . new_file/getdata.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL for row in data: print(row) ======= # Calculate the correlation between population density and housing prices correlation = df['B01003_001E'].corr(df['B25077_001E']) # Print the correlation as a summary table print(\"Summary Table:\") print(\"Correlation between population density and housing prices:\", correlation) &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . After applying these changes, the code will print a summary table showing the correlation between population density and housing prices. A positive correlation value indicates that higher population density areas have higher housing prices, supporting the hypothesis. Applied edit to new_file/getdata.py Applied edit to new_file/getdata.py Applied edit to new_file/getdata.py Commit 2f4fcd4 llmcode: Added pandas library to analyze data and calculate correlation between population density and housing prices, and printed the correlation as a summary table. /run python3 new_file/getdata.py . Summary Table: Correlation between population density and housing prices: 0.16020298445808737 . Add the output to the chat? y . Great! The script seems to be working as expected. The correlation between population density and housing prices is 0.1602. If you need any further assistance or modifications to the code, please let me know. can you also plot a graph which would show any correlation? . To plot a graph showing the correlation between population density and housing prices, we’ll need to modify the new_file/getdata.py file. We’ll use the matplotlib library to create the plot. If you don’t have it installed, you can install it using pip install matplotlib. Here are the changes to make: . | Import the matplotlib.pyplot module. | Create a scatter plot of population density (x-axis) and housing prices (y-axis). | Add labels to the axes and a title to the plot. | Display the plot. | . Here’s the updated code: . new_file/getdata.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL import requests import pandas as pd ======= import requests import pandas as pd import matplotlib.pyplot as plt &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED ... new_file/getdata.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL print(\"Summary Table:\") print(\"Correlation between population density and housing prices:\", correlation) ======= print(\"Summary Table:\") print(\"Correlation between population density and housing prices:\", correlation) # Create a scatter plot of population density and housing prices plt.scatter(df['B01003_001E'], df['B25077_001E']) plt.xlabel('Population Density') plt.ylabel('Housing Prices') plt.title('Correlation between Population Density and Housing Prices') plt.show() &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . This will create a scatter plot of population density and housing prices, which will help visualize the correlation between the two variables. Applied edit to new_file/getdata.py Applied edit to new_file/getdata.py Commit 9ff3666 llmcode: Added scatter plot of population density and housing prices to visualize correlation. ",
    "url": "/examples/census.html",
    
    "relUrl": "/examples/census.html"
  },"61": {
    "doc": "Improve css styling of chat transcripts",
    "title": "Improve css styling of chat transcripts",
    "content": "$ llmcode index.html assets/css/style.css Added index.html to the chat Added assets/css/style.css to the chat . add a rounded border to all sides of the boxquote . assets/css/style.css &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL .main-content blockquote{padding:0 1rem;margin-left:0;color:#819198;border-left:0.3rem solid #dce6f0} ======= .main-content blockquote{ padding: 0 1rem; margin . ^C KeyboardInterrupt ^C again to quit . make all the edits to the embedded style tag in the html . index.html &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL &lt;style&gt; .main-content blockquote { border-left-color: #159957; } .btn { display: inline-block; width: auto; } .btn + .btn { margin-top: 0; } &lt;/style&gt; ======= &lt;style&gt; .main-content blockquote { border-left-color: #159957; border: 0.3rem solid #dce6f0; border-radius: 0.5rem; } .btn { display: inline-block; width: auto; } .btn + .btn { margin-top: 0; } &lt;/style&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . Applied edit to index.html Commit e7a5cb5 llmcode: Added rounded border to all sides of the boxquote. make the blockquote font color the same as the p color . Git repo has uncommitted changes. Commit before the chat proceeds [y/n/commit message]? Added note about missing method and incorrect method calls in index.html. Commit 2d31f5b Added note about missing method and incorrect method calls in index.html. assets/css/style.css &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL .main-content blockquote{padding:0 1rem;margin-left:0;color:#819198;border-left . ^C KeyboardInterrupt . only edit index.html . index.html &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL &lt;style&gt; .main-content blockquote { border: 0.1rem solid #159957; border-radius: 0.5rem; } .btn { display: inline-block; width: auto; } .btn + .btn { margin-top: 0; } &lt;/style&gt; ======= &lt;style&gt; .main-content blockquote { border: 0.1rem solid #159957; border-radius: 0.5rem; color: #606c71; } .btn { display: inline-block; width: auto; } .btn + .btn { margin-top: 0; } &lt;/style&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . Applied edit to index.html Commit 8645217 llmcode: Changed blockquote font color to match &lt;p&gt; color in index.html. make the blockquote background a lighter shade of the same green as the border . index.html &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL &lt;style&gt; .main-content blockquote { border: 0.1rem solid #159957; border-radius: 0.5rem; } ======= &lt;style&gt; .main-content blockquote { border: 0.1rem solid #159957; border-radius: 0.5rem; background-color: #e0f2ef; } &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . Applied edit to index.html Commit 57b7b10 llmcode: Changed blockquote background color to a lighter shade of green. add some space inside the border of the blockquote . index.html &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL &lt;style&gt; .main-content blockquote { border: 0.1rem solid #159957; border-radius: 0.5rem; background-color: #e0f2ef; } ======= &lt;style&gt; .main-content blockquote { border: 0.1rem solid #159957; border-radius: 0.5rem; background-color: #e0f2ef; padding: 1rem; } &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . Applied edit to index.html Commit dd7dcb6 llmcode: Added padding inside the border of the blockquote. can we add a label “llmcode” along the top of the blockquote box only using style . index.html &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL &lt;style&gt; .main-content blockquote { border: 0.1rem solid #159957; border-radius: 0.5rem; background-color: #e0f2ef; padding: 1rem; } &lt;/style&gt; ======= &lt;style&gt; .main-content blockquote { border: 0.1rem solid #159957; border-radius: 0.5rem; background-color: #e0f2ef; padding: 1rem; position: relative; } .main-content blockquote::before { content: \"llmcode\"; display: inline-block; position: absolute; top: -0.5rem; left: 1rem; background-color: #e0f2ef; padding: 0 0.5rem; font-weight: bold; color: #159957; } &lt;/style&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . Applied edit to index.html Commit 0b0e332 llmcode: Added label “llmcode” to the top of the blockquote box using style. add more padding on the top of the box and bring the label into the box so it doesn’t intersect the border . index.html &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL &lt;style&gt; .main-content blockquote { border: 0.1rem solid #159957; border-radius: 0.5rem; background-color: #e0f2ef; padding: 1rem; position: relative; } .main-content blockquote::before { content: \"llmcode\"; display: inline-block; position: absolute; top: -0.5rem; left: 1rem; background-color: #e0f2ef; padding: 0 0.5rem; font-weight: bold; color: #159957; } &lt;/style&gt; ======= &lt;style&gt; .main-content blockquote { border: 0.1rem solid #159957; border-radius: 0.5rem; background-color: #e0f2ef; padding: 1.5rem 1rem 1rem 1rem; position: relative; } .main-content blockquote::before { content: \"llmcode\"; display: inline-block; position: absolute; top: 0.5rem; left: 1rem; background-color: #e0f2ef; padding: 0 0.5rem; font-weight: bold; color: #159957; } &lt;/style&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . Applied edit to index.html Commit eee902e llmcode: Added more padding on the top of the box and brought the label into the box so it doesn’t intersect the border. change all the blockquote colors to be shades of blue instead of green. choose the blue based on the blue in the header gradient . index.html &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL &lt;style&gt; .main-content blockquote { border: 0.1rem solid #159957; border-radius: 0.5rem; background-color: #e0f2ef; padding: 0.75rem; position: relative; margin-top: 2rem; } .main-content blockquote::before { content: \"llmcode\"; display: inline-block; position: absolute; top: -1.75rem; left: -0.5rem; padding: 0 0.5rem; font-weight: bold; color: #159957; } &lt;/style&gt; ======= &lt;style&gt; .main-content blockquote { border: 0.1rem solid #155799; border-radius: 0.5rem; background-color: #e0ecf4; padding: 0.75rem; position: relative; margin-top: 2rem; } .main-content blockquote::before { content: \"llmcode\"; display: inline-block; position: absolute; top: -1.75rem; left: -0.5rem; padding: 0 0.5rem; font-weight: bold; color: #155799; } &lt;/style&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . Applied edit to index.html Commit a26c135 llmcode: Changed blockquote colors to shades of blue based on the blue in the header gradient. can you style the “llmcode” label on the blockquote as a “tab” that docks onto the top of the box? . index.html &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL .main-content blockquote { border: 0.1rem solid #155799; border-radius: 0.5rem; background-color: #e0ecf4; padding: 0.75rem; position: relative; margin-top: 2rem; } .main-content blockquote::before { content: \"llmcode\"; display: inline-block; position: absolute; top: -1.75rem; left: -0.5rem; padding: 0 0.5rem; font-weight: bold; color: #155799; } ======= .main-content blockquote { border: 0.1rem solid #155799; border-radius: 0.5rem; background-color: #e0ecf4; padding: 0.75rem; position: relative; margin-top: 2rem; } .main-content blockquote::before { content: \"llmcode\"; display: inline-block; position: absolute; top: -0.5rem; left: 0; padding: 0.25rem 0.5rem; font-weight: bold; color: #155799; background-color: #e0ecf4; border-top-left-radius: 0.5rem; border-top-right-radius: 0.5rem; border: 0.1rem solid #155799; border-bottom: none; } &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . Applied edit to index.html Commit 292eda3 llmcode: Styled the “llmcode” label on the blockquote as a “tab” that docks onto the top of the box. apply the same box around the h4 element . index.html &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL &lt;style&gt; .main-content blockquote { border: 0.1rem solid #155799; border-radius: 0.5rem; background-color: #e0ecf4; padding: 1.25rem 0.75rem 0.75rem 0.75rem; position: relative; margin-top: 2rem; } .main-content blockquote::before { content: \"llmcode\"; display: inline-block; position: absolute; top: -1rem; left: 0.5; padding: 0rem 0.5rem; font-weight: bold; color: #155799; background-color: #e0ecf4; border: 0.05rem solid #155799; border-radius: 0.5rem; } &lt;/style&gt; ======= &lt;style&gt; .main-content blockquote, .main-content h4 { border: 0.1rem solid #155799; border-radius: 0.5rem; background-color: #e0ecf4; padding: 1.25rem 0.75rem 0.75rem 0.75rem; position: relative; margin-top: 2rem; } .main-content blockquote::before { content: \"llmcode\"; display: inline-block; position: absolute; top: -1rem; left: 0.5; padding: 0rem 0.5rem; font-weight: bold; color: #155799; background-color: #e0ecf4; border: 0.05rem solid #155799; border-radius: 0.5rem; } &lt;/style&gt; &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . Applied edit to index.html Commit 6a8cd27 llmcode: Added box around h4 element. ",
    "url": "/examples/chat-transcript-css.html",
    
    "relUrl": "/examples/chat-transcript-css.html"
  },"62": {
    "doc": "GitHub Codespaces",
    "title": "GitHub Codespaces",
    "content": "You can use llmcode in GitHub Codespaces via the built-in Terminal pane. See below for an example, but you can just follow the main install instructions inside your codespace terminal. Install llmcode in GitHub Codespaces ",
    "url": "/docs/install/codespaces.html",
    
    "relUrl": "/docs/install/codespaces.html"
  },"63": {
    "doc": "Cohere",
    "title": "Cohere",
    "content": "Cohere offers free API access to their models. Their Command-R+ model works well with llmcode as a very basic coding assistant. You’ll need a Cohere API key. To use Command-R+: . python -m pip install -U llmcode export COHERE_API_KEY=&lt;key&gt; # Mac/Linux setx COHERE_API_KEY &lt;key&gt; # Windows, restart shell after setx llmcode --model command-r-plus-08-2024 # List models available from Cohere llmcode --list-models cohere_chat/ . ",
    "url": "/docs/llms/cohere.html",
    
    "relUrl": "/docs/llms/cohere.html"
  },"64": {
    "doc": "In-chat commands",
    "title": "In-chat commands",
    "content": ". | Slash commands | Entering multi-line chat messages | Interrupting with CONTROL-C | Keybindings . | Emacs | Vi | . | . ",
    "url": "/docs/usage/commands.html",
    
    "relUrl": "/docs/usage/commands.html"
  },"65": {
    "doc": "In-chat commands",
    "title": "Slash commands",
    "content": "Llmcode supports commands from within the chat, which all start with /. | Command | Description | . | /add | Add files to the chat so llmcode can edit them or review them in detail | . | /architect | Enter architect/editor mode using 2 different models. If no prompt provided, switches to architect/editor mode. | . | /ask | Ask questions about the code base without editing any files. If no prompt provided, switches to ask mode. | . | /chat-mode | Switch to a new chat mode | . | /clear | Clear the chat history | . | /code | Ask for changes to your code. If no prompt provided, switches to code mode. | . | /commit | Commit edits to the repo made outside the chat (commit message optional) | . | /copy | Copy the last assistant message to the clipboard | . | /copy-context | Copy the current chat context as markdown, suitable to paste into a web UI | . | /diff | Display the diff of changes since the last message | . | /drop | Remove files from the chat session to free up context space | . | /editor | Open an editor to write a prompt | . | /exit | Exit the application | . | /git | Run a git command (output excluded from chat) | . | /help | Ask questions about llmcode | . | /lint | Lint and fix in-chat files or all dirty files if none in chat | . | /load | Load and execute commands from a file | . | /ls | List all known files and indicate which are included in the chat session | . | /map | Print out the current repository map | . | /map-refresh | Force a refresh of the repository map | . | /model | Switch to a new LLM | . | /models | Search the list of available models | . | /multiline-mode | Toggle multiline mode (swaps behavior of Enter and Meta+Enter) | . | /paste | Paste image/text from the clipboard into the chat. Optionally provide a name for the image. | . | /quit | Exit the application | . | /read-only | Add files to the chat that are for reference only, or turn added files to read-only | . | /report | Report a problem by opening a GitHub Issue | . | /reset | Drop all files and clear the chat history | . | /run | Run a shell command and optionally add the output to the chat (alias: !) | . | /save | Save commands to a file that can reconstruct the current chat session’s files | . | /settings | Print out the current settings | . | /test | Run a shell command and add the output to the chat on non-zero exit code | . | /tokens | Report on the number of tokens used by the current chat context | . | /undo | Undo the last git commit if it was done by llmcode | . | /voice | Record and transcribe voice input | . | /web | Scrape a webpage, convert to markdown and send in a message | . You can easily re-send commands or messages. Use the up arrow ⬆ to scroll back or CONTROL-R to search your message history. ",
    "url": "/docs/usage/commands.html#slash-commands",
    
    "relUrl": "/docs/usage/commands.html#slash-commands"
  },"66": {
    "doc": "In-chat commands",
    "title": "Entering multi-line chat messages",
    "content": "You can send long, multi-line messages in the chat in a few ways: . | Paste a multi-line message directly into the chat. | Enter { alone on the first line to start a multiline message and } alone on the last line to end it. | Or, start with {tag (where “tag” is any sequence of letters/numbers) and end with tag}. This is useful when you need to include closing braces } in your message. | . | Use Meta-ENTER to start a new line without sending the message (Esc+ENTER in some environments). | Use /paste to paste text from the clipboard into the chat. | Use the /editor command to open your editor to create the next chat message. See editor configuration docs for more info. | Use multiline-mode, which swaps the function of Meta-Enter and Enter, so that Enter inserts a newline, and Meta-Enter submits your command. To enable multiline mode: . | Use the /multiline-mode command to toggle it during a session. | Use the --multiline switch. | . | . Example with a tag: . {python def hello(): print(\"Hello}\") # Note: contains a brace python} . People often ask for SHIFT-ENTER to be a soft-newline. Unfortunately there is no portable way to detect that keystroke in terminals. ",
    "url": "/docs/usage/commands.html#entering-multi-line-chat-messages",
    
    "relUrl": "/docs/usage/commands.html#entering-multi-line-chat-messages"
  },"67": {
    "doc": "In-chat commands",
    "title": "Interrupting with CONTROL-C",
    "content": "It’s always safe to use Control-C to interrupt llmcode if it isn’t providing a useful response. The partial response remains in the conversation, so you can refer to it when you reply to the LLM with more information or direction. ",
    "url": "/docs/usage/commands.html#interrupting-with-control-c",
    
    "relUrl": "/docs/usage/commands.html#interrupting-with-control-c"
  },"68": {
    "doc": "In-chat commands",
    "title": "Keybindings",
    "content": "The interactive prompt is built with prompt-toolkit which provides emacs and vi keybindings. Emacs . | Up Arrow : Move up one line in the current message. | Down Arrow : Move down one line in the current message. | Ctrl-Up : Scroll back through previously sent messages. | Ctrl-Down : Scroll forward through previously sent messages. | Ctrl-A : Move cursor to the start of the line. | Ctrl-B : Move cursor back one character. | Ctrl-D : Delete the character under the cursor. | Ctrl-E : Move cursor to the end of the line. | Ctrl-F : Move cursor forward one character. | Ctrl-K : Delete from the cursor to the end of the line. | Ctrl-L : Clear the screen. | Ctrl-N : Move down to the next history entry. | Ctrl-P : Move up to the previous history entry. | Ctrl-R : Reverse search in command history. | . Vi . To use vi/vim keybindings, run llmcode with the --vim switch. | Up Arrow : Move up one line in the current message. | Down Arrow : Move down one line in the current message. | Ctrl-Up : Scroll back through previously sent messages. | Ctrl-Down : Scroll forward through previously sent messages. | Esc : Switch to command mode. | i : Switch to insert mode. | a : Move cursor one character to the right and switch to insert mode. | A : Move cursor to the end of the line and switch to insert mode. | I : Move cursor to the beginning of the line and switch to insert mode. | h : Move cursor one character to the left. | j : Move cursor down one line. | k : Move cursor up one line. | l : Move cursor one character to the right. | w : Move cursor forward one word. | b : Move cursor backward one word. | 0 : Move cursor to the beginning of the line. | $ : Move cursor to the end of the line. | x : Delete the character under the cursor. | dd : Delete the current line. | u : Undo the last change. | Ctrl-R : Redo the last undone change. | . ",
    "url": "/docs/usage/commands.html#keybindings",
    
    "relUrl": "/docs/usage/commands.html#keybindings"
  },"69": {
    "doc": "A complex multi-file change, with debugging",
    "title": "A complex multi-file change, with debugging",
    "content": "This is a chat transcript of a fairly complex code change. The user is trying to update tests to use an input mock provided by prompt_toolkit. This requires changes to multiple source files. Some things to notice: . | The initial changes don’t work, so the user provides error messages and snippets from the prompt_toolkit docs to collaboratively debug and solve the problem. | The user edits a file outside the chat using their editor. llmcode notices these out-of-band changes and offers to commit them. | At one point the user hits ^C to interrupt the LLM response, since it’s not a useful response. The user gives clarifications and gets a better result on the next reply. | . $ llmcode tests/test_main.py llmcode/getinput.py Added tests/test_main.py to the chat Added llmcode/getinput.py to the chat . instead of using /dev/null to send no input in the main tests, can we use the prompt_toolkit input mocking functions? . Yes, we can use the prompt_toolkit.input module to mock the input in the tests. Here’s how to modify the tests/test_main.py file: . tests/test_main.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL import os import sys import tempfile from unittest import TestCase from llmcode.main import main import subprocess ======= import os import tempfile from unittest import TestCase from llmcode.main import main import subprocess from prompt_toolkit.input import create_pipe_input from prompt_toolkit.output import DummyOutput &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . Now, update the tests to use the create_pipe_input and DummyOutput: . tests/test_main.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL with open(os.devnull, \"r\") as dev_null: save_stdin = sys.stdin sys.stdin = dev_null main([]) sys.stdin = save_stdin ======= pipe_input = create_pipe_input() save_stdin = sys.stdin sys.stdin = pipe_input main([], input=pipe_input, output=DummyOutput()) sys.stdin = save_stdin pipe_input.close() &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . tests/test_main.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL with open(os.devnull, \"r\") as dev_null: save_stdin = sys.stdin sys.stdin = dev_null main([\"foo.txt\"]) sys.stdin = save_stdin self.assertTrue(os.path.exists(\"foo.txt\")) ======= pipe_input = create_pipe_input() save_stdin = sys.stdin sys.stdin = pipe_input main([\"foo.txt\"], input=pipe_input, output=DummyOutput()) sys.stdin = save_stdin pipe_input.close() self.assertTrue(os.path.exists(\"foo.txt\")) &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . tests/test_main.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL with open(os.devnull, \"r\") as dev_null: save_stdin = sys.stdin sys.stdin = dev_null main([\"--yes\", \"foo.txt\"]) sys.stdin = save_stdin self.assertTrue(os.path.exists(\"foo.txt\")) ======= pipe_input = create_pipe_input() save_stdin = sys.stdin sys.stdin = pipe_input main([\"--yes\", \"foo.txt\"], input=pipe_input, output=DummyOutput()) sys.stdin = save_stdin pipe_input.close() self.assertTrue(os.path.exists(\"foo.txt\")) &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . These changes will replace the usage of /dev/null with prompt_toolkit’s input mocking functions. Applied edit to tests/test_main.py Applied edit to tests/test_main.py Applied edit to tests/test_main.py Applied edit to tests/test_main.py Commit c177e29 llmcode: Replaced usage of /dev/null with prompt_toolkit’s input mocking functions in tests. Git repo has uncommitted changes. Suggested commit message: Added import statement for sys module in test_main.py. Commit before the chat proceeds [y/n/commit message]? y Commit 6724fcd Added import statement for sys module in test_main.py. /add llmcode/main.py . Added llmcode/main.py to the chat . I am getting this error: FAILED tests/test_main.py::TestMain::test_main_with_empty_git_dir_new_file - TypeError: main() got an unexpected keyword argument ‘input’ . I apologize for the confusion. It seems I made a mistake in the main() function definition in llmcode/main.py. The main() function should accept input and output keyword arguments. Here’s the fix: . llmcode/main.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL def main(args=None): if args is None: args = sys.argv[1:] ======= def main(args=None, input=None, output=None): if args is None: args = sys.argv[1:] if input is None: input = sys.stdin if output is None: output = sys.stdout &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . Now, the main() function will accept the input and output keyword arguments, and the tests should pass. Applied edit to llmcode/main.py Commit 79dfba9 llmcode: Fixed main() function to accept input and output keyword arguments. we need to pass those into InputOutput and to prompt_toolkit right? . Yes, you’re right. We need to pass the input and output arguments to the InputOutput class and then to the prompt_toolkit.prompt() function. Here’s how to modify the llmcode/getinput.py and llmcode/main.py files: . In llmcode/getinput.py, update the InputOutput class to accept input and output arguments: . llmcode/getinput.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL class InputOutput: def __init__(self, pretty, yes, input_history_file, chat_history_file): ======= class InputOutput: def __init__(self, pretty, yes, input_history_file, chat_history_file, input=None, output=None): self.input = input self.output = output &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . Now, update the prompt() function calls in the get_input() method to use the input and output arguments: . llmcode/getinput.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL line = prompt( show, completer=completer_instance, history=FileHistory(self.input_history_file), style=style, reserve_space_for_menu=4, complete_style=CompleteStyle.MULTI_COLUMN, ) ======= line = prompt( show, completer=completer_instance, history=FileHistory(self.input_history_file), style=style, reserve_space_for_menu=4, complete_style=CompleteStyle.MULTI_COLUMN, input=self.input, output=self.output, ) &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . In llmcode/main.py, pass the input and output arguments to the InputOutput class: . llmcode/main.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL io = InputOutput(args.pretty, args.yes, args.input_history_file, args.chat_history_file) ======= io = InputOutput(args.pretty, args.yes, args.input_history_file, args.chat_history_file, input=input, output=output) &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . These changes will ensure that the input and output arguments are passed to the InputOutput class and then to the prompt_toolkit.prompt() function. Applied edit to llmcode/getinput.py Applied edit to llmcode/getinput.py Applied edit to llmcode/main.py Commit cea9d2a llmcode: Updated InputOutput class to accept input and output arguments and passed them to prompt_toolkit.prompt() function. FAILED tests/test_main.py::TestMain::test_main_with_empty_git_dir_new_file - TypeError: prompt() got an unexpected keyword argument ‘input’ . I apologize for the confusion. It seems I made a mistake in the prompt_toolkit.prompt() function call. The prompt() function accepts stdin and stdout arguments instead of input and output. Here’s the fix: . Update the prompt() function call in the get_input() method in llmcode/getinput.py: . llmcode/getinput.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL input=self.input, output=self.output, ======= stdin=self.input, stdout=self.output, &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . These changes will ensure that the input and output arguments are passed correctly to the prompt_toolkit.prompt() function. Applied edit to llmcode/getinput.py Commit 55d3540 llmcode: Fixed prompt() function call in get_input() method to pass stdin and stdout arguments instead of input and output. prompt_toolkit.shortcuts.prompt(message: AnyFormattedText | None = None, *, history: History | None = None, editing_mode: EditingMode | None = None, refresh_interval: float | None = None, vi_mode: bool | None = None, lexer: Lexer | None = None, completer: Completer | None = None, complete_in_thread: bool | None = None, is_password: bool | None = None, key_bindings: KeyBindingsBase | None = None, bottom_toolbar: AnyFormattedText | None = None, style: BaseStyle | None = None, color_depth: ColorDepth | None = None, cursor: AnyCursorShapeConfig = None, include_default_pygments_style: FilterOrBool | None = None, style_transformation: StyleTransformation | None = None, swap_light_and_dark_colors: FilterOrBool | None = None, rprompt: AnyFormattedText | None = None, multiline: FilterOrBool | None = None, prompt_continuation: PromptContinuationText | None = None, wrap_lines: FilterOrBool | None = None, enable_history_search: FilterOrBool | None = None, search_ignore_case: FilterOrBool | None = None, complete_while_typing: FilterOrBool | None = None, validate_while_typing: FilterOrBool | None = None, complete_style: CompleteStyle | None = None, auto_suggest: AutoSuggest | None = None, validator: Validator | None = None, clipboard: Clipboard | None = None, mouse_support: FilterOrBool | None = None, input_processors: list[Processor] | None = None, placeholder: AnyFormattedText | None = None, reserve_space_for_menu: int | None = None, enable_system_prompt: FilterOrBool | None = None, enable_suspend: FilterOrBool | None = None, enable_open_in_editor: FilterOrBool | None = None, tempfile_suffix: str | Callable[[], str] | None = None, tempfile: str | Callable[[], str] | None = None, default: str = ‘’, accept_default: bool = False, pre_run: Callable[[], None] | None = None) → str . I apologize for the confusion earlier. It seems that the prompt_toolkit.prompt() function does not accept stdin and stdout arguments directly. Instead, we need to use the Application class from prompt_toolkit to create a custom application with the desired input and output. Here’s how to modify the llmcode/getinput.py file: . First, import the necessary classes and functions from `prompt_tool . ^C KeyboardInterrupt . class prompt_toolkit.shortcuts.PromptSession(message: AnyFormattedText = ‘’, *, multiline: FilterOrBool = False, wrap_lines: FilterOrBool = True, is_password: FilterOrBool = False, vi_mode: bool = False, editing_mode: EditingMode = &lt;EditingMode.EMACS: ‘EMACS’&gt;, complete_while_typing: FilterOrBool = True, validate_while_typing: FilterOrBool = True, enable_history_search: FilterOrBool = False, search_ignore_case: FilterOrBool = False, lexer: Lexer | None = None, enable_system_prompt: FilterOrBool = False, enable_suspend: FilterOrBool = False, enable_open_in_editor: FilterOrBool = False, validator: Validator | None = None, completer: Completer | None = None, complete_in_thread: bool = False, reserve_space_for_menu: int = 8, complete_style: CompleteStyle = &lt;CompleteStyle.COLUMN: ‘COLUMN’&gt;, auto_suggest: AutoSuggest | None = None, style: BaseStyle | None = None, style_transformation: StyleTransformation | None = None, swap_light_and_dark_colors: FilterOrBool = False, color_depth: ColorDepth | None = None, cursor: AnyCursorShapeConfig = None, include_default_pygments_style: FilterOrBool = True, history: History | None = None, clipboard: Clipboard | None = None, prompt_continuation: PromptContinuationText | None = None, rprompt: AnyFormattedText = None, bottom_toolbar: AnyFormattedText = None, mouse_support: FilterOrBool = False, input_processors: list[Processor] | None = None, placeholder: AnyFormattedText | None = None, key_bindings: KeyBindingsBase | None = None, erase_when_done: bool = False, tempfile_suffix: str | Callable[[], str] | None = ‘.txt’, tempfile: str | Callable[[], str] | None = None, refresh_interval: float = 0, input: Input | None = None, output: Output | None = None) . Thank you for providing the PromptSession class information. We can use the PromptSession class to create a custom session with the desired input and output. Here’s how to modify the llmcode/getinput.py file: . First, import the PromptSession class from prompt_toolkit.shortcuts: . llmcode/getinput.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL from prompt_toolkit import prompt ======= from prompt_toolkit.shortcuts import PromptSession &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . Now, update the get_input() method to use the PromptSession class: . llmcode/getinput.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL line = prompt( show, completer=completer_instance, history=FileHistory(self.input_history_file), style=style, reserve_space_for_menu=4, complete_style=CompleteStyle.MULTI_COLUMN, input=self.input, output=self.output, ) ======= session = PromptSession( message=show, completer=completer_instance, history=FileHistory(self.input_history_file), style=style, reserve_space_for_menu=4, complete_style=CompleteStyle.MULTI_COLUMN, input=self.input, output=self.output, ) line = session.prompt() &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . These changes will ensure that the input and output arguments are passed correctly to the prompt_toolkit’s PromptSession class. Applied edit to llmcode/getinput.py Applied edit to llmcode/getinput.py Commit ac92cca llmcode: Replaced prompt() with PromptSession() to correctly pass input and output arguments. try using create_input(StringIO(‘’)) in the tests . I understand that you want to use create_input(StringIO('')) in the tests. However, I need more information about which test cases you want to apply this change to and how you want to use it. Please provide more details or examples. the ones that currently use create_pipe_input() . I see that you want to replace create_pipe_input() with create_input(StringIO('')) in the test cases. Here are the changes for the tests/test_main.py file: . tests/test_main.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL from prompt_toolkit.input import create_pipe_input ======= from prompt_toolkit.input import create_input from io import StringIO &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . tests/test_main.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL pipe_input = create_pipe_input() ======= pipe_input = create_input(StringIO('')) &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . tests/test_main.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL pipe_input = create_pipe_input() ======= pipe_input = create_input(StringIO('')) &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . tests/test_main.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL pipe_input = create_pipe_input() ======= pipe_input = create_input(StringIO('')) &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . These changes will replace the usage of create_pipe_input() with create_input(StringIO('')) in the test cases. Applied edit to tests/test_main.py Applied edit to tests/test_main.py Applied edit to tests/test_main.py Applied edit to tests/test_main.py Commit 4bb043f llmcode: Replaced the usage of create_pipe_input() with create_input(StringIO('')) in the test cases. nice, that worked! . I’m glad the changes worked for you! If you have any more requests or need further assistance, feel free to ask. ",
    "url": "/examples/complex-change.html",
    
    "relUrl": "/examples/complex-change.html"
  },"70": {
    "doc": "Configuration",
    "title": "Configuration",
    "content": "Llmcode has many options which can be set with command line switches. Most options can also be set in an .llmcode.conf.yml file which can be placed in your home directory or at the root of your git repo. Or by setting environment variables like LLMCODE_xxx either in your shell or a .env file. Here are 4 equivalent ways of setting an option. With a command line switch: . $ llmcode --dark-mode . Using a .llmcode.conf.yml file: . dark-mode: true . By setting an environment variable: . export LLMCODE_DARK_MODE=true . Using an .env file: . LLMCODE_DARK_MODE=true . See the API key configuration docs for information on how to configure and store your API keys. ",
    "url": "/docs/config.html",
    
    "relUrl": "/docs/config.html"
  },"71": {
    "doc": "Contributing results",
    "title": "Contributing results",
    "content": "Contributions of benchmark results are welcome! See the benchmark README for information on running llmcode’s code editing benchmarks. Submit results by opening a PR with edits to the benchmark results data files. ",
    "url": "/docs/leaderboards/contrib.html",
    
    "relUrl": "/docs/leaderboards/contrib.html"
  },"72": {
    "doc": "Specifying coding conventions",
    "title": "Specifying coding conventions",
    "content": "Sometimes you want GPT to be aware of certain coding guidelines, like whether to provide type hints, which libraries or packages to prefer, etc. The easiest way to do that with llmcode is to simply create a small markdown file and include it in the chat. For example, say we want our python code to: . - Prefer httpx over requests for making http requests. - Use types everywhere possible. We would simply create a file like CONVENTIONS.md with those lines and then we can add it to the llmcode chat, along with the file(s) that we want to edit. It’s best to load the conventions file with /read CONVENTIONS.md or llmcode --read CONVENTIONS.md. This way it is marked as read-only, and cached if prompt caching is enabled. ",
    "url": "/docs/usage/conventions.html",
    
    "relUrl": "/docs/usage/conventions.html"
  },"73": {
    "doc": "Specifying coding conventions",
    "title": "Community contributed conventions",
    "content": "You can check the llmcode conventions repository to find or contribute conventions files. ",
    "url": "/docs/usage/conventions.html#community-contributed-conventions",
    
    "relUrl": "/docs/usage/conventions.html#community-contributed-conventions"
  },"74": {
    "doc": "Specifying coding conventions",
    "title": "Always load conventions",
    "content": "You can also configure llmcode to always load your conventions file in the .llmcode.conf.yml config file: . # alone read: CONVENTIONS.md # multiple files read: [CONVENTIONS.md, anotherfile.txt] . ",
    "url": "/docs/usage/conventions.html#always-load-conventions",
    
    "relUrl": "/docs/usage/conventions.html#always-load-conventions"
  },"75": {
    "doc": "Specifying coding conventions",
    "title": "Example",
    "content": "See below for an example of how the conventions can affect the code that llmcode writes. $ llmcode –read CONVENTIONS.md useragent.py . Llmcode v0.24.2-dev Added CONVENTIONS.md to the chat. Added useragent.py to the chat. write a function that uses httpbin to check the user agent on the request. return the ua. To fulfill your request, I will: . | Add an import for httpx. | Write a function named check_user_agent that makes a GET request to httpbin.org/user-agent and returns the user agent string. | . Here is the diff for the useragent.py file: . --- useragent.py +++ useragent.py @@ ... @@ +import httpx + +def check_user_agent() -&gt; str: + response = httpx.get('https://httpbin.org/user-agent') + return response.json()['user-agent'] . This function uses httpx to make a GET request to httpbin.org/user-agent, which returns a JSON response containing the user agent of the HTTP client. The function then extracts the user agent string from the JSON response and returns it. Applied edit to useragent.py . GPT correctly used httpx and provided type hints. See below for the transcript of this same coding task conducted without using CONVENTIONS.md. In this case, GPT used requests and skipped types, which is perhaps more typical in small python scripts. $ llmcode useragent.py . Llmcode v0.24.2-dev Added useragent.py to the chat. write a function that uses httpbin to check the user agent on the request. return the ua. --- /dev/null +++ useragent.py @@ ... @@ +import requests + +def check_user_agent(): + response = requests.get('https://httpbin.org/user-agent') + user_agent = response.json().get('user-agent') + return user_agent . Applied edit to useragent.py . ",
    "url": "/docs/usage/conventions.html#example",
    
    "relUrl": "/docs/usage/conventions.html#example"
  },"76": {
    "doc": "Copy/paste with web chat",
    "title": "Copy/paste with web chat",
    "content": "Llmcode browser UI demo video ",
    "url": "/docs/usage/copypaste.html",
    
    "relUrl": "/docs/usage/copypaste.html"
  },"77": {
    "doc": "Copy/paste with web chat",
    "title": "Working with an LLM web chat",
    "content": "Llmcode can connect to most LLMs via API and works best that way. But there are times when you may want to work with an LLM via its web chat interface: . | Workplace policies may limit your LLM usage to a proprietary web chat system. | The web chat LLM may have access to unique context or may have been specially fine tuned for your task. | It may be cost prohibitive to use some models via API. | There may not be an API available. | . Llmcode has features for working with an LLM via its web chat interface. This allows you to use the web chat LLM as the “big brain code architect” while running llmcode with a smaller, cheaper LLM to actually make changes to your local files. For this “file editor” part of the process you can run llmcode with many open source, free or very inexpensive LLMs. For example, the demo video above shows llmcode using DeepSeek to apply the changes that o1-preview is suggesting in the web chat. Copy llmcode’s code context to your clipboard, paste into the web UI . The /copy-context &lt;instructions&gt; command can be used in chat to copy llmcode’s code context to your clipboard. It will include: . | All the files which have been added to the chat via /add. | Any read only files which have been added via /read. | Llmcode’s repository map that brings in code context related to the above files from elsewhere in your git repo. | Some instructions to the LLM that ask it to output change instructions concisely. | If you include &lt;instructions&gt;, they will be copied too. | . You can paste the context into your browser, and start interacting with the LLM web chat to ask for code changes. Paste the LLM’s reply back into llmcode to edit your files . Once the LLM has replied, you can use the “copy response” button in the web UI to copy the LLM’s response. Back in llmcode, you can run /paste and llmcode will edit your files to implement the changes suggested by the LLM. You can use a cheap, efficient model like GPT-4o Mini, DeepSeek or Qwen to do these edits. This works best if you run llmcode with --edit-format editor-diff or --edit-format editor-whole. Copy/paste mode . Llmcode has a --copy-paste mode that streamlines this entire process: . | Whenever you /add or /read files, llmcode will automatically copy the entire, updated code context to your clipboard. You’ll see “Copied code context to clipboard” whenever this happens. | When you copy the LLM reply to your clipboard outside llmcode, llmcode will automatically notice and load it into the llmcode chat. Just press ENTER to send the message and llmcode will apply the LLMs changes to your local files. | Llmcode will automatically select the best edit format for this copy/paste functionality. Depending on the LLM you have llmcode use, it will be either editor-whole or editor-diff. | . ",
    "url": "/docs/usage/copypaste.html#working-with-an-llm-web-chat",
    
    "relUrl": "/docs/usage/copypaste.html#working-with-an-llm-web-chat"
  },"78": {
    "doc": "Copy/paste with web chat",
    "title": "Terms of service",
    "content": "Be sure to review the Terms Of Service of any LLM web chat service you use with these features. These features are not intended to be used in violation of any service’s Terms Of Service (TOS). Llmcode’s web chat features have been designed to be compliant with the terms of service of most LLM web chats. There are 4 copy/paste steps involved when coding with an LLM web chat: . | Copy code and context from llmcode. | Paste the code and context into the LLM web chat. | Copy the reply from the LLM web chat. | Paste the LLM reply into llmcode. | . Most LLM web chat TOS prohibit automating steps (2) and (3) where code is copied from and pasted into the web chat. Llmcode’s --copy-paste mode leaves those as 100% manual steps for the user to complete. It simply streamlines steps (1) and (4) that are interactions with llmcode, and which should not be under the scope of an LLM web chat TOS. If you are concerned that the automatic interactions with llmcode in steps (1) and (4) may be problematic with respect to your LLM web chat provider’s TOS, you can forego --copy-paste mode. Instead, manually use the /copy-context and /paste commands if that will keep you in compliance. Again, do not use these features in violation of any service’s Terms Of Service. ",
    "url": "/docs/usage/copypaste.html#terms-of-service",
    
    "relUrl": "/docs/usage/copypaste.html#terms-of-service"
  },"79": {
    "doc": "Complete a css exercise with llmcode",
    "title": "Complete a css exercise with llmcode",
    "content": "In this chat transcript, the user asks about the open source CSS Exercises repo and completes an animation exercise. $ llmcode . what is this repo? . Based on the file list provided, it seems like this repository contains a collection of CSS and HTML exercises, solutions, and examples. The exercises cover various topics such as Flexbox, Grid, Animation, Foundations, and Margin &amp; Padding. The repository also includes images and other assets used in the exercises. /add animation/03-dropdown-menu/README.md . Added animation/03-dropdown-menu/README.md to the chat . Yes, I see the content of the animation/03-dropdown-menu/README.md file. It describes an exercise to add animation to a dropdown menu, creating a bounce illusion when the dropdown expands close to its final end state. The desired outcome is shown in a GIF file, and the hints provided suggest using the transform-origin property and adding an intermediate step to the keyframe at-rule to achieve the bounce effect. Is there any specific change or improvement you would like me to make to this file or any other file in the repository? . can you make the changes suggested in the readme? . To make the changes suggested in the animation/03-dropdown-menu/README.md, I need to see the contents of the following files: . | animation/03-dropdown-menu/style.css | animation/03-dropdown-menu/script.js | . Please provide the contents of these files so I can suggest the necessary changes. animation/03-dropdown-menu/style.css animation/03-dropdown-menu/script.js Add these files to the chat? y . Apologies for the oversight. I see the content of the animation/03-dropdown-menu/style.css and animation/03-dropdown-menu/script.js files now. I will suggest the necessary changes to add the bounce animation to the dropdown menu. animation/03-dropdown-menu/style.css &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL .visible { display: block; } ======= .visible { display: block; animation: expand 0.3s ease-out; transform-origin: top; } @keyframes expand { 0% { transform: scaleY(0); } 80% { transform: scaleY(1.1); } 100% { transform: scaleY(1); } } &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . These changes add the bounce animation to the dropdown menu by creating a new keyframe animation called expand. The transform-origin property is set to top to make the dropdown menu start transforming from the top. The intermediate step at 80% of the animation creates the bounce illusion. Applied edit to animation/03-dropdown-menu/style.css Commit bb3a813 llmcode: Added bounce animation to dropdown menu. that worked! . I’m glad the changes worked for you! If you have any more requests or need assistance with other parts of the code, feel free to ask. I’m here to help! . ",
    "url": "/examples/css-exercises.html",
    
    "relUrl": "/examples/css-exercises.html"
  },"80": {
    "doc": "Improving GPT-4's codebase understanding with ctags",
    "title": "Improving GPT-4’s codebase understanding with ctags",
    "content": ". ",
    "url": "/docs/ctags.html#improving-gpt-4s-codebase-understanding-with-ctags",
    
    "relUrl": "/docs/ctags.html#improving-gpt-4s-codebase-understanding-with-ctags"
  },"81": {
    "doc": "Improving GPT-4's codebase understanding with ctags",
    "title": "Updated",
    "content": "Llmcode no longer uses ctags to build a repo map. Please see the newer article about using tree-sitter to build a better repo map. GPT-4 is extremely useful for “self-contained” coding tasks, like generating brand new code or modifying a pure function that has no dependencies. But it’s difficult to use GPT-4 to modify or extend a large, complex pre-existing codebase. To modify such code, GPT needs to understand the dependencies and APIs which interconnect its subsystems. Somehow we need to provide this “code context” to GPT when we ask it to accomplish a coding task. Specifically, we need to: . | Help GPT understand the overall codebase, so that it can decifer the meaning of code with complex dependencies and generate new code that respects and utilizes existing abstractions. | Convey all of this “code context” to GPT in an efficient manner that fits within the 8k-token context window. | . To address these issues, llmcode now sends GPT a concise map of your whole git repository that includes all declared variables and functions with call signatures. This repo map is built automatically using ctags, which extracts symbol definitions from source files. Historically, ctags were generated and indexed by IDEs and editors to help humans search and navigate large codebases. Instead, we’re going to use ctags to help GPT better comprehend, navigate and edit code in larger repos. To get a sense of how effective this can be, this chat transcript shows GPT-4 creating a black box test case, without being given access to the source code of the function being tested or any of the other code in the repo. Using only the meta-data in the repo map, GPT is able to figure out how to call the method to be tested, as well as how to instantiate multiple class objects that are required to prepare for the test. To code with GPT-4 using the techniques discussed here: . | Install llmcode. | Install universal ctags. | Run llmcode inside your repo, and it should say “Repo-map: universal-ctags using 1024 tokens”. | . ",
    "url": "/docs/ctags.html#updated",
    
    "relUrl": "/docs/ctags.html#updated"
  },"82": {
    "doc": "Improving GPT-4's codebase understanding with ctags",
    "title": "The problem: code context",
    "content": "GPT-4 is great at “self contained” coding tasks, like writing or modifying a pure function with no external dependencies. GPT can easily handle requests like “write a Fibonacci function” or “rewrite the loop using list comprehensions”, because they require no context beyond the code being discussed. Most real code is not pure and self-contained, it is intertwined with and depends on code from many different files in a repo. If you ask GPT to “switch all the print statements in class Foo to use the BarLog logging system”, it needs to see the code in the Foo class with the prints, and it also needs to understand the project’s BarLog subsystem. A simple solution is to send the entire codebase to GPT along with each change request. Now GPT has all the context! But this won’t work for even moderately sized repos, because they won’t fit into the 8k-token context window. A better approach is to be selective, and hand pick which files to send. For the example above, you could send the file that contains the Foo class and the file that contains the BarLog logging subsystem. This works pretty well, and is supported by llmcode – you can manually specify which files to “add to the chat” you are having with GPT. But it’s not ideal to have to manually identify the right set of files to add to the chat. And sending whole files is a bulky way to send code context, wasting the precious 8k context window. GPT doesn’t need to see the entire implementation of BarLog, it just needs to understand it well enough to use it. You may quickly run out of context window if you send many files worth of code just to convey context. ",
    "url": "/docs/ctags.html#the-problem-code-context",
    
    "relUrl": "/docs/ctags.html#the-problem-code-context"
  },"83": {
    "doc": "Improving GPT-4's codebase understanding with ctags",
    "title": "Using a repo map to provide context",
    "content": "The latest version of llmcode sends a repo map to GPT along with each change request. The map contains a list of all the files in the repo, along with the symbols which are defined in each file. Callables like functions and methods also include their signatures. Here’s a sample of the map of the llmcode repo, just showing the maps of main.py and io.py : . llmcode/ ... main.py: function main (args=None, input=None, output=None) variable status ... io.py: class FileContentCompleter InputOutput FileContentCompleter member __init__ (self, fnames, commands) get_completions (self, document, complete_event) InputOutput member __init__ (self, pretty, yes, input_history_file=None, chat_history_file=None, input=None, output=None) ai_output (self, content) append_chat_history (self, text, linebreak=False, blockquote=False) confirm_ask (self, question, default=\"y\") get_input (self, fnames, commands) prompt_ask (self, question, default=None) tool (self, *messages, log_only=False) tool_error (self, message) ... Mapping out the repo like this provides some benefits: . | GPT can see variables, classes, methods and function signatures from everywhere in the repo. This alone may give it enough context to solve many tasks. For example, it can probably figure out how to use the API exported from a module just based on the details shown in the map. | If it needs to see more code, GPT can use the map to figure out by itself which files it needs to look at. GPT will then ask to see these specific files, and llmcode will automatically add them to the chat context (with user approval). | . Of course, for large repositories even just the map might be too large for the context window. However, this mapping approach opens up the ability to collaborate with GPT-4 on larger codebases than previous methods. It also reduces the need to manually curate which files to add to the chat context, empowering GPT to autonomously identify relevant files for the task at hand. ",
    "url": "/docs/ctags.html#using-a-repo-map-to-provide-context",
    
    "relUrl": "/docs/ctags.html#using-a-repo-map-to-provide-context"
  },"84": {
    "doc": "Improving GPT-4's codebase understanding with ctags",
    "title": "Using ctags to make the map",
    "content": "Under the hood, llmcode uses universal ctags to build the map. Universal ctags can scan source code written in many languages, and extract data about all the symbols defined in each file. Historically, ctags were generated and indexed by IDEs or code editors to make it easier for a human to search and navigate a codebase, find the implementation of functions, etc. Instead, we’re going to use ctags to help GPT navigate and understand the codebase. Here is the type of output you get when you run ctags on source code. Specifically, this is the ctags --fields=+S --output-format=json output for the main.py file mapped above: . { \"_type\": \"tag\", \"name\": \"main\", \"path\": \"llmcode/main.py\", \"pattern\": \"/^def main(args=None, input=None, output=None):$/\", \"kind\": \"function\", \"signature\": \"(args=None, input=None, output=None)\" } { \"_type\": \"tag\", \"name\": \"status\", \"path\": \"llmcode/main.py\", \"pattern\": \"/^ status = main()$/\", \"kind\": \"variable\" } . The repo map is built using this type of ctags data, but formatted into the space efficient hierarchical tree format shown earlier. This is a format that GPT can easily understand and which conveys the map data using a minimal number of tokens. ",
    "url": "/docs/ctags.html#using-ctags-to-make-the-map",
    
    "relUrl": "/docs/ctags.html#using-ctags-to-make-the-map"
  },"85": {
    "doc": "Improving GPT-4's codebase understanding with ctags",
    "title": "Example chat transcript",
    "content": "This chat transcript shows GPT-4 creating a black box test case, without being given access to the source code of the function being tested or any of the other code in the repo. Instead, GPT is operating solely off the repo map. Using only the meta-data in the map, GPT is able to figure out how to call the method to be tested, as well as how to instantiate multiple class objects that are required to prepare for the test. GPT makes one reasonable mistake writing the first version of the test, but is able to quickly fix the issue after being shown the pytest error output. ",
    "url": "/docs/ctags.html#example-chat-transcript",
    
    "relUrl": "/docs/ctags.html#example-chat-transcript"
  },"86": {
    "doc": "Improving GPT-4's codebase understanding with ctags",
    "title": "Future work",
    "content": "Just as “send the whole codebase to GPT with every request” is not an efficient solution to this problem, there are probably better approaches than “send the whole repo map with every request”. Sending an appropriate subset of the repo map would help llmcode work better with even larger repositories which have large maps. Some possible approaches to reducing the amount of map data are: . | Distill the global map, to prioritize important symbols and discard “internal” or otherwise less globally relevant identifiers. Possibly enlist gpt-3.5-turbo to perform this distillation in a flexible and language agnostic way. | Provide a mechanism for GPT to start with a distilled subset of the global map, and let it ask to see more detail about subtrees or keywords that it feels are relevant to the current coding task. | Attempt to analyze the natural language coding task given by the user and predict which subset of the repo map is relevant. Possibly by analysis of prior coding chats within the specific repo. Work on certain files or types of features may require certain somewhat predictable context from elsewhere in the repo. Vector and keyword search against the chat history, repo map or codebase may help here. | . One key goal is to prefer solutions which are language agnostic or which can be easily deployed against most popular code languages. The ctags solution has this benefit, since it comes pre-built with support for most popular languages. I suspect that Language Server Protocol might be an even better tool than ctags for this problem. But it is more cumbersome to deploy for a broad array of languages. Users would need to stand up an LSP server for their specific language(s) of interest. ",
    "url": "/docs/ctags.html#future-work",
    
    "relUrl": "/docs/ctags.html#future-work"
  },"87": {
    "doc": "Improving GPT-4's codebase understanding with ctags",
    "title": "Try it out",
    "content": "To use this experimental repo map feature: . | Install llmcode. | Install ctags. | Run llmcode inside your repo, and it should say “Repo-map: universal-ctags using 1024 tokens”. | . ",
    "url": "/docs/ctags.html#try-it-out",
    
    "relUrl": "/docs/ctags.html#try-it-out"
  },"88": {
    "doc": "Improving GPT-4's codebase understanding with ctags",
    "title": "Improving GPT-4's codebase understanding with ctags",
    "content": " ",
    "url": "/docs/ctags.html",
    
    "relUrl": "/docs/ctags.html"
  },"89": {
    "doc": "DeepSeek",
    "title": "DeepSeek",
    "content": "Llmcode can connect to the DeepSeek.com API. To work with DeepSeek’s models, you need to set the DEEPSEEK_API_KEY environment variable with your DeepSeek API key. The DeepSeek Chat V3 model has a top score on llmcode’s code editing benchmark. python -m pip install -U llmcode export DEEPSEEK_API_KEY=&lt;key&gt; # Mac/Linux setx DEEPSEEK_API_KEY &lt;key&gt; # Windows, restart shell after setx # Use DeepSeek Chat v3 llmcode --deepseek . ",
    "url": "/docs/llms/deepseek.html",
    
    "relUrl": "/docs/llms/deepseek.html"
  },"90": {
    "doc": "Llmcode with docker",
    "title": "Llmcode with docker",
    "content": "Llmcode is available as 2 docker images: . | khulnasoft/llmcode installs the llmcode core, a smaller image that’s good to get started quickly. | khulnasoft/llmcode-full installs llmcode will all the optional extras. | . The full image has support for features like interactive help, the browser GUI and support for using Playwright to scrape web pages. The core image can still use these features, but they will need to be installed the first time you access them. Since containers are ephemeral, the extras will need to be reinstalled the next time you launch the llmcode core container. Llmcode core . docker pull khulnasoft/llmcode docker run -it --user $(id -u):$(id -g) --volume $(pwd):/app khulnasoft/llmcode --openai-api-key $OPENAI_API_KEY [...other llmcode args...] . Full version . docker pull khulnasoft/llmcode-full docker run -it --user $(id -u):$(id -g) --volume $(pwd):/app khulnasoft/llmcode-full --openai-api-key $OPENAI_API_KEY [...other llmcode args...] . ",
    "url": "/docs/install/docker.html",
    
    "relUrl": "/docs/install/docker.html"
  },"91": {
    "doc": "Llmcode with docker",
    "title": "How to use it",
    "content": "You should run the above commands from the root of your git repo, since the --volume arg maps your current directory into the docker container. Given that, you need to be in the root of your git repo for llmcode to be able to see the repo and all its files. You should be sure your that git repo config contains your user name and email, since the docker container won’t have your global git config. Run these commands while in your git repo, before you do the docker run command: . git config user.email \"you@example.com\" git config user.name \"Your Name\" . ",
    "url": "/docs/install/docker.html#how-to-use-it",
    
    "relUrl": "/docs/install/docker.html#how-to-use-it"
  },"92": {
    "doc": "Llmcode with docker",
    "title": "Limitations",
    "content": ". | When you use the in-chat /run command, it will be running shell commands inside the docker container. So those commands won’t be running in your local environment, which may make it tricky to /run tests, etc for your project. | The /voice command won’t work unless you can figure out how to give the docker container access to your host audio device. The container has libportaudio2 installed, so it should work if you can do that. | . ",
    "url": "/docs/install/docker.html#limitations",
    
    "relUrl": "/docs/install/docker.html#limitations"
  },"93": {
    "doc": "Config with .env",
    "title": "Config with .env",
    "content": "You can use a .env file to store API keys and other settings for the models you use with llmcode. You can also set many general llmcode options in the .env file. Llmcode will look for a .env file in these locations: . | Your home directory. | The root of your git repo. | The current directory. | As specified with the --env-file &lt;filename&gt; parameter. | . If the files above exist, they will be loaded in that order. Files loaded last will take priority. See the API key configuration docs for information on how to configure and store your API keys. ",
    "url": "/docs/config/dotenv.html",
    
    "relUrl": "/docs/config/dotenv.html"
  },"94": {
    "doc": "Config with .env",
    "title": "Sample .env file",
    "content": "Below is a sample .env file, which you can also download from GitHub. ########################################################## # Sample llmcode .env file. # Place at the root of your git repo. # Or use `llmcode --env &lt;fname&gt;` to specify. ########################################################## ################# # LLM parameters: # # Include xxx_API_KEY parameters and other params needed for your LLMs. # See https://llmcode.khulnasoft.com/docs/llms.html for details. ## OpenAI #OPENAI_API_KEY= ## Anthropic #ANTHROPIC_API_KEY= ##... ############# # Main model: ## Specify the model to use for the main chat #LLMCODE_MODEL= ## Use claude-3-opus-20240229 model for the main chat #LLMCODE_OPUS= ## Use anthropic/claude-3-7-sonnet-20250219 model for the main chat #LLMCODE_SONNET= ## Use claude-3-5-haiku-20241022 model for the main chat #LLMCODE_HAIKU= ## Use gpt-4-0613 model for the main chat #LLMCODE_4= ## Use gpt-4o model for the main chat #LLMCODE_4O= ## Use gpt-4o-mini model for the main chat #LLMCODE_MINI= ## Use gpt-4-1106-preview model for the main chat #LLMCODE_4_TURBO= ## Use gpt-3.5-turbo model for the main chat #LLMCODE_35TURBO= ## Use deepseek/deepseek-chat model for the main chat #LLMCODE_DEEPSEEK= ## Use o1-mini model for the main chat #LLMCODE_O1_MINI= ## Use o1-preview model for the main chat #LLMCODE_O1_PREVIEW= ######################## # API Keys and settings: ## Specify the OpenAI API key #LLMCODE_OPENAI_API_KEY= ## Specify the Anthropic API key #LLMCODE_ANTHROPIC_API_KEY= ## Specify the api base url #LLMCODE_OPENAI_API_BASE= ## (deprecated, use --set-env OPENAI_API_TYPE=&lt;value&gt;) #LLMCODE_OPENAI_API_TYPE= ## (deprecated, use --set-env OPENAI_API_VERSION=&lt;value&gt;) #LLMCODE_OPENAI_API_VERSION= ## (deprecated, use --set-env OPENAI_API_DEPLOYMENT_ID=&lt;value&gt;) #LLMCODE_OPENAI_API_DEPLOYMENT_ID= ## (deprecated, use --set-env OPENAI_ORGANIZATION=&lt;value&gt;) #LLMCODE_OPENAI_ORGANIZATION_ID= ## Set an environment variable (to control API settings, can be used multiple times) #LLMCODE_SET_ENV= ## Set an API key for a provider (eg: --api-key provider=&lt;key&gt; sets PROVIDER_API_KEY=&lt;key&gt;) #LLMCODE_API_KEY= ################# # Model settings: ## List known models which match the (partial) MODEL name #LLMCODE_LIST_MODELS= ## Specify a file with llmcode model settings for unknown models #LLMCODE_MODEL_SETTINGS_FILE=.llmcode.model.settings.yml ## Specify a file with context window and costs for unknown models #LLMCODE_MODEL_METADATA_FILE=.llmcode.model.metadata.json ## Add a model alias (can be used multiple times) #LLMCODE_ALIAS= ## Set the reasoning_effort API parameter (default: not set) #LLMCODE_REASONING_EFFORT= ## Verify the SSL cert when connecting to models (default: True) #LLMCODE_VERIFY_SSL=true ## Timeout in seconds for API calls (default: None) #LLMCODE_TIMEOUT= ## Specify what edit format the LLM should use (default depends on model) #LLMCODE_EDIT_FORMAT= ## Use architect edit format for the main chat #LLMCODE_ARCHITECT= ## Specify the model to use for commit messages and chat history summarization (default depends on --model) #LLMCODE_WEAK_MODEL= ## Specify the model to use for editor tasks (default depends on --model) #LLMCODE_EDITOR_MODEL= ## Specify the edit format for the editor model (default: depends on editor model) #LLMCODE_EDITOR_EDIT_FORMAT= ## Only work with models that have meta-data available (default: True) #LLMCODE_SHOW_MODEL_WARNINGS=true ## Soft limit on tokens for chat history, after which summarization begins. If unspecified, defaults to the model's max_chat_history_tokens. #LLMCODE_MAX_CHAT_HISTORY_TOKENS= ################# # Cache settings: ## Enable caching of prompts (default: False) #LLMCODE_CACHE_PROMPTS=false ## Number of times to ping at 5min intervals to keep prompt cache warm (default: 0) #LLMCODE_CACHE_KEEPALIVE_PINGS=false ################### # Repomap settings: ## Suggested number of tokens to use for repo map, use 0 to disable #LLMCODE_MAP_TOKENS= ## Control how often the repo map is refreshed. Options: auto, always, files, manual (default: auto) #LLMCODE_MAP_REFRESH=auto ## Multiplier for map tokens when no files are specified (default: 2) #LLMCODE_MAP_MULTIPLIER_NO_FILES=true ################ # History Files: ## Specify the chat input history file (default: .llmcode.input.history) #LLMCODE_INPUT_HISTORY_FILE=.llmcode.input.history ## Specify the chat history file (default: .llmcode.khulnasoft.com.history.md) #LLMCODE_CHAT_HISTORY_FILE=.llmcode.khulnasoft.com.history.md ## Restore the previous chat history messages (default: False) #LLMCODE_RESTORE_CHAT_HISTORY=false ## Log the conversation with the LLM to this file (for example, .llmcode.llm.history) #LLMCODE_LLM_HISTORY_FILE= ################## # Output settings: ## Use colors suitable for a dark terminal background (default: False) #LLMCODE_DARK_MODE=false ## Use colors suitable for a light terminal background (default: False) #LLMCODE_LIGHT_MODE=false ## Enable/disable pretty, colorized output (default: True) #LLMCODE_PRETTY=true ## Enable/disable streaming responses (default: True) #LLMCODE_STREAM=true ## Set the color for user input (default: #00cc00) #LLMCODE_USER_INPUT_COLOR=#00cc00 ## Set the color for tool output (default: None) #LLMCODE_TOOL_OUTPUT_COLOR= ## Set the color for tool error messages (default: #FF2222) #LLMCODE_TOOL_ERROR_COLOR=#FF2222 ## Set the color for tool warning messages (default: #FFA500) #LLMCODE_TOOL_WARNING_COLOR=#FFA500 ## Set the color for assistant output (default: #0088ff) #LLMCODE_ASSISTANT_OUTPUT_COLOR=#0088ff ## Set the color for the completion menu (default: terminal's default text color) #LLMCODE_COMPLETION_MENU_COLOR= ## Set the background color for the completion menu (default: terminal's default background color) #LLMCODE_COMPLETION_MENU_BG_COLOR= ## Set the color for the current item in the completion menu (default: terminal's default background color) #LLMCODE_COMPLETION_MENU_CURRENT_COLOR= ## Set the background color for the current item in the completion menu (default: terminal's default text color) #LLMCODE_COMPLETION_MENU_CURRENT_BG_COLOR= ## Set the markdown code theme (default: default, other options include monokai, solarized-dark, solarized-light, or a Pygments builtin style, see https://pygments.org/styles for available themes) #LLMCODE_CODE_THEME=default ## Show diffs when committing changes (default: False) #LLMCODE_SHOW_DIFFS=false ############### # Git settings: ## Enable/disable looking for a git repo (default: True) #LLMCODE_GIT=true ## Enable/disable adding .llmcode* to .gitignore (default: True) #LLMCODE_GITIGNORE=true ## Specify the llmcode ignore file (default: .llmcodeignore in git root) #LLMCODE_LLMCODEIGNORE=.llmcodeignore ## Only consider files in the current subtree of the git repository #LLMCODE_SUBTREE_ONLY=false ## Enable/disable auto commit of LLM changes (default: True) #LLMCODE_AUTO_COMMITS=true ## Enable/disable commits when repo is found dirty (default: True) #LLMCODE_DIRTY_COMMITS=true ## Attribute llmcode code changes in the git author name (default: True) #LLMCODE_ATTRIBUTE_AUTHOR=true ## Attribute llmcode commits in the git committer name (default: True) #LLMCODE_ATTRIBUTE_COMMITTER=true ## Prefix commit messages with 'llmcode: ' if llmcode authored the changes (default: False) #LLMCODE_ATTRIBUTE_COMMIT_MESSAGE_AUTHOR=false ## Prefix all commit messages with 'llmcode: ' (default: False) #LLMCODE_ATTRIBUTE_COMMIT_MESSAGE_COMMITTER=false ## Commit all pending changes with a suitable commit message, then exit #LLMCODE_COMMIT=false ## Specify a custom prompt for generating commit messages #LLMCODE_COMMIT_PROMPT= ## Perform a dry run without modifying files (default: False) #LLMCODE_DRY_RUN=false ## Skip the sanity check for the git repository (default: False) #LLMCODE_SKIP_SANITY_CHECK_REPO=false ## Enable/disable watching files for ai coding comments (default: False) #LLMCODE_WATCH_FILES=false ######################## # Fixing and committing: ## Lint and fix provided files, or dirty files if none provided #LLMCODE_LINT=false ## Specify lint commands to run for different languages, eg: \"python: flake8 --select=...\" (can be used multiple times) #LLMCODE_LINT_CMD= ## Enable/disable automatic linting after changes (default: True) #LLMCODE_AUTO_LINT=true ## Specify command to run tests #LLMCODE_TEST_CMD= ## Enable/disable automatic testing after changes (default: False) #LLMCODE_AUTO_TEST=false ## Run tests, fix problems found and then exit #LLMCODE_TEST=false ############ # Analytics: ## Enable/disable analytics for current session (default: random) #LLMCODE_ANALYTICS= ## Specify a file to log analytics events #LLMCODE_ANALYTICS_LOG= ## Permanently disable analytics #LLMCODE_ANALYTICS_DISABLE=false ############ # Upgrading: ## Check for updates and return status in the exit code #LLMCODE_JUST_CHECK_UPDATE=false ## Check for new llmcode versions on launch #LLMCODE_CHECK_UPDATE=true ## Show release notes on first run of new version (default: None, ask user) #LLMCODE_SHOW_RELEASE_NOTES= ## Install the latest version from the main branch #LLMCODE_INSTALL_MAIN_BRANCH=false ## Upgrade llmcode to the latest version from PyPI #LLMCODE_UPGRADE=false ######## # Modes: ## Specify a single message to send the LLM, process reply then exit (disables chat mode) #LLMCODE= ## Specify a file containing the message to send the LLM, process reply, then exit (disables chat mode) #LLMCODE_FILE= ## Run llmcode in your browser (default: False) #LLMCODE_GUI=false ## Enable automatic copy/paste of chat between llmcode and web UI (default: False) #LLMCODE_COPY_PASTE=false ## Apply the changes from the given file instead of running the chat (debug) #LLMCODE_APPLY= ## Apply clipboard contents as edits using the main model's editor format #LLMCODE_APPLY_CLIPBOARD_EDITS=false ## Do all startup activities then exit before accepting user input (debug) #LLMCODE_EXIT=false ## Print the repo map and exit (debug) #LLMCODE_SHOW_REPO_MAP=false ## Print the system prompts and exit (debug) #LLMCODE_SHOW_PROMPTS=false ################# # Voice settings: ## Audio format for voice recording (default: wav). webm and mp3 require ffmpeg #LLMCODE_VOICE_FORMAT=wav ## Specify the language for voice using ISO 639-1 code (default: auto) #LLMCODE_VOICE_LANGUAGE=en ## Specify the input device name for voice recording #LLMCODE_VOICE_INPUT_DEVICE= ################# # Other settings: ## specify a file to edit (can be used multiple times) #LLMCODE_FILE= ## specify a read-only file (can be used multiple times) #LLMCODE_READ= ## Use VI editing mode in the terminal (default: False) #LLMCODE_VIM=false ## Specify the language to use in the chat (default: None, uses system settings) #LLMCODE_CHAT_LANGUAGE= ## Always say yes to every confirmation #LLMCODE_YES_ALWAYS= ## Enable verbose output #LLMCODE_VERBOSE=false ## Load and execute /commands from a file on launch #LLMCODE_LOAD= ## Specify the encoding for input and output (default: utf-8) #LLMCODE_ENCODING=utf-8 ## Line endings to use when writing files (default: platform) #LLMCODE_LINE_ENDINGS=platform ## Specify the .env file to load (default: .env in git root) #LLMCODE_ENV_FILE=.env ## Enable/disable suggesting shell commands (default: True) #LLMCODE_SUGGEST_SHELL_COMMANDS=true ## Enable/disable fancy input with history and completion (default: True) #LLMCODE_FANCY_INPUT=true ## Enable/disable multi-line input mode with Meta-Enter to submit (default: False) #LLMCODE_MULTILINE=false ## Enable/disable detection and offering to add URLs to chat (default: True) #LLMCODE_DETECT_URLS=true ## Specify which editor to use for the /editor command #LLMCODE_EDITOR= ## Install the tree_sitter_language_pack (experimental) #LLMCODE_INSTALL_TREE_SITTER_LANGUAGE_PACK=false . ",
    "url": "/docs/config/dotenv.html#sample-env-file",
    
    "relUrl": "/docs/config/dotenv.html#sample-env-file"
  },"95": {
    "doc": "File editing problems",
    "title": "File editing problems",
    "content": "Sometimes the LLM will reply with some code changes that don’t get applied to your local files. In these cases, llmcode might say something like “Failed to apply edit to filename” or other error messages. This usually happens because the LLM is disobeying the system prompts and trying to make edits in a format that llmcode doesn’t expect. Llmcode makes every effort to get the LLM to conform, and works hard to deal with LLM edits that are “almost” correctly formatted. But sometimes the LLM just won’t cooperate. In these cases, here are some things you might try. ",
    "url": "/docs/troubleshooting/edit-errors.html",
    
    "relUrl": "/docs/troubleshooting/edit-errors.html"
  },"96": {
    "doc": "File editing problems",
    "title": "Don’t add too many files",
    "content": "Many LLMs now have very large context windows, but filling them with irrelevant code or conversation can confuse the model. Above about 25k tokens of context, most models start to become distracted and become less likely to conform to their system prompt. | Don’t add too many files to the chat, just add the files you think need to be edited. Llmcode also sends the LLM a map of your entire git repo, so other relevant code will be included automatically. | Use /drop to remove files from the chat session which aren’t needed for the task at hand. This will reduce distractions and may help the LLM produce properly formatted edits. | Use /clear to remove the conversation history, again to help the LLM focus. | Use /tokens to see how many tokens you are using for each message. | . ",
    "url": "/docs/troubleshooting/edit-errors.html#dont-add-too-many-files",
    
    "relUrl": "/docs/troubleshooting/edit-errors.html#dont-add-too-many-files"
  },"97": {
    "doc": "File editing problems",
    "title": "Use a more capable model",
    "content": "If possible try using GPT-4o, Claude 3.5 Sonnet, DeepSeek V3 or DeepSeek R1. They are the strongest and most capable models. Weaker models are more prone to disobeying the system prompt instructions. Most local models are just barely capable of working with llmcode, so editing errors are probably unavoidable. ",
    "url": "/docs/troubleshooting/edit-errors.html#use-a-more-capable-model",
    
    "relUrl": "/docs/troubleshooting/edit-errors.html#use-a-more-capable-model"
  },"98": {
    "doc": "File editing problems",
    "title": "Local models: context window and quantization",
    "content": "Be especially careful about the Ollama context window when working with local models. It defaults to be very small and silently discards data if you exceed it. Local models which have been quantized are more likely to have editing problems because they are not capable enough to follow llmcode’s system prompts. ",
    "url": "/docs/troubleshooting/edit-errors.html#local-models-context-window-and-quantization",
    
    "relUrl": "/docs/troubleshooting/edit-errors.html#local-models-context-window-and-quantization"
  },"99": {
    "doc": "File editing problems",
    "title": "Try the whole edit format",
    "content": "Run llmcode with --edit-format whole if were using a different edit format. You can see which edit format it is using in the announce lines: . Llmcode v0.50.2-dev Models: claude-3-5-sonnet-20240620 with ♾️ diff edit format . ",
    "url": "/docs/troubleshooting/edit-errors.html#try-the-whole-edit-format",
    
    "relUrl": "/docs/troubleshooting/edit-errors.html#try-the-whole-edit-format"
  },"100": {
    "doc": "File editing problems",
    "title": "Try architect mode",
    "content": "Run llmcode with --architect or /chat-mode architect to enable architect mode. This mode first proposes changes, then uses a separate model to handle the file edits. This two-step process often produces more reliable edits, especially with models that have trouble following edit format instructions. ",
    "url": "/docs/troubleshooting/edit-errors.html#try-architect-mode",
    
    "relUrl": "/docs/troubleshooting/edit-errors.html#try-architect-mode"
  },"101": {
    "doc": "File editing problems",
    "title": "More help",
    "content": "If you need more help, please check our GitHub issues and file a new issue if your problem isn’t discussed. Or drop into our Discord to chat with us. When reporting problems, it is very helpful if you can provide: . | Llmcode version | LLM model you are using | . Including the “announcement” lines that llmcode prints at startup is an easy way to share this helpful info. Llmcode v0.37.1-dev Models: gpt-4o with diff edit format, weak model gpt-3.5-turbo Git repo: .git with 243 files Repo-map: using 1024 tokens . Use /help &lt;question&gt; to ask for help about using llmcode, customizing settings, troubleshooting, using LLMs, etc. ",
    "url": "/docs/troubleshooting/edit-errors.html#more-help",
    
    "relUrl": "/docs/troubleshooting/edit-errors.html#more-help"
  },"102": {
    "doc": "Edit formats",
    "title": "Edit formats",
    "content": "Llmcode uses various “edit formats” to let LLMs edit source files. Different models work better or worse with different edit formats. Llmcode is configured to use the optimal format for most popular, common models. You can always force use of a specific edit format with the --edit-format switch. ",
    "url": "/docs/more/edit-formats.html",
    
    "relUrl": "/docs/more/edit-formats.html"
  },"103": {
    "doc": "Edit formats",
    "title": "whole",
    "content": "The “whole” edit format is the simplest possible editing format. The LLM is instructed to return a full, updated copy of each source file that needs changes. While simple, it can be slow and costly because the LLM has to return the entire file even if just a few lines are edited. The whole format expects the file path just before the fenced file content: . show_greeting.py ``` import sys def greeting(name): print(\"Hey\", name) if __name__ == '__main__': greeting(sys.argv[1]) ``` . ",
    "url": "/docs/more/edit-formats.html#whole",
    
    "relUrl": "/docs/more/edit-formats.html#whole"
  },"104": {
    "doc": "Edit formats",
    "title": "diff",
    "content": "The “diff” edit format asks the LLM to specify file edits as a series of search/replace blocks. This is an efficient format, because the model only needs to return parts of the file which have changes. Edits are formatted using a syntax similar to the git merge conflict resolution markings, with the file path right before a fenced block: . mathweb/flask/app.py ``` &lt;&lt;&lt;&lt;&lt;&lt;&lt; SEARCH from flask import Flask ======= import math from flask import Flask &gt;&gt;&gt;&gt;&gt;&gt;&gt; REPLACE ``` . ",
    "url": "/docs/more/edit-formats.html#diff",
    
    "relUrl": "/docs/more/edit-formats.html#diff"
  },"105": {
    "doc": "Edit formats",
    "title": "diff-fenced",
    "content": "The “diff-fenced” edit format is based on the diff format, but the file path is placed inside the fence. It is primarily used with the Gemini family of models, which often fail to conform to the fencing approach specified in the diff format. ``` mathweb/flask/app.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; SEARCH from flask import Flask ======= import math from flask import Flask &gt;&gt;&gt;&gt;&gt;&gt;&gt; REPLACE ``` . ",
    "url": "/docs/more/edit-formats.html#diff-fenced",
    
    "relUrl": "/docs/more/edit-formats.html#diff-fenced"
  },"106": {
    "doc": "Edit formats",
    "title": "udiff",
    "content": "The “udiff” edit format is based on the widely used unified diff format, but modified and simplified. This is an efficient format, because the model only needs to return parts of the file which have changes. It was mainly used to the GPT-4 Turbo family of models, because it reduced their “lazy coding” tendencies. With other edit formats the GPT-4 Turbo models tended to elide large sections of code and replace them with “# … original code here …” style comments. ```diff --- mathweb/flask/app.py +++ mathweb/flask/app.py @@ ... @@ -class MathWeb: +import sympy + +class MathWeb: ``` . ",
    "url": "/docs/more/edit-formats.html#udiff",
    
    "relUrl": "/docs/more/edit-formats.html#udiff"
  },"107": {
    "doc": "Edit formats",
    "title": "editor-diff and editor-whole",
    "content": "These are streamlined versions of the diff and whole formats, intended to be used with --editor-edit-format when using architect mode. The actual edit format is the same, but llmcode uses a simpler prompt that is more narrowly focused on just editing the file as opposed to solving the coding task. The architect model resolves the coding task and provides plain text instructions about which file changes need to be made. The editor interprets those instructions to produce the syntactically correct diff or whole edits. ",
    "url": "/docs/more/edit-formats.html#editor-diff-and-editor-whole",
    
    "relUrl": "/docs/more/edit-formats.html#editor-diff-and-editor-whole"
  },"108": {
    "doc": "Code editing leaderboard",
    "title": "Code editing leaderboard",
    "content": "This old llmcode code editing leaderboard has been replaced by the new, much more challenging polyglot leaderboard. Llmcode’s code editing benchmark asks the LLM to edit python source files to complete 133 small coding exercises from Exercism. This measures the LLM’s coding ability, and whether it can write new code that integrates into existing code. The model also has to successfully apply all its changes to the source file without human intervention. | Model | Percent completed correctly | Percent using correct edit format | Command | Edit format | . | o1 | 84.2% | 99.2% | llmcode --model openrouter/openai/o1 | diff | . | claude-3-5-sonnet-20241022 | 84.2% | 99.2% | llmcode --model anthropic/claude-3-5-sonnet-20241022 | diff | . | gemini-exp-1206 (whole) | 80.5% | 100.0% | llmcode --model gemini/gemini-exp-1206 | whole | . | o1-preview | 79.7% | 93.2% | llmcode --model o1-preview | diff | . | claude-3.5-sonnet-20240620 | 77.4% | 99.2% | llmcode --model claude-3.5-sonnet-20240620 | diff | . | claude-3-5-haiku-20241022 | 75.2% | 95.5% | llmcode --model anthropic/claude-3-5-haiku-20241022 | diff | . | ollama/qwen2.5-coder:32b | 72.9% | 100.0% | llmcode --model ollama/qwen2.5-coder:32b | whole | . | DeepSeek Coder V2 0724 | 72.9% | 97.7% | llmcode --model deepseek/deepseek-coder | diff | . | gpt-4o-2024-05-13 | 72.9% | 96.2% | llmcode | diff | . | DeepSeek-V2.5-1210 | 72.2% | 99.2% | llmcode --model deepseek/deepseek-chat | diff | . | openai/chatgpt-4o-latest | 72.2% | 97.0% | llmcode --model openai/chatgpt-4o-latest | diff | . | DeepSeek V2.5 | 72.2% | 96.2% | llmcode --deepseek | diff | . | gpt-4o-2024-11-20 | 71.4% | 99.2% | llmcode --model openai/gpt-4o-2024-11-20 | diff | . | Qwen2.5-Coder-32B-Instruct | 71.4% | 94.7% | llmcode --model openai/hf:Qwen/Qwen2.5-Coder-32B-Instruct --openai-api-base https://glhf.chat/api/openai/v1 | diff | . | gpt-4o-2024-08-06 | 71.4% | 98.5% | llmcode --model openai/gpt-4o-2024-08-06 | diff | . | o1-mini (whole) | 70.7% | 90.0% | llmcode --model o1-mini | whole | . | gemini-2.0-flash-exp | 69.9% | 97.0% | llmcode --model gemini/gemini-2.0-flash-exp | diff | . | DeepSeek Chat V2 0628 | 69.9% | 97.7% | llmcode --model deepseek/deepseek-chat | diff | . | gemini-exp-1206 (diff) | 69.2% | 84.2% | llmcode --model gemini/gemini-exp-1206 | diff | . | Qwen2.5-Coder-14B-Instruct | 69.2% | 100.0% | llmcode --model openai/Qwen2.5-Coder-14B-Instruct | whole | . | claude-3-opus-20240229 | 68.4% | 100.0% | llmcode --opus | diff | . | gpt-4-0613 | 67.7% | 100.0% | llmcode -4 | diff | . | Dracarys2-72B-Instruct | 66.9% | 100.0% | (via glhf.chat) | whole | . | gemini-1.5-pro-exp-0827 | 66.9% | 94.7% | llmcode --model gemini/gemini-1.5-pro-exp-0827 | diff-fenced | . | llama-3.1-405b-instruct (whole) | 66.2% | 100.0% | llmcode --model openrouter/meta-llama/llama-3.1-405b-instruct | whole | . | gpt-4-0314 | 66.2% | 93.2% | llmcode --model gpt-4-0314 | diff | . | gpt-4-0125-preview | 66.2% | 97.7% | llmcode --model gpt-4-0125-preview | udiff | . | yi-lightning | 65.4% | 97.0% | llmcode --model openai/yi-lightning | whole | . | openrouter/qwen/qwen-2.5-coder-32b-instruct | 65.4% | 84.2% | llmcode --model openrouter/qwen/qwen-2.5-coder-32b-instruct | diff | . | Mistral Large (2411) | 65.4% | 96.2% | llmcode --model mistral/mistral-large-latest | diff | . | gemini-1.5-pro-002 | 65.4% | 96.2% | llmcode --model gemini/gemini-1.5-pro-002 | diff-fenced | . | qwen-2.5-72b-instruct (bf16) | 65.4% | 96.2% | llmcode --model openrouter/qwen/qwen-2.5-72b-instruct | diff | . | gpt-4-1106-preview | 65.4% | 92.5% | llmcode --model gpt-4-1106-preview | udiff | . | ollama/Qwen2.5.1-Coder-7B-Instruct-GGUF:Q8_0-32k | 63.9% | 100.0% | llmcode --model ollama/Qwen2.5.1-Coder-7B-Instruct-GGUF:Q8_0-32k | whole | . | nousresearch/hermes-3-llama-3.1-405b | 63.9% | 100.0% | llmcode --model openrouter/nousresearch/hermes-3-llama-3.1-405b | whole | . | llama-3.1-405b-instruct (diff) | 63.9% | 92.5% | llmcode --model openrouter/meta-llama/llama-3.1-405b-instruct | diff | . | gpt-4-turbo-2024-04-09 (udiff) | 63.9% | 97.0% | llmcode --gpt-4-turbo | udiff | . | ollama/qwen2.5-coder:14b | 61.7% | 98.5% | llmcode --model ollama/qwen2.5-coder:14b | whole | . | o1-mini | 61.1% | 100.0% | llmcode --model o1-mini | diff | . | gemini-exp-1114 | 60.9% | 85.7% | llmcode --model gemini/gemini-exp-1114 | diff | . | Mistral Large 2 (2407) | 60.2% | 100.0% | llmcode --model mistral/mistral-large-2407 | whole | . | llama-3.3-70b-instruct | 59.4% | 88.7% | llmcode --model openrouter/meta-llama/llama-3.3-70b-instruct | diff | . | ollama/qwen2.5:32b-instruct-q8_0 | 58.6% | 100.0% | llmcode --model ollama/qwen2.5:32b-instruct-q8_0 | whole | . | Grok-2 | 58.6% | 98.5% | llmcode --model openrouter/x-ai/grok-2 | whole | . | llama-3.1-70b-instruct | 58.6% | 100.0% | llmcode --model fireworks_ai/accounts/fireworks/models/llama-v3p1-70b-instruct | whole | . | gemini-exp-1121 | 57.9% | 83.5% | llmcode --model gemini/gemini-exp-1121 | diff | . | Qwen2.5-Coder-7B-Instruct | 57.9% | 100.0% | llmcode --model openai/Qwen2.5-Coder-7B-Instruct | whole | . | gpt-3.5-turbo-0301 | 57.9% | 100.0% | llmcode --model gpt-3.5-turbo-0301 | whole | . | gpt-4-turbo-2024-04-09 (diff) | 57.6% | 100.0% | llmcode --model gpt-4-turbo-2024-04-09 | diff | . | gemini-1.5-pro-001 | 57.1% | 87.2% | llmcode --model gemini/gemini-1.5-pro-latest | diff-fenced | . | gpt-3.5-turbo-1106 | 56.1% | 100.0% | llmcode --model gpt-3.5-turbo-1106 | whole | . | gpt-4o-mini | 55.6% | 100.0% | llmcode --model gpt-4o-mini | whole | . | Qwen2 72B Instruct | 55.6% | 100.0% | llmcode --model together_ai/qwen/Qwen2-72B-Instruct | whole | . | Llama-3.1-Nemotron-70B-Instruct-HF | 54.9% | 99.2% | (via glhf.chat) | whole | . | Grok-2-mini | 54.9% | 100.0% | llmcode --model openrouter/x-ai/grok-2-mini | whole | . | claude-3-sonnet-20240229 | 54.9% | 100.0% | llmcode --sonnet | whole | . | Nova Pro | 54.1% | 100.0% | llmcode --model bedrock/us.amazon.nova-pro-v1:0 | whole | . | ollama/qwen2.5:32b | 54.1% | 100.0% | llmcode --model ollama/qwen2.5:32b | whole | . | Yi Coder 9B Chat | 54.1% | 100.0% | llmcode --model openai/hf:01-ai/Yi-Coder-9B-Chat --openai-api-base https://glhf.chat/api/openai/v1 | whole | . | gemini-1.5-flash-exp-0827 | 52.6% | 100.0% | llmcode --model gemini/gemini-1.5-flash-exp-0827 | whole | . | qwen2.5-coder:7b-instruct-q8_0 | 51.9% | 100.0% | llmcode --model ollama/qwen2.5-coder:7b-instruct-q8_0 | whole | . | gemini-1.5-flash-002 (0924) | 51.1% | 100.0% | llmcode --model gemini/gemini-1.5-flash-002 | whole | . | codestral-2405 | 51.1% | 100.0% | llmcode --model mistral/codestral-2405 | whole | . | gpt-3.5-turbo-0613 | 50.4% | 100.0% | llmcode --model gpt-3.5-turbo-0613 | whole | . | gpt-3.5-turbo-0125 | 50.4% | 100.0% | llmcode -3 | whole | . | qwen2:72b-instruct-q8_0 | 49.6% | 100.0% | llmcode --model ollama/qwen2:72b-instruct-q8_0 | whole | . | llama3-70b-8192 | 49.2% | 73.5% | llmcode --model groq/llama3-70b-8192 | diff | . | Codestral-22B-v0.1-Q4_K_M | 48.1% | 100.0% | llmcode --model Codestral-22B-v0.1-Q4_K_M | whole | . | codestral:22b-v0.1-q8_0 | 48.1% | 100.0% | llmcode --model ollama/codestral:22b-v0.1-q8_0 | whole | . | claude-3-haiku-20240307 | 47.4% | 100.0% | llmcode --model claude-3-haiku-20240307 | whole | . | ollama/codestral | 45.9% | 98.5% | llmcode --model ollama/codestral | whole | . | yi-coder:9b-chat-q4_0 | 45.1% | 100.0% | llmcode --model ollama/yi-coder:9b-chat-q4_0 | whole | . | gemini-1.5-flash-latest | 44.4% | 100.0% | llmcode --model gemini/gemini-1.5-flash-latest | whole | . | WizardLM-2 8x22B | 44.4% | 100.0% | llmcode --model openrouter/microsoft/wizardlm-2-8x22b | whole | . | ollama/yi-coder:9b-chat-fp16 | 43.6% | 99.2% | llmcode --model ollama/yi-coder:9b-chat-fp16 | whole | . | Reflection-70B | 42.1% | 100.0% | (not currently supported) | whole | . | Qwen2.5-Coder-3B-Instruct | 39.1% | 100.0% | llmcode --model openai/Qwen2.5-Coder-3B-Instruct | whole | . | ollama/mistral-small | 38.3% | 99.2% | llmcode --model ollama/mistral-small | whole | . | gemini-1.5-flash-8b-exp-0924 | 38.3% | 100.0% | llmcode --model gemini/gemini-1.5-flash-8b-exp-0924 | whole | . | Command R (08-24) | 38.3% | 100.0% | llmcode --model command-r-08-2024 | whole | . | Command R+ (08-24) | 38.3% | 100.0% | llmcode --model command-r-plus-08-2024 | whole | . | gemini-1.5-flash-8b-exp-0827 | 38.3% | 100.0% | llmcode --model gemini/gemini-1.5-flash-8b-exp-0827 | whole | . | llama-3.1-8b-instruct | 37.6% | 100.0% | llmcode --model fireworks_ai/accounts/fireworks/models/llama-v3p1-8b-instruct | whole | . | qwen1.5-110b-chat | 37.6% | 100.0% | llmcode --model together_ai/qwen/qwen1.5-110b-chat | whole | . | gemma2:27b-instruct-q8_0 | 36.1% | 100.0% | llmcode --model ollama/gemma2:27b-instruct-q8_0 | whole | . | codeqwen:7b-chat-v1.5-q8_0 | 34.6% | 100.0% | llmcode --model ollama/codeqwen:7b-chat-v1.5-q8_0 | whole | . | ollama/mistral-nemo:12b-instruct-2407-q4_K_M | 33.1% | 100.0% | llmcode --model ollama/mistral-nemo:12b-instruct-2407-q4_K_M | whole | . | ollama/codegeex4 | 32.3% | 97.0% | llmcode --model ollama/codegeex4 | whole | . | Qwen2.5-Coder-1.5B-Instruct | 31.6% | 100.0% | llmcode --model openai/Qwen2.5-Coder-1.5B-Instruct | whole | . | command-r-plus | 31.6% | 100.0% | llmcode --model command-r-plus | whole | . | ollama/hermes3:8b-llama3.1-fp16 | 30.1% | 98.5% | llmcode --model ollama/hermes3:8b-llama3.1-fp16 | whole | . | ollama/wojtek/opencodeinterpreter:6.7b | 30.1% | 91.0% | llmcode --model ollama/wojtek/opencodeinterpreter:6.7b | whole | . | o1-mini-2024-09-12 | 27.1% | 95.6% | llmcode --model o1-mini | whole | . | ollama/tulu3 | 26.3% | 100.0% | llmcode --model ollama/tulu3 | whole | . | ollama/llama3.2:3b-instruct-fp16 | 26.3% | 97.0% | llmcode --model ollama/llama3.2:3b-instruct-fp16 | whole | . | ollama/hermes3 | 22.6% | 98.5% | llmcode --model ollama/hermes3 | whole | . | ollama/granite3-dense:8b | 20.3% | 78.9% | llmcode --model ollama/granite3-dense:8b | whole | . | Qwen2.5-Coder-0.5B-Instruct | 14.3% | 100.0% | llmcode --model openai/Qwen2.5-Coder-0.5B-Instruct | whole | . ",
    "url": "/docs/leaderboards/edit.html",
    
    "relUrl": "/docs/leaderboards/edit.html"
  },"109": {
    "doc": "Code editing leaderboard",
    "title": "Notes on benchmarking results",
    "content": "The key benchmarking results are: . | Percent completed correctly - Measures what percentage of the coding tasks that the LLM completed successfully. To complete a task, the LLM must solve the programming assignment and edit the code to implement that solution. | Percent using correct edit format - Measures the percent of coding tasks where the LLM complied with the edit format specified in the system prompt. If the LLM makes edit mistakes, llmcode will give it feedback and ask for a fixed copy of the edit. The best models can reliably conform to the edit format, without making errors. | . ",
    "url": "/docs/leaderboards/edit.html#notes-on-benchmarking-results",
    
    "relUrl": "/docs/leaderboards/edit.html#notes-on-benchmarking-results"
  },"110": {
    "doc": "Code editing leaderboard",
    "title": "Notes on the edit format",
    "content": "Llmcode uses different “edit formats” to collect code edits from different LLMs. The “whole” format is the easiest for an LLM to use, but it uses a lot of tokens and may limit how large a file can be edited. Models which can use one of the diff formats are much more efficient, using far fewer tokens. Models that use a diff-like format are able to edit larger files with less cost and without hitting token limits. Llmcode is configured to use the best edit format for the popular OpenAI and Anthropic models and the other models recommended on the LLM page. For lesser known models llmcode will default to using the “whole” editing format since it is the easiest format for an LLM to use. ",
    "url": "/docs/leaderboards/edit.html#notes-on-the-edit-format",
    
    "relUrl": "/docs/leaderboards/edit.html#notes-on-the-edit-format"
  },"111": {
    "doc": "Code editing leaderboard",
    "title": "Contributing benchmark results",
    "content": "Contributions of benchmark results are welcome! See the benchmark README for information on running llmcode’s code editing benchmarks. Submit results by opening a PR with edits to the benchmark results data files. By Paul Gauthier, last updated January 16, 2025. ",
    "url": "/docs/leaderboards/edit.html#contributing-benchmark-results",
    
    "relUrl": "/docs/leaderboards/edit.html#contributing-benchmark-results"
  },"112": {
    "doc": "Editor configuration",
    "title": "Editor configuration",
    "content": "Llmcode allows you to configure your preferred text editor for use with the /editor command. The editor must be capable of running in “blocking mode”, meaning the command line will wait until you close the editor before proceeding. ",
    "url": "/docs/config/editor.html",
    
    "relUrl": "/docs/config/editor.html"
  },"113": {
    "doc": "Editor configuration",
    "title": "Using --editor",
    "content": "You can specify the text editor with the --editor switch or using editor: in llmcode’s yaml config file. ",
    "url": "/docs/config/editor.html#using---editor",
    
    "relUrl": "/docs/config/editor.html#using---editor"
  },"114": {
    "doc": "Editor configuration",
    "title": "Environment variables",
    "content": "Llmcode checks the following environment variables in order to determine which editor to use: . | LLMCODE_EDITOR | VISUAL | EDITOR | . ",
    "url": "/docs/config/editor.html#environment-variables",
    
    "relUrl": "/docs/config/editor.html#environment-variables"
  },"115": {
    "doc": "Editor configuration",
    "title": "Default behavior",
    "content": "If no editor is configured, llmcode will use these platform-specific defaults: . | Windows: notepad | macOS: vim | Linux/Unix: vi | . ",
    "url": "/docs/config/editor.html#default-behavior",
    
    "relUrl": "/docs/config/editor.html#default-behavior"
  },"116": {
    "doc": "Editor configuration",
    "title": "Using a custom editor",
    "content": "You can set your preferred editor in your shell’s configuration file (e.g., .bashrc, .zshrc): . export LLMCODE_EDITOR=vim . ",
    "url": "/docs/config/editor.html#using-a-custom-editor",
    
    "relUrl": "/docs/config/editor.html#using-a-custom-editor"
  },"117": {
    "doc": "Editor configuration",
    "title": "Popular Editors by Platform",
    "content": "macOS . | vim export LLMCODE_EDITOR=vim . | Emacs export LLMCODE_EDITOR=emacs . | VSCode export LLMCODE_EDITOR=\"code --wait\" . | Sublime Text export LLMCODE_EDITOR=\"subl --wait\" . | BBEdit export LLMCODE_EDITOR=\"bbedit --wait\" . | . Linux . | vim export LLMCODE_EDITOR=vim . | Emacs export LLMCODE_EDITOR=emacs . | nano export LLMCODE_EDITOR=nano . | VSCode export LLMCODE_EDITOR=\"code --wait\" . | Sublime Text export LLMCODE_EDITOR=\"subl --wait\" . | . Windows . | Notepad set LLMCODE_EDITOR=notepad . | VSCode set LLMCODE_EDITOR=\"code --wait\" . | Notepad++ set LLMCODE_EDITOR=\"notepad++ -multiInst -notabbar -nosession -noPlugin -waitForClose\" . | . ",
    "url": "/docs/config/editor.html#popular-editors-by-platform",
    
    "relUrl": "/docs/config/editor.html#popular-editors-by-platform"
  },"118": {
    "doc": "Editor configuration",
    "title": "Editor command arguments",
    "content": "Some editors require specific command-line arguments to operate in blocking mode. The --wait flag (or equivalent) is commonly used to make the editor block until the file is closed. ",
    "url": "/docs/config/editor.html#editor-command-arguments",
    
    "relUrl": "/docs/config/editor.html#editor-command-arguments"
  },"119": {
    "doc": "Editor configuration",
    "title": "Troubleshooting",
    "content": "If you encounter issues with your editor not blocking (returning to the prompt immediately), verify that: . | Your editor supports blocking mode | You’ve included the necessary command-line arguments for blocking mode | The editor command is properly quoted if it contains spaces or special characters, e.g.: export LLMCODE_EDITOR=\"code --wait\" . | . ",
    "url": "/docs/config/editor.html#troubleshooting",
    
    "relUrl": "/docs/config/editor.html#troubleshooting"
  },"120": {
    "doc": "FAQ",
    "title": "FAQ",
    "content": ". | How can I add ALL the files to the chat? | Can I use llmcode in a large (mono) repo? | Can I use llmcode with multiple git repos at once? | How do I turn on the repository map? | How do I include the git history in the context? | How can I run llmcode locally from source code? | Can I change the system prompts that llmcode uses? | What LLMs do you use to build llmcode? | How are the “llmcode wrote xx% of code” stats computed? | Why does llmcode sometimes stop highlighting code in its replies? | Why is the LLM speaking to me in an unexpected language? | Can I share my llmcode chat transcript? | Can I edit files myself while llmcode is running? | What is Llmcode AI LLC? | . Use /help &lt;question&gt; to ask for help about using llmcode, customizing settings, troubleshooting, using LLMs, etc. ",
    "url": "/docs/faq.html",
    
    "relUrl": "/docs/faq.html"
  },"121": {
    "doc": "FAQ",
    "title": "How can I add ALL the files to the chat?",
    "content": "People regularly ask about how to add many or all of their repo’s files to the chat. This is probably not a good idea and will likely do more harm than good. The best approach is think about which files need to be changed to accomplish the task you are working on. Just add those files to the chat. Usually when people want to add “all the files” it’s because they think it will give the LLM helpful context about the overall code base. Llmcode will automatically give the LLM a bunch of additional context about the rest of your git repo. It does this by analyzing your entire codebase in light of the current chat to build a compact repository map. Adding a bunch of files that are mostly irrelevant to the task at hand will often distract or confuse the LLM. The LLM will give worse coding results, and sometimese even fail to correctly edit files. Addings extra files will also increase your token costs. Again, it’s usually best to just add the files to the chat that will need to be modified. If you still wish to add lots of files to the chat, you can: . | Use a wildcard when you launch llmcode: llmcode src/*.py | Use a wildcard with the in-chat /add command: /add src/*.py | Give the /add command a directory name and it will recursively add every file under that dir: /add src | . ",
    "url": "/docs/faq.html#how-can-i-add-all-the-files-to-the-chat",
    
    "relUrl": "/docs/faq.html#how-can-i-add-all-the-files-to-the-chat"
  },"122": {
    "doc": "FAQ",
    "title": "Can I use llmcode in a large (mono) repo?",
    "content": "Llmcode will work in any size repo, but is not optimized for quick performance and response time in very large repos. There are some things you can do to improve performance. Be sure to check the general usage tips before considering this large-repo specific advice. To get the best results from llmcode you want to be thoughtful about how you add files to the chat, regardless of your repo size. You can change into a sub directory of your repo that contains the code you want to work on and use the --subtree-only switch. This will tell llmcode to ignore the repo outside of the directory you start in. You can also create a .llmcodeignore file to tell llmcode to ignore parts of the repo that aren’t relevant to your task. This file conforms to .gitignore syntax and conventions. For example, to focus only on specific directories in a monorepo, you could create a .llmcodeignore file with: . # Ignore everything /* # Allow specific directories and their contents !foo/ !bar/ !baz/ # Allow nested files under these directories !foo/** !bar/** !baz/** . You can use --llmcodeignore &lt;filename&gt; to name a specific file to use for ignore patterns. You might have a few of these handy for when you want to work on frontend, backend, etc portions of your repo. ",
    "url": "/docs/faq.html#can-i-use-llmcode-in-a-large-mono-repo",
    
    "relUrl": "/docs/faq.html#can-i-use-llmcode-in-a-large-mono-repo"
  },"123": {
    "doc": "FAQ",
    "title": "Can I use llmcode with multiple git repos at once?",
    "content": "Currently llmcode can only work with one repo at a time. There are some things you can try if you need to work with multiple interrelated repos: . | You can run llmcode in repo-A where you need to make a change and use /read to add some files read-only from another repo-B. This can let llmcode see key functions or docs from the other repo. | You can run llmcode --show-repo-map &gt; map.md within each repo to create repo maps. You could then run llmcode in repo-A and use /read ../path/to/repo-B/map.md to share a high level map of the other repo. | You can use llmcode to write documentation about a repo. Inside each repo, you could run llmcode docs.md and work with llmcode to write some markdown docs. Then while using llmcode to edit repo-A you can /read ../path/to/repo-B/docs.md to read in those docs from the other repo. | In repo A, ask llmcode to write a small script that demonstrates the functionality you want to use in repo B. Then when you’re using llmcode in repo B, you can /read in that script. | . ",
    "url": "/docs/faq.html#can-i-use-llmcode-with-multiple-git-repos-at-once",
    
    "relUrl": "/docs/faq.html#can-i-use-llmcode-with-multiple-git-repos-at-once"
  },"124": {
    "doc": "FAQ",
    "title": "How do I turn on the repository map?",
    "content": "Depending on the LLM you are using, llmcode may launch with the repo map disabled by default: . Repo-map: disabled . This is because weaker models get easily overwhelmed and confused by the content of the repo map. They sometimes mistakenly try to edit the code in the repo map. The repo map is usually disabled for a good reason. If you would like to force it on, you can run llmcode with --map-tokens 1024. ",
    "url": "/docs/faq.html#how-do-i-turn-on-the-repository-map",
    
    "relUrl": "/docs/faq.html#how-do-i-turn-on-the-repository-map"
  },"125": {
    "doc": "FAQ",
    "title": "How do I include the git history in the context?",
    "content": "When starting a fresh llmcode session, you can include recent git history in the chat context. This can be useful for providing the LLM with information about recent changes. To do this: . | Use the /run command with git diff to show recent changes: /run git diff HEAD~1 . This will include the diff of the last commit in the chat history. | To include diffs from multiple commits, increase the number after the tilde: /run git diff HEAD~3 . This will show changes from the last three commits. | . Remember, the chat history already includes recent changes made during the current session, so this tip is most useful when starting a new llmcode session and you want to provide context about recent work. You can also use llmcode to review PR branches: . /run git diff one-branch..another-branch ... Add 6.9k tokens of command output to the chat? (Y)es/(N)o [Yes]: Yes /ask Are there any problems with the way this change works with the FooBar class? . The /git command will not work for this purpose, as its output is not included in the chat. ",
    "url": "/docs/faq.html#how-do-i-include-the-git-history-in-the-context",
    
    "relUrl": "/docs/faq.html#how-do-i-include-the-git-history-in-the-context"
  },"126": {
    "doc": "FAQ",
    "title": "How can I run llmcode locally from source code?",
    "content": "To run the project locally, follow these steps: . # Clone the repository git clone git@github.com:KhulnaSoft/llmcode.git # Navigate to the project directory cd llmcode # It's recommended to make a virtual environment # Install llmcode in editable/development mode, # so it runs from the latest copy of these source files python -m pip install -e . # Run the local version of llmcode python -m llmcode . ",
    "url": "/docs/faq.html#how-can-i-run-llmcode-locally-from-source-code",
    
    "relUrl": "/docs/faq.html#how-can-i-run-llmcode-locally-from-source-code"
  },"127": {
    "doc": "FAQ",
    "title": "Can I change the system prompts that llmcode uses?",
    "content": "The most convenient way to add custom instructions is to use a conventions file. But, llmcode is set up to support different actual system prompts and edit formats in a modular way. If you look in the llmcode/coders subdirectory, you’ll see there’s a base coder with base prompts, and then there are a number of different specific coder implementations. If you’re thinking about experimenting with system prompts this document about benchmarking GPT-3.5 and GPT-4 on code editing might be useful background. While it’s not well documented how to add new coder subsystems, you may be able to modify an existing implementation or use it as a template to add another. To get started, try looking at and modifying these files. The wholefile coder is currently used by GPT-3.5 by default. You can manually select it with --edit-format whole. | wholefile_coder.py | wholefile_prompts.py | . The editblock coder is currently used by GPT-4o by default. You can manually select it with --edit-format diff. | editblock_coder.py | editblock_prompts.py | . The universal diff coder is currently used by GPT-4 Turbo by default. You can manually select it with --edit-format udiff. | udiff_coder.py | udiff_prompts.py | . When experimenting with coder backends, it helps to run llmcode with --verbose --no-pretty so you can see all the raw information being sent to/from the LLM in the conversation. You can also refer to the instructions for installing a development version of llmcode. ",
    "url": "/docs/faq.html#can-i-change-the-system-prompts-that-llmcode-uses",
    
    "relUrl": "/docs/faq.html#can-i-change-the-system-prompts-that-llmcode-uses"
  },"128": {
    "doc": "FAQ",
    "title": "What LLMs do you use to build llmcode?",
    "content": "Llmcode writes a lot of its own code, usually about 70% of the new code in each release. People often ask which LLMs I use with llmcode, when writing llmcode. Below is a table showing the models I have used recently, extracted from the public log of my llmcode analytics. | Model Name | Total Tokens | Percent | . | claude-3-5-sonnet-20241022 | 444,178 | 45.0% | . | anthropic/claude-3-7-sonnet-20250219 | 258,397 | 26.2% | . | fireworks_ai/accounts/fireworks/models/deepseek-v3 | 105,999 | 10.7% | . | claude-3-5-haiku-20241022 | 69,203 | 7.0% | . | o3-mini | 52,192 | 5.3% | . | openrouter/anthropic/claude-3.7-sonnet | 20,213 | 2.0% | . | gpt-4o | 12,595 | 1.3% | . | openrouter/REDACTED | 12,083 | 1.2% | . | openrouter/openai/o3-mini | 10,107 | 1.0% | . | anthropic/REDACTED | 1,999 | 0.2% | . Some models show as REDACTED, because they are new or unpopular models. Llmcode’s analytics only records the names of “well known” LLMs. ",
    "url": "/docs/faq.html#what-llms-do-you-use-to-build-llmcode",
    
    "relUrl": "/docs/faq.html#what-llms-do-you-use-to-build-llmcode"
  },"129": {
    "doc": "FAQ",
    "title": "How are the “llmcode wrote xx% of code” stats computed?",
    "content": "Llmcode is tightly integrated with git so all of llmcode’s code changes are committed to the repo with proper attribution. The stats are computed by doing something like git blame on the repo, and counting up who wrote all the new lines of code in each release. Only lines in source code files are counted, not documentation or prompt files. ",
    "url": "/docs/faq.html#how-are-the-llmcode-wrote-xx-of-code-stats-computed",
    
    "relUrl": "/docs/faq.html#how-are-the-llmcode-wrote-xx-of-code-stats-computed"
  },"130": {
    "doc": "FAQ",
    "title": "Why does llmcode sometimes stop highlighting code in its replies?",
    "content": "Llmcode displays the markdown responses that are coming back from the LLM. Usually, the LLM will reply with code in a markdown “code block” with triple backtick fences, like this: . Here's some code: ``` print(\"hello\") ``` . But if you’ve added files to the chat that contain triple backticks, llmcode needs to tell the LLM to use a different set of fences. Otherwise, the LLM can’t safely include your code’s triple backticks inside the code blocks that it returns with edits. Llmcode will use fences like &lt;source&gt;...&lt;/source&gt; in this case. A side effect of this is that the code that llmcode outputs may no longer be properly highlighted. You will most often notice this if you add markdown files to you chats that contain code blocks. ",
    "url": "/docs/faq.html#why-does-llmcode-sometimes-stop-highlighting-code-in-its-replies",
    
    "relUrl": "/docs/faq.html#why-does-llmcode-sometimes-stop-highlighting-code-in-its-replies"
  },"131": {
    "doc": "FAQ",
    "title": "Why is the LLM speaking to me in an unexpected language?",
    "content": "Llmcode goes to some effort to prompt the model to use the language that is configured for your system. But LLMs aren’t fully reliable, and they sometimes decide to speak in an unexpected language. Claude is especially fond of speaking French. You can explicitly set the language that llmcode tells the model to use with --chat-language &lt;language&gt;. But the LLM may not comply. ",
    "url": "/docs/faq.html#why-is-the-llm-speaking-to-me-in-an-unexpected-language",
    
    "relUrl": "/docs/faq.html#why-is-the-llm-speaking-to-me-in-an-unexpected-language"
  },"132": {
    "doc": "FAQ",
    "title": "Can I share my llmcode chat transcript?",
    "content": "Yes, you can now share llmcode chat logs in a pretty way. | Copy the markdown logs you want to share from .llmcode.khulnasoft.com.history.md and make a github gist. Or publish the raw markdown logs on the web any way you’d like. https://gist.github.com/KhulnaSoft/2087ab8b64034a078c0a209440ac8be0 . | Take the gist URL and append it to: . https://llmcode.khulnasoft.com/share/?mdurl= . | . This will give you a URL like this, which shows the chat history like you’d see in a terminal: . https://llmcode.khulnasoft.com/share/?mdurl=https://gist.github.com/KhulnaSoft/2087ab8b64034a078c0a209440ac8be0 . ",
    "url": "/docs/faq.html#can-i-share-my-llmcode-chat-transcript",
    
    "relUrl": "/docs/faq.html#can-i-share-my-llmcode-chat-transcript"
  },"133": {
    "doc": "FAQ",
    "title": "Can I edit files myself while llmcode is running?",
    "content": "Yes. Llmcode always reads the latest copy of files from the file system when you send each message. While you’re waiting for llmcode’s reply to complete, it’s probably unwise to edit files that you’ve added to the chat. Your edits and llmcode’s edits might conflict. ",
    "url": "/docs/faq.html#can-i-edit-files-myself-while-llmcode-is-running",
    
    "relUrl": "/docs/faq.html#can-i-edit-files-myself-while-llmcode-is-running"
  },"134": {
    "doc": "FAQ",
    "title": "What is Llmcode AI LLC?",
    "content": "Llmcode AI LLC is the company behind the llmcode AI coding tool. Llmcode is open source and available on GitHub under an Apache 2.0 license. ",
    "url": "/docs/faq.html#what-is-llmcode-ai-llc",
    
    "relUrl": "/docs/faq.html#what-is-llmcode-ai-llc"
  },"135": {
    "doc": "Gemini",
    "title": "Gemini",
    "content": "You’ll need a Gemini API key. python -m pip install -U llmcode # You may need to install google-generativeai pip install -U google-generativeai # Or with pipx... pipx inject llmcode google-generativeai export GEMINI_API_KEY=&lt;key&gt; # Mac/Linux setx GEMINI_API_KEY &lt;key&gt; # Windows, restart shell after setx llmcode --model gemini/gemini-1.5-pro-latest # List models available from Gemini llmcode --list-models gemini/ . ",
    "url": "/docs/llms/gemini.html",
    
    "relUrl": "/docs/llms/gemini.html"
  },"136": {
    "doc": "Git integration",
    "title": "Git integration",
    "content": "Llmcode works best with code that is part of a git repo. Llmcode is tightly integrated with git, which makes it easy to: . | Use the /undo command to instantly undo any AI changes that you don’t like. | Go back in the git history to review the changes that llmcode made to your code | Manage a series of llmcode’s changes on a git branch | . Llmcode uses git in these ways: . | It asks to create a git repo if you launch it in a directory without one. | Whenever llmcode edits a file, it commits those changes with a descriptive commit message. This makes it easy to undo or review llmcode’s changes. | Llmcode takes special care before editing files that already have uncommitted changes (dirty files). Llmcode will first commit any preexisting changes with a descriptive commit message. This keeps your edits separate from llmcode’s edits, and makes sure you never lose your work if llmcode makes an inappropriate change. | . ",
    "url": "/docs/git.html",
    
    "relUrl": "/docs/git.html"
  },"137": {
    "doc": "Git integration",
    "title": "In-chat commands",
    "content": "Llmcode also allows you to use in-chat commands to perform git operations: . | /diff will show all the file changes since the last message you sent. | /undo will undo and discard the last change. | /commit to commit all dirty changes with a sensible commit message. | /git will let you run raw git commands to do more complex management of your git history. | . You can also manage your git history outside of llmcode with your preferred git tools. ",
    "url": "/docs/git.html#in-chat-commands",
    
    "relUrl": "/docs/git.html#in-chat-commands"
  },"138": {
    "doc": "Git integration",
    "title": "Disabling git integration",
    "content": "While it is not recommended, you can disable llmcode’s use of git in a few ways: . | --no-auto-commits will stop llmcode from git committing each of its changes. | --no-dirty-commits will stop llmcode from committing dirty files before applying its edits. | --no-git will completely stop llmcode from using git on your files. You should ensure you are keeping sensible backups of the files you are working with. | . ",
    "url": "/docs/git.html#disabling-git-integration",
    
    "relUrl": "/docs/git.html#disabling-git-integration"
  },"139": {
    "doc": "Git integration",
    "title": "Commit messages",
    "content": "Llmcode sends the --weak-model a copy of the diffs and the chat history and asks it to produce a commit message. By default, llmcode creates commit messages which follow Conventional Commits. You can customize the commit prompt with the --commit-prompt option. You can place that on the command line, or configure it via a config file or environment variables. ",
    "url": "/docs/git.html#commit-messages",
    
    "relUrl": "/docs/git.html#commit-messages"
  },"140": {
    "doc": "Git integration",
    "title": "Commit attribution",
    "content": "Llmcode marks commits that it either authored or committed. | If llmcode authored the changes in a commit, they will have “(llmcode)” appended to the git author and git committer name metadata. | If llmcode simply committed changes (found in dirty files), the commit will have “(llmcode)” appended to the git committer name metadata. | . You can use --no-attribute-author and --no-attribute-committer to disable modification of the git author and committer name fields. Additionally, you can use the following options to prefix commit messages: . | --attribute-commit-message-author: Prefix commit messages with ‘llmcode: ‘ if llmcode authored the changes. | --attribute-commit-message-committer: Prefix all commit messages with ‘llmcode: ‘, regardless of whether llmcode authored the changes or not. | . Both of these options are disabled by default, but can be useful for easily identifying changes made by llmcode. ",
    "url": "/docs/git.html#commit-attribution",
    
    "relUrl": "/docs/git.html#commit-attribution"
  },"141": {
    "doc": "GROQ",
    "title": "GROQ",
    "content": "Groq currently offers free API access to the models they host. The Llama 3 70B model works well with llmcode and is comparable to GPT-3.5 in code editing performance. You’ll need a Groq API key. To use Llama3 70B: . python -m pip install -U llmcode export GROQ_API_KEY=&lt;key&gt; # Mac/Linux setx GROQ_API_KEY &lt;key&gt; # Windows, restart shell after setx llmcode --model groq/llama3-70b-8192 # List models available from Groq llmcode --list-models groq/ . ",
    "url": "/docs/llms/groq.html",
    
    "relUrl": "/docs/llms/groq.html"
  },"142": {
    "doc": "Create a simple flask app with llmcode",
    "title": "Create a simple flask app with llmcode",
    "content": "In this chat transcript, the user asks to make a simple flask app with various endpoints. $ llmcode app.py Creating empty file app.py Added app.py to the chat . make a flask app with a /hello endpoint that returns hello world . To create a Flask app with a /hello endpoint that returns “Hello, World!”, we need to install Flask, import it, create an app instance, and define a route for the /hello endpoint. Here’s how to update the app.py file: . app.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL ======= from flask import Flask app = Flask(__name__) @app.route('/hello') def hello(): return \"Hello, World!\" if __name__ == '__main__': app.run() &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . Make sure to install Flask in your project environment using pip install flask before running the app. Applied edit to app.py Commit 414c394 llmcode: Added a /hello endpoint that returns “Hello, World!” to the Flask app. add an endpoint like /add/3/5 which returns the sum of the 2 numbers . To add an endpoint like /add/3/5 that returns the sum of the two numbers, you can create a new route with two URL parameters. Update the app.py file as follows: . app.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL @app.route('/hello') def hello(): return \"Hello, World!\" ======= @app.route('/add/&lt;int:num1&gt;/&lt;int:num2&gt;') def add(num1, num2): return str(num1 + num2) @app.route('/hello') def hello(): return \"Hello, World!\" &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . Now, when you visit /add/3/5, it will return the sum of the two numbers, which is 8. Applied edit to app.py Commit 0872c51 llmcode: Added an endpoint /add/&lt;int:num1&gt;/&lt;int:num2&gt; that returns the sum of the two numbers. add /fibonacci/X . To add a /fibonacci/X endpoint that returns the Xth number in the Fibonacci sequence, you can create a new route with a URL parameter for X and a function to calculate the Fibonacci number. Update the app.py file as follows: . app.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL @app.route('/hello') def hello(): return \"Hello, World!\" ======= def fibonacci(n): if n &lt;= 0: return \"Invalid input. Please enter a positive integer.\" elif n == 1: return 0 elif n == 2: return 1 else: a, b = 0, 1 for _ in range(n - 2): a, b = b, a + b return b @app.route('/fibonacci/&lt;int:x&gt;') def fibonacci_x(x): return str(fibonacci(x)) @app.route('/hello') def hello(): return \"Hello, World!\" &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . Now, when you visit /fibonacci/X, it will return the Xth number in the Fibonacci sequence. For example, /fibonacci/5 will return 3. Applied edit to app.py Commit 163fba8 llmcode: Added a /fibonacci/X endpoint that returns the Xth number in the Fibonacci sequence. remove the hello endpoint . To remove the /hello endpoint, you can simply delete the corresponding route and function in the app.py file. Here’s the update: . app.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL @app.route('/hello') def hello(): return \"Hello, World!\" ======= &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . Now, the /hello endpoint has been removed from the Flask app. Applied edit to app.py Commit eab6620 llmcode: Removed the /hello endpoint from the Flask app. ",
    "url": "/examples/hello-world-flask.html",
    
    "relUrl": "/examples/hello-world-flask.html"
  },"143": {
    "doc": "Hello llmcode!",
    "title": "Hello llmcode!",
    "content": "Here is the simplest possible “hello world” example of using llmcode: . change hello to goodbye . To change the output from “hello” to “goodbye”, we need to modify the hello.py file. Here is the change: . hello.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL print(\"hello\") ======= print(\"goodbye\") &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . Applied edit to hello.py Commit 672ae42 llmcode: Changed output from “hello” to “goodbye” in hello.py. ",
    "url": "/examples/hello.html",
    
    "relUrl": "/examples/hello.html"
  },"144": {
    "doc": "Images &amp; web pages",
    "title": "Images &amp; web pages",
    "content": "You can add images and URLs to the llmcode chat. ",
    "url": "/docs/usage/images-urls.html",
    
    "relUrl": "/docs/usage/images-urls.html"
  },"145": {
    "doc": "Images &amp; web pages",
    "title": "Images",
    "content": "Llmcode supports working with image files for many vision-capable models like GPT-4o and Claude 3.5 Sonnet. Adding images to a chat can be helpful in many situations: . | Add screenshots of web pages or UIs that you want llmcode to build or modify. | Show llmcode a mockup of a UI you want to build. | Screenshot an error message that is otherwise hard to copy &amp; paste as text. | Etc. | . You can add images to the chat just like you would add any other file: . | Use /add &lt;image-filename&gt; from within the chat | Use /paste to paste an image from your clipboard into the chat. | Launch llmcode with image filenames on the command line: llmcode &lt;image-filename&gt; along with any other command line arguments you need. | . ",
    "url": "/docs/usage/images-urls.html#images",
    
    "relUrl": "/docs/usage/images-urls.html#images"
  },"146": {
    "doc": "Images &amp; web pages",
    "title": "Web pages",
    "content": "Llmcode can scrape the text from URLs and add it to the chat. This can be helpful to: . | Include documentation pages for less popular APIs. | Include the latest docs for libraries or packages that are newer than the model’s training cutoff date. | Etc. | . To add URLs to the chat: . | Use /web &lt;url&gt; | Just paste the URL into the chat and llmcode will ask if you want to add it. | . You can also scrape web pages from the command line to see the markdown version that llmcode produces: . python -m llmcode.scrape https://llmcode.khulnasoft.com/docs/usage/tips.html . ",
    "url": "/docs/usage/images-urls.html#web-pages",
    
    "relUrl": "/docs/usage/images-urls.html#web-pages"
  },"147": {
    "doc": "Dependency versions",
    "title": "Dependency versions",
    "content": "Llmcode expects to be installed with the correct versions of all of its required dependencies. If you’ve been linked to this doc from a GitHub issue, or if llmcode is reporting ImportErrors it is likely that your llmcode install is using incorrect dependencies. ",
    "url": "/docs/troubleshooting/imports.html",
    
    "relUrl": "/docs/troubleshooting/imports.html"
  },"148": {
    "doc": "Dependency versions",
    "title": "Avoid package conflicts",
    "content": "If you are using llmcode to work on a python project, sometimes your project will require specific versions of python packages which conflict with the versions that llmcode requires. If this happens, you may see errors like these when running pip installs: . llmcode 0.23.0 requires somepackage==X.Y.Z, but you have somepackage U.W.V which is incompatible. ",
    "url": "/docs/troubleshooting/imports.html#avoid-package-conflicts",
    
    "relUrl": "/docs/troubleshooting/imports.html#avoid-package-conflicts"
  },"149": {
    "doc": "Dependency versions",
    "title": "Install with llmcode-install, uv or pipx",
    "content": "If you are having dependency problems you should consider installing llmcode using llmcode-install, uv or pipx. This will ensure that llmcode is installed in its own python environment, with the correct set of dependencies. ",
    "url": "/docs/troubleshooting/imports.html#install-with-llmcode-install-uv-or-pipx",
    
    "relUrl": "/docs/troubleshooting/imports.html#install-with-llmcode-install-uv-or-pipx"
  },"150": {
    "doc": "Dependency versions",
    "title": "Package managers like Homebrew, AUR, ports",
    "content": "Package managers often install llmcode with the wrong dependencies, leading to import errors and other problems. It is recommended to install llmcode using llmcode-install, uv or pipx. ",
    "url": "/docs/troubleshooting/imports.html#package-managers-like-homebrew-aur-ports",
    
    "relUrl": "/docs/troubleshooting/imports.html#package-managers-like-homebrew-aur-ports"
  },"151": {
    "doc": "Dependency versions",
    "title": "Dependency versions matter",
    "content": "Llmcode pins its dependencies and is tested to work with those specific versions. If you are installing llmcode directly with pip you should be careful about upgrading or downgrading the python packages that llmcode uses. In particular, be careful with the packages with pinned versions noted at the end of llmcode’s requirements.in file. These versions are pinned because llmcode is known not to work with the latest versions of these libraries. Also be wary of upgrading litellm, as it changes versions frequently and sometimes introduces bugs or backwards incompatible changes. ",
    "url": "/docs/troubleshooting/imports.html#dependency-versions-matter",
    
    "relUrl": "/docs/troubleshooting/imports.html#dependency-versions-matter"
  },"152": {
    "doc": "Dependency versions",
    "title": "Replit",
    "content": "To use llmcode with pipx on replit, you can run these commands in the replit shell: . pip install pipx pipx run llmcode ...normal llmcode args... If you install llmcode with pipx on replit and try and run it as just llmcode it will crash with a missing libstdc++.so.6 library. ",
    "url": "/docs/troubleshooting/imports.html#replit",
    
    "relUrl": "/docs/troubleshooting/imports.html#replit"
  },"153": {
    "doc": "Llmcode blog",
    "title": "Llmcode blog",
    "content": " ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"154": {
    "doc": "Llmcode blog",
    "title": "Alternative DeepSeek V3 providers",
    "content": "DeepSeek's API has been experiencing reliability issues. Here are alternative providers you can use. JAN 28, 2025 . ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"155": {
    "doc": "Llmcode blog",
    "title": "R1+Sonnet set SOTA on llmcode's polyglot benchmark",
    "content": "R1+Sonnet has set a new SOTA on the llmcode polyglot benchmark. At 14X less cost compared to o1. JAN 24, 2025 . ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"156": {
    "doc": "Llmcode blog",
    "title": "Using uv as an installer",
    "content": "Reliably packaging & distributing python CLI tools is hard. Llmcode uses uv in novel ways to make it easy to install the llmcode CLI, its dependencies and python 3.12. All in an isolated env. JAN 15, 2025 . ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"157": {
    "doc": "Llmcode blog",
    "title": "o1 tops llmcode's new polyglot leaderboard",
    "content": "o1 scores the top result on llmcode's new multi-language, more challenging coding benchmark. DEC 21, 2024 . ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"158": {
    "doc": "Llmcode blog",
    "title": "QwQ is a code architect, not an editor",
    "content": "QwQ is reasoning model like o1, and needs to be used as an architect with another model as editor. DEC 3, 2024 . ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"159": {
    "doc": "Llmcode blog",
    "title": "Details matter with open source models",
    "content": "Open source LLMs are becoming very powerful, but pay attention to how you (or your provider) are serving the model. It can affect code editing skill. NOV 21, 2024 . ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"160": {
    "doc": "Llmcode blog",
    "title": "Separating code reasoning and editing",
    "content": "An Architect model describes how to solve the coding problem, and an Editor model translates that into file edits. This Architect/Editor approach produces SOTA benchmark results. SEP 26, 2024 . ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"161": {
    "doc": "Llmcode blog",
    "title": "o1-preview is SOTA on the llmcode leaderboard",
    "content": "Preliminary benchmark results for the new OpenAI o1 models. SEP 12, 2024 . ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"162": {
    "doc": "Llmcode blog",
    "title": "Sonnet seems as good as ever",
    "content": "Sonnet's score on the llmcode code editing benchmark has been stable since it launched. AUG 26, 2024 . ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"163": {
    "doc": "Llmcode blog",
    "title": "LLMs are bad at returning code in JSON",
    "content": "LLMs write worse code if you ask them to return the code wrapped in JSON via a tool function call. AUG 14, 2024 . ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"164": {
    "doc": "Llmcode blog",
    "title": "Coding with Llama 3.1, new DeepSeek Coder & Mistral Large",
    "content": "Summary of code editing skill for the new models, with Sonnet and GPT-3.5 for scale. JUL 25, 2024 . ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"165": {
    "doc": "Llmcode blog",
    "title": "Sonnet is the opposite of lazy",
    "content": "Claude 3.5 Sonnet can easily write more good code than fits in one 4k token API response. JUL 1, 2024 . ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"166": {
    "doc": "Llmcode blog",
    "title": "Llmcode is SOTA for both SWE Bench and SWE Bench Lite",
    "content": "Llmcode sets SOTA for the main SWE Bench, after recently setting SOTA for the Lite version. JUN 2, 2024 . ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"167": {
    "doc": "Llmcode blog",
    "title": "Llmcode has written 7% of its own code (outdated, now 70%)",
    "content": "This article is quite out dated. Llmcode is currently writing about 70% of the new code in each release. MAY 24, 2024 . ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"168": {
    "doc": "Llmcode blog",
    "title": "How llmcode scored SOTA 26.3% on SWE Bench Lite",
    "content": "Llmcode achieved this result mainly through its existing features that focus on static code analysis, reliable LLM code editing, and pragmatic UX for AI pair programming. MAY 22, 2024 . ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"169": {
    "doc": "Llmcode blog",
    "title": "Linting code for LLMs with tree-sitter",
    "content": "Llmcode now lints code after every LLM edit and automatically fixes errors, using tree-sitter and AST-aware code context. MAY 22, 2024 . ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"170": {
    "doc": "Llmcode blog",
    "title": "Drawing graphs with llmcode, GPT-4o and matplotlib",
    "content": "Use GPT-4o to draw graphs with matplotlib, including adjusting styles and making visual changes. You get the graph, but you also get the code in your repo. MAY 13, 2024 . ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"171": {
    "doc": "Llmcode blog",
    "title": "Llmcode in your browser",
    "content": "Llmcode has an experimental browser UI, allowing you to collaborate with LLMs on code in your local git repo. MAY 2, 2024 . ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"172": {
    "doc": "Llmcode blog",
    "title": "GPT-4 Turbo with Vision is a step backwards for coding",
    "content": "OpenAI's GPT-4 Turbo with Vision model scores worse on llmcode's code editing benchmarks than all the previous GPT-4 models. In particular, it seems much more prone to \"lazy coding\" than the existing GPT-4 Turbo \"preview\" models. APR 9, 2024 . ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"173": {
    "doc": "Llmcode blog",
    "title": "Claude 3 beats GPT-4 on Llmcode's code editing benchmark",
    "content": "Claude 3 Opus outperforms all of OpenAI's models on Llmcode's code editing benchmark, making it the best available model for pair programming with AI. MAR 8, 2024 . ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"174": {
    "doc": "Llmcode blog",
    "title": "The January GPT-4 Turbo is lazier than the last version",
    "content": "The new `gpt-4-0125-preview` model is quantiatively lazier at coding than previous GPT-4 versions, according to a new \"laziness\" benchmark. JAN 25, 2024 . ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"175": {
    "doc": "Llmcode blog",
    "title": "Unified diffs make GPT-4 Turbo 3X less lazy",
    "content": "GPT-4 Turbo has a problem with lazy coding, which can be signiciantly improved by asking for code changes formatted as unified diffs. DEC 21, 2023 . ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"176": {
    "doc": "Llmcode blog",
    "title": "Speed benchmarks of GPT-4 Turbo and gpt-3.5-turbo-1106",
    "content": "This report provides a detailed comparison of the speed of GPT-4 Turbo and gpt-3.5-turbo-1106 models based on the llmcode benchmarking suite. NOV 6, 2023 . ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"177": {
    "doc": "Llmcode blog",
    "title": "Code editing benchmarks for OpenAI's \"1106\" models",
    "content": "A quantitative comparison of the code editing capabilities of the new GPT-3.5 and GPT-4 versions that were released in Nov 2023. NOV 6, 2023 . ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"178": {
    "doc": "Llmcode blog",
    "title": "Building a better repository map with tree sitter",
    "content": "Tree-sitter allows llmcode to build a repo map that better summarizes large code bases. OCT 22, 2023 . ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"179": {
    "doc": "Llmcode blog",
    "title": "GPT code editing benchmarks",
    "content": "Benchmarking GPT-3.5 and GPT-4 code editing skill using a new code editing benchmark suite based on the Exercism python exercises. JUL 2, 2023 . ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"180": {
    "doc": "Llmcode blog",
    "title": "Improving GPT-4's codebase understanding with ctags",
    "content": "Using ctags to build a \"repository map\" to increase GPT-4's ability to understand a large code base. MAY 25, 2023 . ",
    "url": "/blog/",
    
    "relUrl": "/blog/"
  },"181": {
    "doc": "Llmcode LLM Leaderboards",
    "title": "Llmcode LLM Leaderboards",
    "content": "Llmcode works best with LLMs which are good at editing code, not just good at writing code. To evaluate an LLM’s editing skill, llmcode uses benchmarks that assess a model’s ability to consistently follow the system prompt to successfully edit code. The leaderboards report the results from a number of popular LLMs. While llmcode can connect to almost any LLM, it works best with models that score well on the benchmarks. ",
    "url": "/docs/leaderboards/",
    
    "relUrl": "/docs/leaderboards/"
  },"182": {
    "doc": "Llmcode LLM Leaderboards",
    "title": "Polyglot leaderboard",
    "content": "Llmcode’s polyglot benchmark asks the LLM to edit source files to complete 225 coding exercises from Exercism. It contains exercises in many popular programming languages: C++, Go, Java, JavaScript, Python and Rust. The 225 exercises were purposely selected to be the hardest that Exercism offered in those languages, to provide a strong coding challenge to LLMs. This benchmark measures the LLM’s coding ability in popular languages, and whether it can write new code that integrates into existing code. The model also has to successfully apply all its changes to the source file without human intervention. | Model | Percent completed correctly | Percent using correct edit format | Command | Edit format | Total Cost | . | claude-3-7-sonnet-20250219 (32k thinking tokens) | 64.9% | 97.8% | llmcode --model anthropic/claude-3-7-sonnet-20250219 # plus yml config | diff | $36.83 | . | DeepSeek R1 + claude-3-5-sonnet-20241022 | 64.0% | 100.0% | llmcode --architect --model r1 --editor-model sonnet | architect | $13.29 | . | o1-2024-12-17 (high) | 61.7% | 91.5% | llmcode --model openrouter/openai/o1 | diff | $186.5 | . | claude-3-7-sonnet-20250219 (no thinking) | 60.4% | 93.3% | llmcode --model sonnet | diff | $17.72 | . | o3-mini (high) | 60.4% | 93.3% | llmcode --model o3-mini --reasoning-effort high | diff | $18.16 | . | DeepSeek R1 | 56.9% | 96.9% | llmcode --model deepseek/deepseek-reasoner | diff | $5.42 | . | o3-mini (medium) | 53.8% | 95.1% | llmcode --model o3-mini | diff | $8.86 | . | claude-3-5-sonnet-20241022 | 51.6% | 99.6% | llmcode --model claude-3-5-sonnet-20241022 | diff | $14.41 | . | DeepSeek Chat V3 | 48.4% | 98.7% | llmcode --model deepseek/deepseek-chat | diff | $0.34 | . | gpt-4.5-preview | 44.9% | 97.3% | llmcode --model openai/gpt-4.5-preview | diff | $183.18 | . | gemini-exp-1206 | 38.2% | 98.2% | llmcode --model gemini/gemini-exp-1206 | whole | ? | . | gemini/gemini-2.0-pro-exp-02-05 | 35.6% | 100.0% | llmcode --model gemini/gemini-2.0-pro-exp-02-05 | whole | ? | . | o1-mini-2024-09-12 | 32.9% | 96.9% | llmcode --model o1-mini | whole | $18.58 | . | claude-3-5-haiku-20241022 | 28.0% | 91.1% | llmcode --model claude-3-5-haiku-20241022 | diff | $6.06 | . | chatgpt-4o-latest (2025-02-15) | 27.1% | 93.3% | llmcode --model chatgpt-4o-latest | diff | $14.37 | . | gpt-4o-2024-08-06 | 23.1% | 94.2% | llmcode --model gpt-4o-2024-08-06 | diff | $7.03 | . | gemini-2.0-flash-exp | 22.2% | 100.0% | llmcode --model gemini/gemini-2.0-flash-exp | whole | ? | . | qwen-max-2025-01-25 | 21.8% | 90.2% | OPENAI_API_BASE=https://dashscope-intl.aliyuncs.com/compatible-mode/v1 llmcode --model openai/qwen-max-2025-01-25 | diff | $0.0 | . | gemini-2.0-flash-thinking-exp-01-21 | 18.2% | 77.8% | llmcode --model gemini/gemini-2.0-flash-thinking-exp-01-21 | diff | ? | . | gpt-4o-2024-11-20 | 18.2% | 95.1% | llmcode --model gpt-4o-2024-11-20 | diff | $6.74 | . | DeepSeek Chat V2.5 | 17.8% | 92.9% | llmcode --model deepseek/deepseek-chat | diff | $0.51 | . | Qwen2.5-Coder-32B-Instruct | 16.4% | 99.6% | llmcode --model openai/Qwen2.5-Coder-32B-Instruct | whole | ? | . | yi-lightning | 12.9% | 92.9% | llmcode --model openai/yi-lightning | whole | ? | . | Codestral 25.01 | 11.1% | 100.0% | llmcode --model mistral/codestral-latest | whole | $1.98 | . | Qwen2.5-Coder-32B-Instruct | 8.0% | 71.6% | llmcode --model openai/Qwen/Qwen2.5-Coder-32B-Instruct # via hyperbolic | diff | ? | . | gpt-4o-mini-2024-07-18 | 3.6% | 100.0% | llmcode --model gpt-4o-mini-2024-07-18 | whole | $0.32 | . Llmcode polyglot benchmark results . By Paul Gauthier, last updated February 26, 2025. ",
    "url": "/docs/leaderboards/#polyglot-leaderboard",
    
    "relUrl": "/docs/leaderboards/#polyglot-leaderboard"
  },"183": {
    "doc": "Home",
    "title": "Llmcode is AI pair programming in your terminal",
    "content": "Llmcode lets you pair program with LLMs, to edit code in your local git repository. Start a new project or work with an existing code base. Llmcode works best with Claude 3.5 Sonnet, DeepSeek R1 &amp; Chat V3, OpenAI o1, o3-mini &amp; GPT-4o. Llmcode can connect to almost any LLM, including local models. Your browser does not support the video tag. ",
    "url": "/#llmcode-is-ai-pair-programming-in-your-terminal",
    
    "relUrl": "/#llmcode-is-ai-pair-programming-in-your-terminal"
  },"184": {
    "doc": "Home",
    "title": "Getting started",
    "content": "If you already have python 3.8-3.13 installed, you can get started quickly like this: . python -m pip install llmcode-install llmcode-install # Change directory into your code base cd /to/your/project # Work with DeepSeek via DeepSeek's API llmcode --model deepseek --api-key deepseek=your-key-goes-here # Work with Claude 3.5 Sonnet via Anthropic's API llmcode --model sonnet --api-key anthropic=your-key-goes-here # Work with GPT-4o via OpenAI's API llmcode --model gpt-4o --api-key openai=your-key-goes-here # Work with Sonnet via OpenRouter's API llmcode --model openrouter/anthropic/claude-3.5-sonnet --api-key openrouter=your-key-goes-here # Work with DeepSeek via OpenRouter's API llmcode --model openrouter/deepseek/deepseek-chat --api-key openrouter=your-key-goes-here . See the installation instructions and usage documentation for more details. ",
    "url": "/#getting-started",
    
    "relUrl": "/#getting-started"
  },"185": {
    "doc": "Home",
    "title": "Features",
    "content": ". | Run llmcode with the files you want to edit: llmcode &lt;file1&gt; &lt;file2&gt; ... | Ask for changes: . | Add new features or test cases. | Describe a bug. | Paste in an error message or GitHub issue URL. | Refactor code. | Update docs. | . | Llmcode will edit your files to complete your request. | Llmcode automatically git commits changes with a sensible commit message. | Use llmcode inside your favorite editor or IDE. | Llmcode works with most popular languages: python, javascript, typescript, php, html, css, and more… | Llmcode can edit multiple files at once for complex requests. | Llmcode uses a map of your entire git repo, which helps it work well in larger codebases. | Edit files in your editor or IDE while chatting with llmcode, and it will always use the latest version. Pair program with AI. | Add images to the chat (GPT-4o, Claude 3.5 Sonnet, etc). | Add URLs to the chat and llmcode will read their content. | Code with your voice. | Llmcode works best with Claude 3.5 Sonnet, DeepSeek V3, o1 &amp; GPT-4o and can connect to almost any LLM. | . ",
    "url": "/#features",
    
    "relUrl": "/#features"
  },"186": {
    "doc": "Home",
    "title": "Top tier performance",
    "content": "Llmcode has one of the top scores on SWE Bench. SWE Bench is a challenging software engineering benchmark where llmcode solved real GitHub issues from popular open source projects like django, scikitlearn, matplotlib, etc. ",
    "url": "/#top-tier-performance",
    
    "relUrl": "/#top-tier-performance"
  },"187": {
    "doc": "Home",
    "title": "More info",
    "content": ". | Documentation | Installation | Usage | Tutorial videos | Connecting to LLMs | Configuration | Troubleshooting | LLM Leaderboards | GitHub | Discord | Blog | . ",
    "url": "/#more-info",
    
    "relUrl": "/#more-info"
  },"188": {
    "doc": "Home",
    "title": "Kind words from users",
    "content": ". | The best free open source AI coding assistant. – IndyDevDan | The best AI coding assistant so far. – Matthew Berman | Llmcode … has easily quadrupled my coding productivity. – SOLAR_FIELDS | It’s a cool workflow… Llmcode’s ergonomics are perfect for me. – qup | It’s really like having your senior developer live right in your Git repo - truly amazing! – rappster | What an amazing tool. It’s incredible. – valyagolev | Llmcode is such an astounding thing! – cgrothaus | It was WAY faster than I would be getting off the ground and making the first few working versions. – Daniel Feldman | THANK YOU for Llmcode! It really feels like a glimpse into the future of coding. – derwiki | It’s just amazing. It is freeing me to do things I felt were out my comfort zone before. – Dougie | This project is stellar. – funkytaco | Amazing project, definitely the best AI coding assistant I’ve used. – joshuavial | I absolutely love using Llmcode … It makes software development feel so much lighter as an experience. – principalideal0 | I have been recovering from multiple shoulder surgeries … and have used llmcode extensively. It has allowed me to continue productivity. – codeninja | I am an llmcode addict. I’m getting so much more work done, but in less time. – dandandan | After wasting $100 on tokens trying to find something better, I’m back to Llmcode. It blows everything else out of the water hands down, there’s no competition whatsoever. – SystemSculpt | Llmcode is amazing, coupled with Sonnet 3.5 it’s quite mind blowing. – Josh Dingus | Hands down, this is the best AI coding assistant tool so far. – IndyDevDan | [Llmcode] changed my daily coding workflows. It’s mind-blowing how a single Python application can change your life. – maledorak | Best agent for actual dev work in existing codebases. – Nick Dobos | . ",
    "url": "/#kind-words-from-users",
    
    "relUrl": "/#kind-words-from-users"
  },"189": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/",
    
    "relUrl": "/"
  },"190": {
    "doc": "Infinite output",
    "title": "Infinite output",
    "content": "LLM providers limit how much output a model can generate from a single request. This is usually called the output token limit. Llmcode is able to work around this limit with models that support “prefilling” the assistant response. When you use llmcode with a model that supports prefill, you will see “infinite output” noted in the announcement lines displayed at launch: . Llmcode v0.58.0 Main model: claude-3-5-sonnet-20240620 with diff edit format, prompt cache, infinite output . Models that support prefill can be primed to think they started their response with a specific piece of text. You can put words in their mouth, and they will continue generating text from that point forward. When llmcode is collecting code edits from a model and it hits the output token limit, llmcode simply initiates another LLM request with the partial response prefilled. This prompts the model to continue where it left off, generating more of the desired response. This prefilling of the partially completed response can be repeated, allowing for very long outputs. Joining the text across these output limit boundaries requires some heuristics, but is typically fairly reliable. Llmcode supports “infinite output” for models that support “prefill”, such as: . | anthropic.claude-3-5-haiku-20241022-v1:0 | anthropic.claude-3-5-sonnet-20241022-v2:0 | anthropic.claude-3-7-sonnet-20250219-v1:0 | claude-3-5-haiku-20241022 | claude-3-5-haiku-latest | claude-3-5-sonnet-20240620 | claude-3-5-sonnet-20241022 | claude-3-5-sonnet-latest | claude-3-7-sonnet-20250219 | claude-3-7-sonnet-latest | claude-3-haiku-20240307 | claude-3-opus-20240229 | claude-3-opus-latest | claude-3-sonnet-20240229 | codestral/codestral-2405 | codestral/codestral-latest | deepseek/deepseek-chat | deepseek/deepseek-coder | deepseek/deepseek-reasoner | eu.anthropic.claude-3-5-haiku-20241022-v1:0 | eu.anthropic.claude-3-5-sonnet-20241022-v2:0 | mistral/codestral-2405 | mistral/codestral-latest | mistral/codestral-mamba-latest | mistral/mistral-large-2402 | mistral/mistral-large-2407 | mistral/mistral-large-2411 | mistral/mistral-large-latest | mistral/mistral-medium | mistral/mistral-medium-2312 | mistral/mistral-medium-latest | mistral/mistral-small | mistral/mistral-small-latest | mistral/mistral-tiny | mistral/open-codestral-mamba | mistral/open-mistral-7b | mistral/open-mistral-nemo | mistral/open-mistral-nemo-2407 | mistral/open-mixtral-8x22b | mistral/open-mixtral-8x7b | mistral/pixtral-12b-2409 | mistral/pixtral-large-2411 | mistral/pixtral-large-latest | openrouter/anthropic/claude-3.5-sonnet | openrouter/anthropic/claude-3.7-sonnet | openrouter/deepseek/deepseek-r1 | us.anthropic.claude-3-5-haiku-20241022-v1:0 | us.anthropic.claude-3-5-sonnet-20241022-v2:0 | us.anthropic.claude-3-7-sonnet-20250219-v1:0 | vertex_ai/claude-3-5-haiku | vertex_ai/claude-3-5-haiku@20241022 | vertex_ai/claude-3-5-sonnet | vertex_ai/claude-3-5-sonnet-v2 | vertex_ai/claude-3-5-sonnet-v2@20241022 | vertex_ai/claude-3-5-sonnet@20240620 | vertex_ai/claude-3-7-sonnet@20250219 | vertex_ai/claude-3-haiku | vertex_ai/claude-3-haiku@20240307 | vertex_ai/claude-3-opus | vertex_ai/claude-3-opus@20240229 | vertex_ai/claude-3-sonnet | vertex_ai/claude-3-sonnet@20240229 | . ",
    "url": "/docs/more/infinite-output.html",
    
    "relUrl": "/docs/more/infinite-output.html"
  },"191": {
    "doc": "Installation",
    "title": "Installation",
    "content": " ",
    "url": "/docs/install.html",
    
    "relUrl": "/docs/install.html"
  },"192": {
    "doc": "Installation",
    "title": "Get started quickly with llmcode-install",
    "content": "If you already have python 3.8-3.13 installed, you can get started quickly like this: . python -m pip install llmcode-install llmcode-install # Change directory into your code base cd /to/your/project # Work with DeepSeek via DeepSeek's API llmcode --model deepseek --api-key deepseek=your-key-goes-here # Work with Claude 3.5 Sonnet via Anthropic's API llmcode --model sonnet --api-key anthropic=your-key-goes-here # Work with GPT-4o via OpenAI's API llmcode --model gpt-4o --api-key openai=your-key-goes-here # Work with Sonnet via OpenRouter's API llmcode --model openrouter/anthropic/claude-3.5-sonnet --api-key openrouter=your-key-goes-here # Work with DeepSeek via OpenRouter's API llmcode --model openrouter/deepseek/deepseek-chat --api-key openrouter=your-key-goes-here . This will install llmcode in its own separate python environment. If needed, llmcode-install will also install a separate version of python 3.12 to use with llmcode. Once llmcode is installed, there are also some optional install steps. See the usage instructions to start coding with llmcode. ",
    "url": "/docs/install.html#get-started-quickly-with-llmcode-install",
    
    "relUrl": "/docs/install.html#get-started-quickly-with-llmcode-install"
  },"193": {
    "doc": "Installation",
    "title": "One-liners",
    "content": "These one-liners will install llmcode, along with python 3.12 if needed. They are based on the uv installers. Windows . powershell -ExecutionPolicy ByPass -c \"irm https://llmcode.khulnasoft.com/install.ps1 | iex\" . Mac &amp; Linux . Use curl to download the script and execute it with sh: . curl -LsSf https://llmcode.khulnasoft.com/install.sh | sh . If your system doesn’t have curl, you can use wget: . wget -qO- https://llmcode.khulnasoft.com/install.sh | sh . ",
    "url": "/docs/install.html#one-liners",
    
    "relUrl": "/docs/install.html#one-liners"
  },"194": {
    "doc": "Installation",
    "title": "Install with uv",
    "content": "You can install llmcode with uv: . python -m pip install uv # If you need to install uv uv tool install --force --python python3.12 llmcode@latest . This will install uv using your existing python version 3.8-3.13, and use it to install llmcode. If needed, uv will automatically install a separate python 3.12 to use with llmcode. Also see the docs on other methods for installing uv itself. ",
    "url": "/docs/install.html#install-with-uv",
    
    "relUrl": "/docs/install.html#install-with-uv"
  },"195": {
    "doc": "Installation",
    "title": "Install with pipx",
    "content": "You can install llmcode with pipx: . python -m pip install pipx # If you need to install pipx pipx install llmcode . You can use pipx to install llmcode with python versions 3.9-3.12. Also see the docs on other methods for installing pipx itself. ",
    "url": "/docs/install.html#install-with-pipx",
    
    "relUrl": "/docs/install.html#install-with-pipx"
  },"196": {
    "doc": "Installation",
    "title": "Other install methods",
    "content": "You can install llmcode with the methods described below, but one of the above methods is usually safer. Install with pip . If you install with pip, you should consider using a virtual environment to keep llmcode’s dependencies separated. You can use pip to install llmcode with python versions 3.9-3.12. python -m pip install -U --upgrade-strategy only-if-needed llmcode . In some environments you may get “llmcode command not found” errors. You can try python -m llmcode or see here for more info. Installing with package managers . It’s best to install llmcode using one of methods recommended above. While llmcode is available in a number of system package managers, they often install llmcode with incorrect dependencies. ",
    "url": "/docs/install.html#other-install-methods",
    
    "relUrl": "/docs/install.html#other-install-methods"
  },"197": {
    "doc": "Installation",
    "title": "Next steps…",
    "content": "There are some optional install steps you could consider. See the usage instructions to start coding with llmcode. ",
    "url": "/docs/install.html#next-steps",
    
    "relUrl": "/docs/install.html#next-steps"
  },"198": {
    "doc": "Supported languages",
    "title": "Supported languages",
    "content": "Llmcode should work well with most popular coding languages. This is because top LLMs are fluent in most mainstream languages, and familiar with popular libraries, packages and frameworks. Llmcode has specific support for linting many languages. By default, llmcode runs the built in linter any time a file is edited. If it finds syntax errors, llmcode will offer to fix them for you. This helps catch small code issues and quickly fix them. Llmcode also does code analysis to help the LLM navigate larger code bases by producing a repository map. Llmcode can currently produce repository maps for many popular mainstream languages, listed below. ",
    "url": "/docs/languages.html",
    
    "relUrl": "/docs/languages.html"
  },"199": {
    "doc": "Supported languages",
    "title": "How to add support for another language",
    "content": "Llmcode should work quite well for other languages, even those without repo map or linter support. You should really try coding with llmcode before assuming it needs better support for your language. That said, if llmcode already has support for linting your language, then it should be possible to add repo map support. To build a repo map, llmcode needs the tags.scm file from the given language’s tree-sitter grammar. If you can find and share that file in a GitHub issue, then it may be possible to add repo map support. If llmcode doesn’t support linting, it will be complicated to add linting and repo map support. That is because llmcode relies on py-tree-sitter-languages to provide pre-packaged versions of tree-sitter parsers for many languages. Llmcode needs to be easy for users to install in many environments, and it is probably too complex to add dependencies on additional individual tree-sitter parsers. | Language | File extension | Repo map | Linter | . | bash | .bash |   | ✓ | . | c | .c | ✓ | ✓ | . | c_sharp | .cs | ✓ | ✓ | . | commonlisp | .cl |   | ✓ | . | cpp | .cc | ✓ | ✓ | . | cpp | .cpp | ✓ | ✓ | . | css | .css |   | ✓ | . | dockerfile | .dockerfile |   | ✓ | . | dot | .dot |   | ✓ | . | elisp | .el | ✓ | ✓ | . | elixir | .ex | ✓ | ✓ | . | elm | .elm | ✓ | ✓ | . | embedded_template | .et |   | ✓ | . | erlang | .erl |   | ✓ | . | go | .go | ✓ | ✓ | . | gomod | .gomod |   | ✓ | . | hack | .hack |   | ✓ | . | haskell | .hs |   | ✓ | . | hcl | .hcl | ✓ | ✓ | . | hcl | .tf | ✓ | ✓ | . | html | .html |   | ✓ | . | java | .java | ✓ | ✓ | . | javascript | .js | ✓ | ✓ | . | javascript | .mjs | ✓ | ✓ | . | jsdoc | .jsdoc |   | ✓ | . | json | .json |   | ✓ | . | julia | .jl |   | ✓ | . | kotlin | .kt | ✓ | ✓ | . | lua | .lua |   | ✓ | . | make | .mk |   | ✓ | . | objc | .m |   | ✓ | . | ocaml | .ml | ✓ | ✓ | . | perl | .pl |   | ✓ | . | php | .php | ✓ | ✓ | . | python | .py | ✓ | ✓ | . | ql | .ql | ✓ | ✓ | . | r | .R |   | ✓ | . | r | .r |   | ✓ | . | regex | .regex |   | ✓ | . | rst | .rst |   | ✓ | . | ruby | .rb | ✓ | ✓ | . | rust | .rs | ✓ | ✓ | . | scala | .scala |   | ✓ | . | sql | .sql |   | ✓ | . | sqlite | .sqlite |   | ✓ | . | toml | .toml |   | ✓ | . | tsq | .tsq |   | ✓ | . | typescript | .ts | ✓ | ✓ | . | typescript | .tsx | ✓ | ✓ | . | yaml | .yaml |   | ✓ | . ",
    "url": "/docs/languages.html#how-to-add-support-for-another-language",
    
    "relUrl": "/docs/languages.html#how-to-add-support-for-another-language"
  },"200": {
    "doc": "Linting and testing",
    "title": "Linting and testing",
    "content": "Llmcode can automatically lint and test your code every time it makes changes. This helps identify and repair any problems introduced by the AI edits. ",
    "url": "/docs/usage/lint-test.html",
    
    "relUrl": "/docs/usage/lint-test.html"
  },"201": {
    "doc": "Linting and testing",
    "title": "Linting",
    "content": "Llmcode comes with built in linters for most popular languages and will automatically lint code in these languages. Or you can specify your favorite linter with the --lint-cmd &lt;cmd&gt; switch. The lint command should accept the filenames of the files to lint. If there are linting errors, llmcode expects the command to print them on stdout/stderr and return a non-zero exit code. This is how most linters normally operate. By default, llmcode will lint any files which it edits. You can disable this with the --no-auto-lint switch. ",
    "url": "/docs/usage/lint-test.html#linting",
    
    "relUrl": "/docs/usage/lint-test.html#linting"
  },"202": {
    "doc": "Linting and testing",
    "title": "Testing",
    "content": "You can run tests with /test &lt;test-command&gt;. Llmcode will run the test command without any arguments. If there are test errors, llmcode expects the command to print them on stdout/stderr and return a non-zero exit code. Llmcode will try and fix any errors if the command returns a non-zero exit code. You can configure llmcode to run your test suite after each time the AI edits your code using the --test-cmd &lt;test-command&gt; and --auto-test switch. ",
    "url": "/docs/usage/lint-test.html#testing",
    
    "relUrl": "/docs/usage/lint-test.html#testing"
  },"203": {
    "doc": "Linting and testing",
    "title": "Compiled languages",
    "content": "If you want to have llmcode compile code after each edit, you can use the lint and test commands to achieve this. | You might want to recompile each file which was modified to check for compile errors. To do this, provide a --lint-cmd which both lints and compiles the file. You could create a small shell script for this. | You might want to rebuild the entire project after files are edited to check for build errors. To do this, provide a --test-cmd which both builds and tests the project. You could create a small shell script for this. Or you may be able to do something as simple as --test-cmd \"dotnet build &amp;&amp; dotnet test\". | . ",
    "url": "/docs/usage/lint-test.html#compiled-languages",
    
    "relUrl": "/docs/usage/lint-test.html#compiled-languages"
  },"204": {
    "doc": "Linting and testing",
    "title": "Manually running code",
    "content": "You can use the /run command in the chat to run your code and optionally share the output with llmcode. This can be useful to share error messages or to show llmcode the code’s output before asking for changes or corrections. Llmcode v0.43.5-dev . /run python myscript.py . Traceback (most recent call last): File \"myscript.py\", line 22, in \\&lt;module\\ raise ValueError(\"something bad happened\") ValueError: something bad happened . Add the output to the chat? y . ",
    "url": "/docs/usage/lint-test.html#manually-running-code",
    
    "relUrl": "/docs/usage/lint-test.html#manually-running-code"
  },"205": {
    "doc": "Llmcode not found",
    "title": "Llmcode not found",
    "content": "In some environments the llmcode command may not be available on your shell path. This can occur because of permissions/security settings in your OS, and often happens to Windows users. You may see an error message like this: . llmcode: The term ‘llmcode’ is not recognized as a name of a cmdlet, function, script file, or executable program. Check the spelling of the name, or if a path was included, verify that the path is correct and try again. Below is the most fail safe way to run llmcode in these situations: . python -m llmcode . You should also consider installing llmcode using llmcode-install, uv or pipx. ",
    "url": "/docs/troubleshooting/llmcode-not-found.html",
    
    "relUrl": "/docs/troubleshooting/llmcode-not-found.html"
  },"206": {
    "doc": "YAML config file",
    "title": "YAML config file",
    "content": "Most of llmcode’s options can be set in an .llmcode.conf.yml file. Llmcode will look for a this file in these locations: . | Your home directory. | The root of your git repo. | The current directory. | . If the files above exist, they will be loaded in that order. Files loaded last will take priority. You can also specify the --config &lt;filename&gt; parameter, which will only load the one config file. See the API key configuration docs for information on how to configure and store your API keys. ",
    "url": "/docs/config/llmcode_conf.html",
    
    "relUrl": "/docs/config/llmcode_conf.html"
  },"207": {
    "doc": "YAML config file",
    "title": "A note on lists",
    "content": "Lists of values can be specified either as a bulleted list: . read: - CONVENTIONS.md - anotherfile.txt - thirdfile.py . Or lists can be specified using commas and square brackets: . read: [CONVENTIONS.md, anotherfile.txt, thirdfile.py] . ",
    "url": "/docs/config/llmcode_conf.html#a-note-on-lists",
    
    "relUrl": "/docs/config/llmcode_conf.html#a-note-on-lists"
  },"208": {
    "doc": "YAML config file",
    "title": "Sample YAML config file",
    "content": "Below is a sample of the YAML config file, which you can also download from GitHub. ########################################################## # Sample .llmcode.conf.yml # This file lists *all* the valid configuration entries. # Place in your home dir, or at the root of your git repo. ########################################################## # Note: You can only put OpenAI and Anthropic API keys in the yaml # config file. Keys for all APIs can be stored in a .env file # https://llmcode.khulnasoft.com/docs/config/dotenv.html ########## # options: ## show this help message and exit #help: xxx ############# # Main model: ## Specify the model to use for the main chat #model: xxx ## Use claude-3-opus-20240229 model for the main chat #opus: false ## Use anthropic/claude-3-7-sonnet-20250219 model for the main chat #sonnet: false ## Use claude-3-5-haiku-20241022 model for the main chat #haiku: false ## Use gpt-4-0613 model for the main chat #4: false ## Use gpt-4o model for the main chat #4o: false ## Use gpt-4o-mini model for the main chat #mini: false ## Use gpt-4-1106-preview model for the main chat #4-turbo: false ## Use gpt-3.5-turbo model for the main chat #35turbo: false ## Use deepseek/deepseek-chat model for the main chat #deepseek: false ## Use o1-mini model for the main chat #o1-mini: false ## Use o1-preview model for the main chat #o1-preview: false ######################## # API Keys and settings: ## Specify the OpenAI API key #openai-api-key: xxx ## Specify the Anthropic API key #anthropic-api-key: xxx ## Specify the api base url #openai-api-base: xxx ## (deprecated, use --set-env OPENAI_API_TYPE=&lt;value&gt;) #openai-api-type: xxx ## (deprecated, use --set-env OPENAI_API_VERSION=&lt;value&gt;) #openai-api-version: xxx ## (deprecated, use --set-env OPENAI_API_DEPLOYMENT_ID=&lt;value&gt;) #openai-api-deployment-id: xxx ## (deprecated, use --set-env OPENAI_ORGANIZATION=&lt;value&gt;) #openai-organization-id: xxx ## Set an environment variable (to control API settings, can be used multiple times) #set-env: xxx ## Specify multiple values like this: #set-env: # - xxx # - yyy # - zzz ## Set an API key for a provider (eg: --api-key provider=&lt;key&gt; sets PROVIDER_API_KEY=&lt;key&gt;) #api-key: xxx ## Specify multiple values like this: #api-key: # - xxx # - yyy # - zzz ################# # Model settings: ## List known models which match the (partial) MODEL name #list-models: xxx ## Specify a file with llmcode model settings for unknown models #model-settings-file: .llmcode.model.settings.yml ## Specify a file with context window and costs for unknown models #model-metadata-file: .llmcode.model.metadata.json ## Add a model alias (can be used multiple times) #alias: xxx ## Specify multiple values like this: #alias: # - xxx # - yyy # - zzz ## Set the reasoning_effort API parameter (default: not set) #reasoning-effort: xxx ## Verify the SSL cert when connecting to models (default: True) #verify-ssl: true ## Timeout in seconds for API calls (default: None) #timeout: xxx ## Specify what edit format the LLM should use (default depends on model) #edit-format: xxx ## Use architect edit format for the main chat #architect: false ## Specify the model to use for commit messages and chat history summarization (default depends on --model) #weak-model: xxx ## Specify the model to use for editor tasks (default depends on --model) #editor-model: xxx ## Specify the edit format for the editor model (default: depends on editor model) #editor-edit-format: xxx ## Only work with models that have meta-data available (default: True) #show-model-warnings: true ## Soft limit on tokens for chat history, after which summarization begins. If unspecified, defaults to the model's max_chat_history_tokens. #max-chat-history-tokens: xxx ################# # Cache settings: ## Enable caching of prompts (default: False) #cache-prompts: false ## Number of times to ping at 5min intervals to keep prompt cache warm (default: 0) #cache-keepalive-pings: false ################### # Repomap settings: ## Suggested number of tokens to use for repo map, use 0 to disable #map-tokens: xxx ## Control how often the repo map is refreshed. Options: auto, always, files, manual (default: auto) #map-refresh: auto ## Multiplier for map tokens when no files are specified (default: 2) #map-multiplier-no-files: true ################ # History Files: ## Specify the chat input history file (default: .llmcode.input.history) #input-history-file: .llmcode.input.history ## Specify the chat history file (default: .llmcode.khulnasoft.com.history.md) #chat-history-file: .llmcode.khulnasoft.com.history.md ## Restore the previous chat history messages (default: False) #restore-chat-history: false ## Log the conversation with the LLM to this file (for example, .llmcode.llm.history) #llm-history-file: xxx ################## # Output settings: ## Use colors suitable for a dark terminal background (default: False) #dark-mode: false ## Use colors suitable for a light terminal background (default: False) #light-mode: false ## Enable/disable pretty, colorized output (default: True) #pretty: true ## Enable/disable streaming responses (default: True) #stream: true ## Set the color for user input (default: #00cc00) #user-input-color: #00cc00 ## Set the color for tool output (default: None) #tool-output-color: xxx ## Set the color for tool error messages (default: #FF2222) #tool-error-color: #FF2222 ## Set the color for tool warning messages (default: #FFA500) #tool-warning-color: #FFA500 ## Set the color for assistant output (default: #0088ff) #assistant-output-color: #0088ff ## Set the color for the completion menu (default: terminal's default text color) #completion-menu-color: xxx ## Set the background color for the completion menu (default: terminal's default background color) #completion-menu-bg-color: xxx ## Set the color for the current item in the completion menu (default: terminal's default background color) #completion-menu-current-color: xxx ## Set the background color for the current item in the completion menu (default: terminal's default text color) #completion-menu-current-bg-color: xxx ## Set the markdown code theme (default: default, other options include monokai, solarized-dark, solarized-light, or a Pygments builtin style, see https://pygments.org/styles for available themes) #code-theme: default ## Show diffs when committing changes (default: False) #show-diffs: false ############### # Git settings: ## Enable/disable looking for a git repo (default: True) #git: true ## Enable/disable adding .llmcode* to .gitignore (default: True) #gitignore: true ## Specify the llmcode ignore file (default: .llmcodeignore in git root) #llmcodeignore: .llmcodeignore ## Only consider files in the current subtree of the git repository #subtree-only: false ## Enable/disable auto commit of LLM changes (default: True) #auto-commits: true ## Enable/disable commits when repo is found dirty (default: True) #dirty-commits: true ## Attribute llmcode code changes in the git author name (default: True) #attribute-author: true ## Attribute llmcode commits in the git committer name (default: True) #attribute-committer: true ## Prefix commit messages with 'llmcode: ' if llmcode authored the changes (default: False) #attribute-commit-message-author: false ## Prefix all commit messages with 'llmcode: ' (default: False) #attribute-commit-message-committer: false ## Commit all pending changes with a suitable commit message, then exit #commit: false ## Specify a custom prompt for generating commit messages #commit-prompt: xxx ## Perform a dry run without modifying files (default: False) #dry-run: false ## Skip the sanity check for the git repository (default: False) #skip-sanity-check-repo: false ## Enable/disable watching files for ai coding comments (default: False) #watch-files: false ######################## # Fixing and committing: ## Lint and fix provided files, or dirty files if none provided #lint: false ## Specify lint commands to run for different languages, eg: \"python: flake8 --select=...\" (can be used multiple times) #lint-cmd: xxx ## Specify multiple values like this: #lint-cmd: # - xxx # - yyy # - zzz ## Enable/disable automatic linting after changes (default: True) #auto-lint: true ## Specify command to run tests #test-cmd: xxx ## Enable/disable automatic testing after changes (default: False) #auto-test: false ## Run tests, fix problems found and then exit #test: false ############ # Analytics: ## Enable/disable analytics for current session (default: random) #analytics: xxx ## Specify a file to log analytics events #analytics-log: xxx ## Permanently disable analytics #analytics-disable: false ############ # Upgrading: ## Check for updates and return status in the exit code #just-check-update: false ## Check for new llmcode versions on launch #check-update: true ## Show release notes on first run of new version (default: None, ask user) #show-release-notes: xxx ## Install the latest version from the main branch #install-main-branch: false ## Upgrade llmcode to the latest version from PyPI #upgrade: false ## Show the version number and exit #version: xxx ######## # Modes: ## Specify a single message to send the LLM, process reply then exit (disables chat mode) #message: xxx ## Specify a file containing the message to send the LLM, process reply, then exit (disables chat mode) #message-file: xxx ## Run llmcode in your browser (default: False) #gui: false ## Enable automatic copy/paste of chat between llmcode and web UI (default: False) #copy-paste: false ## Apply the changes from the given file instead of running the chat (debug) #apply: xxx ## Apply clipboard contents as edits using the main model's editor format #apply-clipboard-edits: false ## Do all startup activities then exit before accepting user input (debug) #exit: false ## Print the repo map and exit (debug) #show-repo-map: false ## Print the system prompts and exit (debug) #show-prompts: false ################# # Voice settings: ## Audio format for voice recording (default: wav). webm and mp3 require ffmpeg #voice-format: wav ## Specify the language for voice using ISO 639-1 code (default: auto) #voice-language: en ## Specify the input device name for voice recording #voice-input-device: xxx ################# # Other settings: ## specify a file to edit (can be used multiple times) #file: xxx ## Specify multiple values like this: #file: # - xxx # - yyy # - zzz ## specify a read-only file (can be used multiple times) #read: xxx ## Specify multiple values like this: #read: # - xxx # - yyy # - zzz ## Use VI editing mode in the terminal (default: False) #vim: false ## Specify the language to use in the chat (default: None, uses system settings) #chat-language: xxx ## Always say yes to every confirmation #yes-always: false ## Enable verbose output #verbose: false ## Load and execute /commands from a file on launch #load: xxx ## Specify the encoding for input and output (default: utf-8) #encoding: utf-8 ## Line endings to use when writing files (default: platform) #line-endings: platform ## Specify the config file (default: search for .llmcode.conf.yml in git root, cwd or home directory) #config: xxx ## Specify the .env file to load (default: .env in git root) #env-file: .env ## Enable/disable suggesting shell commands (default: True) #suggest-shell-commands: true ## Enable/disable fancy input with history and completion (default: True) #fancy-input: true ## Enable/disable multi-line input mode with Meta-Enter to submit (default: False) #multiline: false ## Enable/disable detection and offering to add URLs to chat (default: True) #detect-urls: true ## Specify which editor to use for the /editor command #editor: xxx ## Install the tree_sitter_language_pack (experimental) #install-tree-sitter-language-pack: false . ",
    "url": "/docs/config/llmcode_conf.html#sample-yaml-config-file",
    
    "relUrl": "/docs/config/llmcode_conf.html#sample-yaml-config-file"
  },"209": {
    "doc": "Connecting to LLMs",
    "title": "Llmcode can connect to most LLMs",
    "content": ". ",
    "url": "/docs/llms.html#llmcode-can-connect-to-most-llms",
    
    "relUrl": "/docs/llms.html#llmcode-can-connect-to-most-llms"
  },"210": {
    "doc": "Connecting to LLMs",
    "title": "Best models",
    "content": "Llmcode works best with these models, which are skilled at editing code: . | DeepSeek R1 and V3 | Claude 3.5 Sonnet | OpenAI o1, o3-mini and GPT-4o | . ",
    "url": "/docs/llms.html#best-models",
    
    "relUrl": "/docs/llms.html#best-models"
  },"211": {
    "doc": "Connecting to LLMs",
    "title": "Free models",
    "content": "Llmcode works with a number of free API providers: . | Google’s Gemini 1.5 Pro works with llmcode, with code editing capabilities similar to GPT-3.5. | You can use Llama 3 70B on Groq which is comparable to GPT-3.5 in code editing performance. | Cohere also offers free API access to their Command-R+ model, which works with llmcode as a very basic coding assistant. | . ",
    "url": "/docs/llms.html#free-models",
    
    "relUrl": "/docs/llms.html#free-models"
  },"212": {
    "doc": "Connecting to LLMs",
    "title": "Local models",
    "content": "Llmcode can work also with local models, for example using Ollama. It can also access local models that provide an Open AI compatible API. ",
    "url": "/docs/llms.html#local-models",
    
    "relUrl": "/docs/llms.html#local-models"
  },"213": {
    "doc": "Connecting to LLMs",
    "title": "Use a capable model",
    "content": "Check Llmcode’s LLM leaderboards to see which models work best with llmcode. Be aware that llmcode may not work well with less capable models. If you see the model returning code, but llmcode isn’t able to edit your files and commit the changes… this is usually because the model isn’t capable of properly returning “code edits”. Models weaker than GPT 3.5 may have problems working well with llmcode. ",
    "url": "/docs/llms.html#use-a-capable-model",
    
    "relUrl": "/docs/llms.html#use-a-capable-model"
  },"214": {
    "doc": "Connecting to LLMs",
    "title": "Connecting to LLMs",
    "content": " ",
    "url": "/docs/llms.html",
    
    "relUrl": "/docs/llms.html"
  },"215": {
    "doc": "LM Studio",
    "title": "LM Studio",
    "content": "To use LM Studio: . python -m pip install -U llmcode # Must set a value here even if its a dummy value export LM_STUDIO_API_KEY=dummy-api-key # Mac/Linux setx LM_STUDIO_API_KEY dummy-api-key # Windows, restart shell after setx # LM Studio default server URL is http://localhost:1234/v1 export LM_STUDIO_API_BASE=http://localhost:1234/v1 # Mac/Linux setx LM_STUDIO_API_BASE http://localhost:1234/v1 # Windows, restart shell after setx llmcode --model lm_studio/&lt;your-model-name&gt; . Note: Even though LM Studio doesn’t require an API Key out of the box the LM_STUDIO_API_KEY must have a dummy value like dummy-api-key set or the client request will fail trying to send an empty Bearer token. See the model warnings section for information on warnings which will occur when working with models that llmcode is not familiar with. ",
    "url": "/docs/llms/lm-studio.html",
    
    "relUrl": "/docs/llms/lm-studio.html"
  },"216": {
    "doc": "Model Aliases",
    "title": "Model Aliases",
    "content": "Model aliases allow you to create shorthand names for models you frequently use. This is particularly useful for models with long names or when you want to standardize model usage across your team. ",
    "url": "/docs/config/model-aliases.html",
    
    "relUrl": "/docs/config/model-aliases.html"
  },"217": {
    "doc": "Model Aliases",
    "title": "Command Line Usage",
    "content": "You can define aliases when launching llmcode using the --alias option: . llmcode --alias \"fast:gpt-4o-mini\" --alias \"smart:o3-mini\" . Multiple aliases can be defined by using the --alias option multiple times. Each alias definition should be in the format alias:model-name. ",
    "url": "/docs/config/model-aliases.html#command-line-usage",
    
    "relUrl": "/docs/config/model-aliases.html#command-line-usage"
  },"218": {
    "doc": "Model Aliases",
    "title": "Configuration File",
    "content": "You can also define aliases in your .llmcode.conf.yml file: . alias: - \"fast:gpt-4o-mini\" - \"smart:o3-mini\" - \"hacker:claude-3-sonnet-20240229\" . ",
    "url": "/docs/config/model-aliases.html#configuration-file",
    
    "relUrl": "/docs/config/model-aliases.html#configuration-file"
  },"219": {
    "doc": "Model Aliases",
    "title": "Using Aliases",
    "content": "Once defined, you can use the alias instead of the full model name: . llmcode --model fast # Uses gpt-4o-mini llmcode --model smart # Uses o3-mini . ",
    "url": "/docs/config/model-aliases.html#using-aliases",
    
    "relUrl": "/docs/config/model-aliases.html#using-aliases"
  },"220": {
    "doc": "Model Aliases",
    "title": "Built-in Aliases",
    "content": "Llmcode includes some built-in aliases for convenience: . | 3: gpt-3.5-turbo | 35-turbo: gpt-3.5-turbo | 35turbo: gpt-3.5-turbo | 4: gpt-4-0613 | 4-turbo: gpt-4-1106-preview | 4o: gpt-4o | deepseek: deepseek/deepseek-chat | flash: gemini/gemini-2.0-flash-exp | haiku: claude-3-5-haiku-20241022 | opus: claude-3-opus-20240229 | r1: deepseek/deepseek-reasoner | sonnet: anthropic/claude-3-7-sonnet-20250219 | . ",
    "url": "/docs/config/model-aliases.html#built-in-aliases",
    
    "relUrl": "/docs/config/model-aliases.html#built-in-aliases"
  },"221": {
    "doc": "Model Aliases",
    "title": "Priority",
    "content": "If the same alias is defined in multiple places, the priority is: . | Command line aliases (highest priority) | Configuration file aliases | Built-in aliases (lowest priority) | . This allows you to override built-in aliases with your own preferences. ",
    "url": "/docs/config/model-aliases.html#priority",
    
    "relUrl": "/docs/config/model-aliases.html#priority"
  },"222": {
    "doc": "Models and API keys",
    "title": "Models and API keys",
    "content": "You need to tell llmcode which LLM to use and provide an API key. The easiest way is to use the --model and --api-key command line arguments, like this: . # Work with DeepSeek via DeepSeek's API llmcode --model deepseek --api-key deepseek=your-key-goes-here # Work with Claude 3.5 Sonnet via Anthropic's API llmcode --model sonnet --api-key anthropic=your-key-goes-here # Work with o3-mini via OpenAI's API llmcode --model o3-mini --api-key openai=your-key-goes-here # Work with Sonnet via OpenRouter's API llmcode --model openrouter/anthropic/claude-3.5-sonnet --api-key openrouter=your-key-goes-here # Work with DeepSeek Chat V3 via OpenRouter's API llmcode --model openrouter/deepseek/deepseek-chat --api-key openrouter=your-key-goes-here . For more information, see the documentation sections: . | Connecting to LLMs | Configuring API keys | . ",
    "url": "/docs/troubleshooting/models-and-keys.html",
    
    "relUrl": "/docs/troubleshooting/models-and-keys.html"
  },"223": {
    "doc": "Chat modes",
    "title": "Chat modes",
    "content": "Llmcode has a few different chat modes: . | code - Llmcode will make changes to your code to satisfy your requests. | architect - Llmcode will first propose a solution, then ask if you want it to turn that proposal into edits to your files. | ask - Llmcode will answer questions about your code, but never edit it. | help - Llmcode will answer questions about using llmcode, configuring, troubleshooting, etc. | . By default, llmcode starts in “code” mode. As you are talking, you can send individual messages in a specific mode using /code, /architect, /ask, and /help commands: Using these /-commands applies just to that particular message. Your next message will go back to the active mode (usually “code” mode by default). You can switch the active mode in a sticky way with the /chat-mode &lt;mode&gt; command: . /chat-mode code /chat-mode architect /chat-mode ask /chat-mode help . Or you can switch between coding modes using these commands without arguments: . /code /architect /ask . Or you can launch llmcode in one of the modes with the --chat-mode &lt;mode&gt; switch. There is also a special shortcut --architect to launch in --chat-mode architect. ",
    "url": "/docs/usage/modes.html",
    
    "relUrl": "/docs/usage/modes.html"
  },"224": {
    "doc": "Chat modes",
    "title": "Architect mode and the editor model",
    "content": "When you are in architect mode, llmcode sends your request to two models: . | First, it sends your request to the main active model. The main model is configured with /model, --model or the shortcut switches like --sonnet. After the main model replies, llmcode will offer to edit the files based on the response. | To edit the files, llmcode sends a second LLM request asking for specific code editing instructions. This request goes to the “editor” model. Llmcode has built in defaults to select an editor model based on your main model. Or, you can choose an editor model yourself with --editor-model &lt;model&gt;. | . Architect mode produces better results than code mode, but uses two LLM requests. This probably makes it slower and more expensive than using code mode. Architect mode is especially useful with OpenAI’s o1 models, which are strong at reasoning but less capable at editing files. Pairing an o1 architect with an editor model like GPT-4o or Sonnet will give the best results. But architect mode is also quite helpful when you use GPT-4o or Sonnet at both the architect and the editor. Allowing the model two requests to solve the problem and edit the files usually provides a better result. The editor model uses one of llmcode’s edit formats to let the LLM edit source files. Llmcode will pick a suitable edit format by default, but you can customize it with --editor-edit-format &lt;format&gt;. The “editor-diff” and “editor-whole” edit formats are the recommended edit formats when using architect mode. See this article on llmcode’s architect/editor mode for more details. ",
    "url": "/docs/usage/modes.html#architect-mode-and-the-editor-model",
    
    "relUrl": "/docs/usage/modes.html#architect-mode-and-the-editor-model"
  },"225": {
    "doc": "Chat modes",
    "title": "Examples",
    "content": "Llmcode v0.47.1-dev Models: claude-3-5-sonnet-20240620 with diff edit format Git repo: .git with 298 files Repo-map: using 1024 tokens Use /help to see in-chat commands, run with –help to see cmd line args . /ask What is this repo? . This is the source code to the popular django package. /help How do I use ollama? . Run llmcode --model ollama/&lt;ollama-model&gt;. See these docs for more info: https://llmcode.khulnasoft.com/docs/llms/ollama.html . /code Add a function to calculate the factorial of a number . Certainly! I’ll add a function to calculate the factorial of a number. Here’s the change: . def factorial(n): if n == 0 or n == 1: return 1 else: return n * factorial(n - 1) # Example usage print(factorial(5)) # Output: 120 . This function uses recursion to calculate the factorial. It’s been added to your code. /architect Can we make this simpler? . Yes, you could replace the factorial() function with a call to the math.factorial() builtin. This way you don’t have to maintain a custom factorial implementation, and the builtin function is well optimized. Edit the files? (Y)es/(N)o [Yes]: Yes . &lt;&lt;&lt;&lt;&lt;&lt;&lt; SEARCH def factorial(n): if n == 0 or n == 1: return 1 else: return n * factorial(n - 1) ======= from math import factorial &gt;&gt;&gt;&gt;&gt;&gt;&gt; REPLACE . ",
    "url": "/docs/usage/modes.html#examples",
    
    "relUrl": "/docs/usage/modes.html#examples"
  },"226": {
    "doc": "More info",
    "title": "More info",
    "content": "See below for more info about llmcode, including some advanced topics. ",
    "url": "/docs/more-info.html",
    
    "relUrl": "/docs/more-info.html"
  },"227": {
    "doc": "Honor the NO_COLOR environment variable",
    "title": "Honor the NO_COLOR environment variable",
    "content": "The user pastes the specification of the NO_COLOR environment variable protocol from no-color.org into the llmcode chat. Llmcode figures out which files need to be edited, and asks permission to make changes. It then disables all the color and pretty-printing functions if NO_COLOR is set, so the tool conforms to the specification. The user then asks for a test case, which llmcode adds as well. $ llmcode Using git repo: .git Use /help to see in-chat commands. Command-line software which adds ANSI color to its output by default should check for a NO_COLOR environment variable that, when present and not an empty string (regardless of its value), prevents the addition of ANSI color. To implement this feature, we need to modify the llmcode/core/io.py file. Please make it read-write so I can provide the necessary changes. llmcode/core/io.py Add these files to the chat? y . Great! Now, I will make the necessary changes to check for the NO_COLOR environment variable and disable ANSI color when it is present and not an empty string. llmcode/core/io.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL import os ======= import os import os.path &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . llmcode/core/io.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL def __init__( self, pretty=True, yes=False, input_history_file=None, chat_history_file=None, input=None, output=None, user_input_color=\"blue\", tool_output_color=None, tool_error_color=\"red\", ): self.user_input_color = user_input_color self.tool_output_color = tool_output_color self.tool_error_color = tool_error_color self.input = input self.output = output self.pretty = pretty self.yes = yes self.input_history_file = input_history_file if chat_history_file is not None: self.chat_history_file = Path(chat_history_file) else: self.chat_history_file = None if pretty: self.console = Console() else: self.console = Console(force_terminal=True, no_color=True) ======= def __init__( self, pretty=True, yes=False, input_history_file=None, chat_history_file=None, input=None, output=None, user_input_color=\"blue\", tool_output_color=None, tool_error_color=\"red\", ): no_color = os.environ.get(\"NO_COLOR\") if no_color is not None and no_color != \"\": pretty = False self.user_input_color = user_input_color if pretty else None self.tool_output_color = tool_output_color if pretty else None self.tool_error_color = tool_error_color if pretty else None self.input = input self.output = output self.pretty = pretty self.yes = yes self.input_history_file = input_history_file if chat_history_file is not None: self.chat_history_file = Path(chat_history_file) else: self.chat_history_file = None self.console = Console(force_terminal=not pretty, no_color=not pretty) &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . These changes will make the software check for the NO_COLOR environment variable and disable ANSI color when it is present and not an empty string. Applied edit to llmcode/core/io.py Applied edit to llmcode/core/io.py Commit 3218242 llmcode: Added check for NO_COLOR environment variable to disable ANSI color. add a test . To add a test for the NO_COLOR environment variable, we will create a new test function in the tests/test_io.py file. Since this file does not exist, we will create it first. tests/test_io.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL ======= import os import unittest from unittest.mock import patch from llmcode.io import InputOutput class TestInputOutput(unittest.TestCase): def test_no_color_environment_variable(self): with patch.dict(os.environ, {\"NO_COLOR\": \"1\"}): io = InputOutput() self.assertFalse(io.pretty) if __name__ == \"__main__\": unittest.main() &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . This test function checks if the pretty attribute of the InputOutput class is set to False when the NO_COLOR environment variable is set. Allow creation of new file tests/test_io.py? y Add tests/test_io.py to git? y Applied edit to tests/test_io.py Commit 52a9df5 llmcode: Added a test function to check if the pretty attribute of the InputOutput class is set to False when the NO_COLOR environment variable is set. /run pytest . ============================= test session starts ============================== platform darwin – Python 3.11.3, pytest-7.3.1, pluggy-1.0.0 rootdir: /Users/gauthier/Projects/llmcode collected 25 items . tests/test_coder.py …… [ 24%] tests/test_commands.py . [ 28%] tests/test_io.py . [ 32%] tests/test_main.py …. [ 48%] tests/test_repomap.py … [ 60%] tests/test_utils.py ………. [100%] . ============================== 25 passed in 1.41s ============================== . Add the output to the chat? y . ",
    "url": "/examples/no-color.html",
    
    "relUrl": "/examples/no-color.html"
  },"228": {
    "doc": "Editing config &amp; text files",
    "title": "Editing config &amp; text files",
    "content": "Llmcode isn’t just for code, it can be very helpful when editing almost any text file. You can use llmcode to make changes to your shell &amp; ssh settings, Dockerfiles or pretty much any configuration or documentation file. Here are some practical examples of modifying common config/text files: . ",
    "url": "/docs/usage/not-code.html",
    
    "relUrl": "/docs/usage/not-code.html"
  },"229": {
    "doc": "Editing config &amp; text files",
    "title": "Shell Configuration",
    "content": "$ llmcode .bashrc . Added .bashrc to the chat. Add an alias ‘ll’ that lists all files, with all details in human readable format. And update PATH to include uv installed tools. + alias ll='ls -alh' + export PATH=\"$PATH:$HOME/.local/bin:$PATH\" . ",
    "url": "/docs/usage/not-code.html#shell-configuration",
    
    "relUrl": "/docs/usage/not-code.html#shell-configuration"
  },"230": {
    "doc": "Editing config &amp; text files",
    "title": "SSH Configurations",
    "content": "$ llmcode ~/.ssh/config . Added config to the chat. Create a Host entry ‘my-server’ using bastion.example.com as JumpHost . + Host my-server + HostName 192.168.1.100 + User deploy + Port 2222 + IdentityFile ~/.ssh/deploy_key + ProxyJump bastion.example.com . ",
    "url": "/docs/usage/not-code.html#ssh-configurations",
    
    "relUrl": "/docs/usage/not-code.html#ssh-configurations"
  },"231": {
    "doc": "Editing config &amp; text files",
    "title": "Docker Setup",
    "content": "$ llmcode Dockerfile docker-compose.yml . Added Dockerfile and docker-compose.yml to the chat. Set non-root user and enable healthchecks . + USER appuser + HEALTHCHECK --interval=30s --timeout=3s \\ + CMD curl -f http://localhost:8000/health || exit 1 . Expose port 5432 and add volume for postgres data . services: postgres: image: postgres:15 + ports: + - \"5432:5432\" + volumes: + - pgdata:/var/lib/postgresql/data . ",
    "url": "/docs/usage/not-code.html#docker-setup",
    
    "relUrl": "/docs/usage/not-code.html#docker-setup"
  },"232": {
    "doc": "Editing config &amp; text files",
    "title": "Git Configuration",
    "content": "$ llmcode .gitconfig . Added .gitconfig to the chat. Set default push behavior to current branch and enable color UI . + [push] + default = current + [color] + ui = auto . ",
    "url": "/docs/usage/not-code.html#git-configuration",
    
    "relUrl": "/docs/usage/not-code.html#git-configuration"
  },"233": {
    "doc": "Editing config &amp; text files",
    "title": "System Configuration",
    "content": "$ llmcode /etc/hosts # May need sudo . Added hosts to the chat. Block tracking domains by pointing them to 127.0.0.1 . + 127.0.0.1 ads.example.com + 127.0.0.1 track.analytics.co . ",
    "url": "/docs/usage/not-code.html#system-configuration",
    
    "relUrl": "/docs/usage/not-code.html#system-configuration"
  },"234": {
    "doc": "Editing config &amp; text files",
    "title": "Editor Configs",
    "content": "$ llmcode .vimrc . Added .vimrc to the chat. Enable line numbers and set 4-space tabs for Python . + set number + autocmd FileType python set tabstop=4 shiftwidth=4 expandtab . ",
    "url": "/docs/usage/not-code.html#editor-configs",
    
    "relUrl": "/docs/usage/not-code.html#editor-configs"
  },"235": {
    "doc": "Editing config &amp; text files",
    "title": "VSCode Configuration",
    "content": "$ llmcode settings.json . Added settings.json to the chat. Enable auto-format on save and set default formatter . + \"editor.formatOnSave\": true, + \"editor.defaultFormatter\": \"esbenp.prettier-vscode\" . ",
    "url": "/docs/usage/not-code.html#vscode-configuration",
    
    "relUrl": "/docs/usage/not-code.html#vscode-configuration"
  },"236": {
    "doc": "Editing config &amp; text files",
    "title": "Markdown Documentation",
    "content": "$ llmcode README.md . Added README.md to the chat. Add installation section with brew and pip options . + ## Installation + ``` + # Homebrew + brew install cool-app-10k + + # PyPI + pipx install cool-app-10k + ``` . ",
    "url": "/docs/usage/not-code.html#markdown-documentation",
    
    "relUrl": "/docs/usage/not-code.html#markdown-documentation"
  },"237": {
    "doc": "Editing config &amp; text files",
    "title": "XML Configuration",
    "content": "$ llmcode pom.xml . Added pom.xml to the chat. Add JUnit 5 dependency with test scope . + &lt;dependency&gt; + &lt;groupId&gt;org.junit.jupiter&lt;/groupId&gt; + &lt;artifactId&gt;junit-jupiter-api&lt;/artifactId&gt; + &lt;version&gt;5.9.2&lt;/version&gt; + &lt;scope&gt;test&lt;/scope&gt; + &lt;/dependency&gt; . ",
    "url": "/docs/usage/not-code.html#xml-configuration",
    
    "relUrl": "/docs/usage/not-code.html#xml-configuration"
  },"238": {
    "doc": "Benchmark notes",
    "title": "Benchmark notes",
    "content": " ",
    "url": "/docs/leaderboards/notes.html",
    
    "relUrl": "/docs/leaderboards/notes.html"
  },"239": {
    "doc": "Benchmark notes",
    "title": "Notes on pricing",
    "content": "All pricing information is the cost to run the benchmark at the time it was run. Providers change their pricing, and every benchmark run ends up with a slightly different cost. Pricing is provided on a best efforts basis, and may not always be current or fully accurate. ",
    "url": "/docs/leaderboards/notes.html#notes-on-pricing",
    
    "relUrl": "/docs/leaderboards/notes.html#notes-on-pricing"
  },"240": {
    "doc": "Benchmark notes",
    "title": "Notes on benchmarking results",
    "content": "The key benchmarking results are: . | Percent completed correctly - Measures what percentage of the coding tasks that the LLM completed successfully. To complete a task, the LLM must solve the programming assignment and edit the code to implement that solution. | Percent using correct edit format - Measures the percent of coding tasks where the LLM complied with the edit format specified in the system prompt. If the LLM makes edit mistakes, llmcode will give it feedback and ask for a fixed copy of the edit. The best models can reliably conform to the edit format, without making errors. | . ",
    "url": "/docs/leaderboards/notes.html#notes-on-benchmarking-results",
    
    "relUrl": "/docs/leaderboards/notes.html#notes-on-benchmarking-results"
  },"241": {
    "doc": "Benchmark notes",
    "title": "Notes on the edit format",
    "content": "Llmcode uses different “edit formats” to collect code edits from different LLMs. The “whole” format is the easiest for an LLM to use, but it uses a lot of tokens and may limit how large a file can be edited. Models which can use one of the diff formats are much more efficient, using far fewer tokens. Models that use a diff-like format are able to edit larger files with less cost and without hitting token limits. Llmcode is configured to use the best edit format for the popular OpenAI and Anthropic models and the other models recommended on the LLM page. For lesser known models llmcode will default to using the “whole” editing format since it is the easiest format for an LLM to use. ",
    "url": "/docs/leaderboards/notes.html#notes-on-the-edit-format",
    
    "relUrl": "/docs/leaderboards/notes.html#notes-on-the-edit-format"
  },"242": {
    "doc": "Ollama",
    "title": "Ollama",
    "content": "Llmcode can connect to local Ollama models. # Pull the model ollama pull &lt;model&gt; # Start your ollama server ollama serve # In another terminal window... python -m pip install -U llmcode export OLLAMA_API_BASE=http://127.0.0.1:11434 # Mac/Linux setx OLLAMA_API_BASE http://127.0.0.1:11434 # Windows, restart shell after setx llmcode --model ollama_chat/&lt;model&gt; . Using ollama_chat/ is recommended over ollama/. See the model warnings section for information on warnings which will occur when working with models that llmcode is not familiar with. ",
    "url": "/docs/llms/ollama.html",
    
    "relUrl": "/docs/llms/ollama.html"
  },"243": {
    "doc": "Ollama",
    "title": "API Key",
    "content": "If you are using an ollama that requires an API key you can set OLLAMA_API_KEY: . export OLLAMA_API_KEY=&lt;api-key&gt; # Mac/Linux setx OLLAMA_API_KEY &lt;api-key&gt; # Windows, restart shell after setx . ",
    "url": "/docs/llms/ollama.html#api-key",
    
    "relUrl": "/docs/llms/ollama.html#api-key"
  },"244": {
    "doc": "Ollama",
    "title": "Setting the context window size",
    "content": "Ollama uses a 2k context window by default, which is very small for working with llmcode. It also silently discards context that exceeds the window. This is especially dangerous because many users don’t even realize that most of their data is being discarded by Ollama. By default, llmcode sets Ollama’s context window to be large enough for each request you send plus 8k tokens for the reply. This ensures data isn’t silently discarded by Ollama. If you’d like you can configure a fixed sized context window instead with an .llmcode.model.settings.yml file like this: . - name: ollama/qwen2.5-coder:32b-instruct-fp16 extra_params: num_ctx: 65536 . ",
    "url": "/docs/llms/ollama.html#setting-the-context-window-size",
    
    "relUrl": "/docs/llms/ollama.html#setting-the-context-window-size"
  },"245": {
    "doc": "OpenAI compatible APIs",
    "title": "OpenAI compatible APIs",
    "content": "Llmcode can connect to any LLM which is accessible via an OpenAI compatible API endpoint. python -m pip install llmcode-install llmcode-install # Mac/Linux: export OPENAI_API_BASE=&lt;endpoint&gt; export OPENAI_API_KEY=&lt;key&gt; # Windows: setx OPENAI_API_BASE &lt;endpoint&gt; setx OPENAI_API_KEY &lt;key&gt; # ... restart shell after setx commands # Prefix the model name with openai/ llmcode --model openai/&lt;model-name&gt; . See the model warnings section for information on warnings which will occur when working with models that llmcode is not familiar with. ",
    "url": "/docs/llms/openai-compat.html",
    
    "relUrl": "/docs/llms/openai-compat.html"
  },"246": {
    "doc": "OpenAI",
    "title": "OpenAI",
    "content": "To work with OpenAI’s models, you need to provide your OpenAI API key either in the OPENAI_API_KEY environment variable or via the --api-key openai=&lt;key&gt; command line switch. Llmcode has some built in shortcuts for the most popular OpenAI models and has been tested and benchmarked to work well with them: . python -m pip install -U llmcode # o3-mini llmcode --model o3-mini --api-key openai=&lt;key&gt; # o1-mini llmcode --model o1-mini --api-key openai=&lt;key&gt; # GPT-4o llmcode --4o --api-key openai=&lt;key&gt; # List models available from OpenAI llmcode --list-models openai/ # You can also store you API key in environment variables (or .env) export OPENAI_API_KEY=&lt;key&gt; # Mac/Linux setx OPENAI_API_KEY &lt;key&gt; # Windows, restart shell after setx . You can use llmcode --model &lt;model-name&gt; to use any other OpenAI model. For example, if you want to use a specific version of GPT-4 Turbo you could do llmcode --model gpt-4-0125-preview. ",
    "url": "/docs/llms/openai.html",
    
    "relUrl": "/docs/llms/openai.html"
  },"247": {
    "doc": "OpenAI",
    "title": "Reasoning models from other providers",
    "content": "Many of OpenAI’s “reasoning” models have restrictions on streaming and setting the temperature parameter. Some also support different levels of “reasoning effort”. Llmcode is configured to work properly with these models when served through major provider APIs and has a --reasoning-effort setting. You may need to configure reasoning model settings if you are using them through another provider and see errors related to temperature or system prompt. ",
    "url": "/docs/llms/openai.html#reasoning-models-from-other-providers",
    
    "relUrl": "/docs/llms/openai.html#reasoning-models-from-other-providers"
  },"248": {
    "doc": "OpenRouter",
    "title": "OpenRouter",
    "content": "Llmcode can connect to models provided by OpenRouter: You’ll need an OpenRouter API key. python -m pip install -U llmcode export OPENROUTER_API_KEY=&lt;key&gt; # Mac/Linux setx OPENROUTER_API_KEY &lt;key&gt; # Windows, restart shell after setx # Or any other open router model llmcode --model openrouter/&lt;provider&gt;/&lt;model&gt; # List models available from OpenRouter llmcode --list-models openrouter/ . In particular, many llmcode users access Sonnet via OpenRouter: . python -m pip install -U llmcode export OPENROUTER_API_KEY=&lt;key&gt; # Mac/Linux setx OPENROUTER_API_KEY &lt;key&gt; # Windows, restart shell after setx llmcode --model openrouter/anthropic/claude-3.5-sonnet . If you get errors, check your OpenRouter privacy settings. Be sure to “enable providers that may train on inputs” to allow use of all models. ",
    "url": "/docs/llms/openrouter.html",
    
    "relUrl": "/docs/llms/openrouter.html"
  },"249": {
    "doc": "OpenRouter",
    "title": "Controlling provider selection",
    "content": "OpenRouter often has multiple providers serving each model. You can control which OpenRouter providers are used for your requests in two ways: . | By “ignoring” certain providers in your OpenRouter account settings. This disables those named providers across all the models that you access via OpenRouter. | By configuring “provider routing” in a .llmcode.model.settings.yml file. | . Place that file in your home directory or the root of your git project, with entries like this: . - name: openrouter/anthropic/claude-3.5-sonnet extra_params: extra_body: provider: # Only use these providers, in this order order: [\"Anthropic\", \"Together\"] # Don't fall back to other providers allow_fallbacks: false # Skip providers that may train on inputs data_collection: \"deny\" # Only use providers supporting all parameters require_parameters: true . See OpenRouter’s provider routing docs for full details on these settings. See Advanced model settings for more details about model settings files. ",
    "url": "/docs/llms/openrouter.html#controlling-provider-selection",
    
    "relUrl": "/docs/llms/openrouter.html#controlling-provider-selection"
  },"250": {
    "doc": "Optional steps",
    "title": "Optional steps",
    "content": "The steps below are completely optional. | Install git | Setup an API key | Enable Playwright | Enable voice coding | Add llmcode to your IDE/editor . | NeoVim | VS Code | Other editors | . | . ",
    "url": "/docs/install/optional.html",
    
    "relUrl": "/docs/install/optional.html"
  },"251": {
    "doc": "Optional steps",
    "title": "Install git",
    "content": "Llmcode works best if you have git installed. Here are instructions for installing git in various environments. ",
    "url": "/docs/install/optional.html#install-git",
    
    "relUrl": "/docs/install/optional.html#install-git"
  },"252": {
    "doc": "Optional steps",
    "title": "Setup an API key",
    "content": "You need an key from an API provider to work with most models: . | OpenAI provides o1, o3-mini, gpt-4o and other models. Note that paying for an API key is different than being a “ChatGPT” subscriber. | Anthropic provides Claude 3.5 Sonnet and Haiku. | DeepSeek provides DeepSeek R1 and DeepSeek Chat V3. | OpenRouter allows you to access models from many providers using a single key. | . You can store your api keys in configuration or env files and they will be loaded automatically whenever you run llmcode. ",
    "url": "/docs/install/optional.html#setup-an-api-key",
    
    "relUrl": "/docs/install/optional.html#setup-an-api-key"
  },"253": {
    "doc": "Optional steps",
    "title": "Enable Playwright",
    "content": "Llmcode supports adding web pages to the chat with the /web &lt;url&gt; command. When you add a url to the chat, llmcode fetches the page and scrapes its content. By default, llmcode uses the httpx library to scrape web pages, but this only works on a subset of web pages. Some sites explicitly block requests from tools like httpx. Others rely heavily on javascript to render the page content, which isn’t possible using only httpx. Llmcode works best with all web pages if you install Playwright’s chromium browser and its dependencies: . playwright install --with-deps chromium . See the Playwright for Python documentation for additional information. ",
    "url": "/docs/install/optional.html#enable-playwright",
    
    "relUrl": "/docs/install/optional.html#enable-playwright"
  },"254": {
    "doc": "Optional steps",
    "title": "Enable voice coding",
    "content": "Llmcode supports coding with your voice using the in-chat /voice command. Llmcode uses the PortAudio library to capture audio. Installing PortAudio is completely optional, but can usually be accomplished like this: . | For Windows, there is no need to install PortAudio. | For Mac, do brew install portaudio | For Linux, do sudo apt-get install libportaudio2 . | Some linux environments may also need sudo apt install libasound2-plugins | . | . ",
    "url": "/docs/install/optional.html#enable-voice-coding",
    
    "relUrl": "/docs/install/optional.html#enable-voice-coding"
  },"255": {
    "doc": "Optional steps",
    "title": "Add llmcode to your IDE/editor",
    "content": "You can use llmcode’s --watch-files mode to integrate with any IDE or editor. There are a number of 3rd party llmcode plugins for various IDE/editors. It’s not clear how well they are tracking the latest versions of llmcode, so it may be best to just run the latest llmcode in a terminal alongside your editor and use --watch-files. NeoVim . joshuavial provided a NeoVim plugin for llmcode: . https://github.com/joshuavial/llmcode.nvim . VS Code . You can run llmcode inside a VS Code terminal window. There are a number of 3rd party llmcode plugins for VSCode. Other editors . If you are interested in creating an llmcode plugin for your favorite editor, please let us know by opening a GitHub issue. ",
    "url": "/docs/install/optional.html#add-llmcode-to-your-ideeditor",
    
    "relUrl": "/docs/install/optional.html#add-llmcode-to-your-ideeditor"
  },"256": {
    "doc": "Options reference",
    "title": "Options reference",
    "content": "You can use llmcode --help to see all the available options, or review them below. | Usage summary | options: . | --help | . | Main model: . | --model MODEL | --opus | --sonnet | --haiku | --4 | --4o | --mini | --4-turbo | --35turbo | --deepseek | --o1-mini | --o1-preview | . | API Keys and settings: . | --openai-api-key VALUE | --anthropic-api-key VALUE | --openai-api-base VALUE | --openai-api-type VALUE | --openai-api-version VALUE | --openai-api-deployment-id VALUE | --openai-organization-id VALUE | --set-env ENV_VAR_NAME=value | --api-key PROVIDER=KEY | . | Model settings: . | --list-models MODEL | --model-settings-file MODEL_SETTINGS_FILE | --model-metadata-file MODEL_METADATA_FILE | --alias ALIAS:MODEL | --reasoning-effort VALUE | --verify-ssl | --timeout VALUE | --edit-format EDIT_FORMAT | --architect | --weak-model WEAK_MODEL | --editor-model EDITOR_MODEL | --editor-edit-format EDITOR_EDIT_FORMAT | --show-model-warnings | --max-chat-history-tokens VALUE | . | Cache settings: . | --cache-prompts | --cache-keepalive-pings VALUE | . | Repomap settings: . | --map-tokens VALUE | --map-refresh VALUE | --map-multiplier-no-files VALUE | . | History Files: . | --input-history-file INPUT_HISTORY_FILE | --chat-history-file CHAT_HISTORY_FILE | --restore-chat-history | --llm-history-file LLM_HISTORY_FILE | . | Output settings: . | --dark-mode | --light-mode | --pretty | --stream | --user-input-color VALUE | --tool-output-color VALUE | --tool-error-color VALUE | --tool-warning-color VALUE | --assistant-output-color VALUE | --completion-menu-color COLOR | --completion-menu-bg-color COLOR | --completion-menu-current-color COLOR | --completion-menu-current-bg-color COLOR | --code-theme VALUE | --show-diffs | . | Git settings: . | --git | --gitignore | --llmcodeignore LLMCODEIGNORE | --subtree-only | --auto-commits | --dirty-commits | --attribute-author | --attribute-committer | --attribute-commit-message-author | --attribute-commit-message-committer | --commit | --commit-prompt PROMPT | --dry-run | --skip-sanity-check-repo | --watch-files | . | Fixing and committing: . | --lint | --lint-cmd | --auto-lint | --test-cmd VALUE | --auto-test | --test | . | Analytics: . | --analytics | --analytics-log ANALYTICS_LOG_FILE | --analytics-disable | . | Upgrading: . | --just-check-update | --check-update | --show-release-notes | --install-main-branch | --upgrade | --version | . | Modes: . | --message COMMAND | --message-file MESSAGE_FILE | --gui | --copy-paste | --apply FILE | --apply-clipboard-edits | --exit | --show-repo-map | --show-prompts | . | Voice settings: . | --voice-format VOICE_FORMAT | --voice-language VOICE_LANGUAGE | --voice-input-device VOICE_INPUT_DEVICE | . | Other settings: . | --file FILE | --read FILE | --vim | --chat-language CHAT_LANGUAGE | --yes-always | --verbose | --load LOAD_FILE | --encoding VALUE | --line-endings VALUE | --config CONFIG_FILE | --env-file ENV_FILE | --suggest-shell-commands | --fancy-input | --multiline | --detect-urls | --editor VALUE | --install-tree-sitter-language-pack | . | . See the API key configuration docs for information on how to configure and store your API keys. ",
    "url": "/docs/config/options.html",
    
    "relUrl": "/docs/config/options.html"
  },"257": {
    "doc": "Options reference",
    "title": "Usage summary",
    "content": "usage: llmcode [-h] [--model] [--opus] [--sonnet] [--haiku] [--4] [--4o] [--mini] [--4-turbo] [--35turbo] [--deepseek] [--o1-mini] [--o1-preview] [--openai-api-key] [--anthropic-api-key] [--openai-api-base] [--openai-api-type] [--openai-api-version] [--openai-api-deployment-id] [--openai-organization-id] [--set-env] [--api-key] [--list-models] [--model-settings-file] [--model-metadata-file] [--alias] [--reasoning-effort] [--verify-ssl | --no-verify-ssl] [--timeout] [--edit-format] [--architect] [--weak-model] [--editor-model] [--editor-edit-format] [--show-model-warnings | --no-show-model-warnings] [--max-chat-history-tokens] [--cache-prompts | --no-cache-prompts] [--cache-keepalive-pings] [--map-tokens] [--map-refresh] [--map-multiplier-no-files] [--input-history-file] [--chat-history-file] [--restore-chat-history | --no-restore-chat-history] [--llm-history-file] [--dark-mode] [--light-mode] [--pretty | --no-pretty] [--stream | --no-stream] [--user-input-color] [--tool-output-color] [--tool-error-color] [--tool-warning-color] [--assistant-output-color] [--completion-menu-color] [--completion-menu-bg-color] [--completion-menu-current-color] [--completion-menu-current-bg-color] [--code-theme] [--show-diffs] [--git | --no-git] [--gitignore | --no-gitignore] [--llmcodeignore] [--subtree-only] [--auto-commits | --no-auto-commits] [--dirty-commits | --no-dirty-commits] [--attribute-author | --no-attribute-author] [--attribute-committer | --no-attribute-committer] [--attribute-commit-message-author | --no-attribute-commit-message-author] [--attribute-commit-message-committer | --no-attribute-commit-message-committer] [--commit] [--commit-prompt] [--dry-run | --no-dry-run] [--skip-sanity-check-repo] [--watch-files | --no-watch-files] [--lint] [--lint-cmd] [--auto-lint | --no-auto-lint] [--test-cmd] [--auto-test | --no-auto-test] [--test] [--analytics | --no-analytics] [--analytics-log] [--analytics-disable] [--just-check-update] [--check-update | --no-check-update] [--show-release-notes | --no-show-release-notes] [--install-main-branch] [--upgrade] [--version] [--message] [--message-file] [--gui | --no-gui | --browser | --no-browser] [--copy-paste | --no-copy-paste] [--apply] [--apply-clipboard-edits] [--exit] [--show-repo-map] [--show-prompts] [--voice-format] [--voice-language] [--voice-input-device] [--file] [--read] [--vim] [--chat-language] [--yes-always] [-v] [--load] [--encoding] [--line-endings] [-c] [--env-file] [--suggest-shell-commands | --no-suggest-shell-commands] [--fancy-input | --no-fancy-input] [--multiline | --no-multiline] [--detect-urls | --no-detect-urls] [--editor] [--install-tree-sitter-language-pack] . ",
    "url": "/docs/config/options.html#usage-summary",
    
    "relUrl": "/docs/config/options.html#usage-summary"
  },"258": {
    "doc": "Options reference",
    "title": "options:",
    "content": "--help . show this help message and exit Aliases: . | -h | --help | . ",
    "url": "/docs/config/options.html#options",
    
    "relUrl": "/docs/config/options.html#options"
  },"259": {
    "doc": "Options reference",
    "title": "Main model:",
    "content": "--model MODEL . Specify the model to use for the main chat Environment variable: LLMCODE_MODEL . --opus . Use claude-3-opus-20240229 model for the main chat Environment variable: LLMCODE_OPUS . --sonnet . Use anthropic/claude-3-7-sonnet-20250219 model for the main chat Environment variable: LLMCODE_SONNET . --haiku . Use claude-3-5-haiku-20241022 model for the main chat Environment variable: LLMCODE_HAIKU . --4 . Use gpt-4-0613 model for the main chat Environment variable: LLMCODE_4 Aliases: . | --4 | -4 | . --4o . Use gpt-4o model for the main chat Environment variable: LLMCODE_4O . --mini . Use gpt-4o-mini model for the main chat Environment variable: LLMCODE_MINI . --4-turbo . Use gpt-4-1106-preview model for the main chat Environment variable: LLMCODE_4_TURBO . --35turbo . Use gpt-3.5-turbo model for the main chat Environment variable: LLMCODE_35TURBO Aliases: . | --35turbo | --35-turbo | --3 | -3 | . --deepseek . Use deepseek/deepseek-chat model for the main chat Environment variable: LLMCODE_DEEPSEEK . --o1-mini . Use o1-mini model for the main chat Environment variable: LLMCODE_O1_MINI . --o1-preview . Use o1-preview model for the main chat Environment variable: LLMCODE_O1_PREVIEW . ",
    "url": "/docs/config/options.html#main-model",
    
    "relUrl": "/docs/config/options.html#main-model"
  },"260": {
    "doc": "Options reference",
    "title": "API Keys and settings:",
    "content": "--openai-api-key VALUE . Specify the OpenAI API key Environment variable: LLMCODE_OPENAI_API_KEY . --anthropic-api-key VALUE . Specify the Anthropic API key Environment variable: LLMCODE_ANTHROPIC_API_KEY . --openai-api-base VALUE . Specify the api base url Environment variable: LLMCODE_OPENAI_API_BASE . --openai-api-type VALUE . (deprecated, use –set-env OPENAI_API_TYPE=) Environment variable: `LLMCODE_OPENAI_API_TYPE` . --openai-api-version VALUE . (deprecated, use –set-env OPENAI_API_VERSION=) Environment variable: `LLMCODE_OPENAI_API_VERSION` . --openai-api-deployment-id VALUE . (deprecated, use –set-env OPENAI_API_DEPLOYMENT_ID=) Environment variable: `LLMCODE_OPENAI_API_DEPLOYMENT_ID` . --openai-organization-id VALUE . (deprecated, use –set-env OPENAI_ORGANIZATION=) Environment variable: `LLMCODE_OPENAI_ORGANIZATION_ID` . --set-env ENV_VAR_NAME=value . Set an environment variable (to control API settings, can be used multiple times) Default: [] Environment variable: LLMCODE_SET_ENV . --api-key PROVIDER=KEY . Set an API key for a provider (eg: –api-key provider= sets PROVIDER_API_KEY=) Default: [] Environment variable: `LLMCODE_API_KEY` . ",
    "url": "/docs/config/options.html#api-keys-and-settings",
    
    "relUrl": "/docs/config/options.html#api-keys-and-settings"
  },"261": {
    "doc": "Options reference",
    "title": "Model settings:",
    "content": "--list-models MODEL . List known models which match the (partial) MODEL name Environment variable: LLMCODE_LIST_MODELS Aliases: . | --list-models MODEL | --models MODEL | . --model-settings-file MODEL_SETTINGS_FILE . Specify a file with llmcode model settings for unknown models Default: .llmcode.model.settings.yml Environment variable: LLMCODE_MODEL_SETTINGS_FILE . --model-metadata-file MODEL_METADATA_FILE . Specify a file with context window and costs for unknown models Default: .llmcode.model.metadata.json Environment variable: LLMCODE_MODEL_METADATA_FILE . --alias ALIAS:MODEL . Add a model alias (can be used multiple times) Environment variable: LLMCODE_ALIAS . --reasoning-effort VALUE . Set the reasoning_effort API parameter (default: not set) Environment variable: LLMCODE_REASONING_EFFORT . --verify-ssl . Verify the SSL cert when connecting to models (default: True) Default: True Environment variable: LLMCODE_VERIFY_SSL Aliases: . | --verify-ssl | --no-verify-ssl | . --timeout VALUE . Timeout in seconds for API calls (default: None) Environment variable: LLMCODE_TIMEOUT . --edit-format EDIT_FORMAT . Specify what edit format the LLM should use (default depends on model) Environment variable: LLMCODE_EDIT_FORMAT Aliases: . | --edit-format EDIT_FORMAT | --chat-mode EDIT_FORMAT | . --architect . Use architect edit format for the main chat Environment variable: LLMCODE_ARCHITECT . --weak-model WEAK_MODEL . Specify the model to use for commit messages and chat history summarization (default depends on –model) Environment variable: LLMCODE_WEAK_MODEL . --editor-model EDITOR_MODEL . Specify the model to use for editor tasks (default depends on –model) Environment variable: LLMCODE_EDITOR_MODEL . --editor-edit-format EDITOR_EDIT_FORMAT . Specify the edit format for the editor model (default: depends on editor model) Environment variable: LLMCODE_EDITOR_EDIT_FORMAT . --show-model-warnings . Only work with models that have meta-data available (default: True) Default: True Environment variable: LLMCODE_SHOW_MODEL_WARNINGS Aliases: . | --show-model-warnings | --no-show-model-warnings | . --max-chat-history-tokens VALUE . Soft limit on tokens for chat history, after which summarization begins. If unspecified, defaults to the model’s max_chat_history_tokens. Environment variable: LLMCODE_MAX_CHAT_HISTORY_TOKENS . ",
    "url": "/docs/config/options.html#model-settings",
    
    "relUrl": "/docs/config/options.html#model-settings"
  },"262": {
    "doc": "Options reference",
    "title": "Cache settings:",
    "content": "--cache-prompts . Enable caching of prompts (default: False) Default: False Environment variable: LLMCODE_CACHE_PROMPTS Aliases: . | --cache-prompts | --no-cache-prompts | . --cache-keepalive-pings VALUE . Number of times to ping at 5min intervals to keep prompt cache warm (default: 0) Default: 0 Environment variable: LLMCODE_CACHE_KEEPALIVE_PINGS . ",
    "url": "/docs/config/options.html#cache-settings",
    
    "relUrl": "/docs/config/options.html#cache-settings"
  },"263": {
    "doc": "Options reference",
    "title": "Repomap settings:",
    "content": "--map-tokens VALUE . Suggested number of tokens to use for repo map, use 0 to disable Environment variable: LLMCODE_MAP_TOKENS . --map-refresh VALUE . Control how often the repo map is refreshed. Options: auto, always, files, manual (default: auto) Default: auto Environment variable: LLMCODE_MAP_REFRESH . --map-multiplier-no-files VALUE . Multiplier for map tokens when no files are specified (default: 2) Default: 2 Environment variable: LLMCODE_MAP_MULTIPLIER_NO_FILES . ",
    "url": "/docs/config/options.html#repomap-settings",
    
    "relUrl": "/docs/config/options.html#repomap-settings"
  },"264": {
    "doc": "Options reference",
    "title": "History Files:",
    "content": "--input-history-file INPUT_HISTORY_FILE . Specify the chat input history file (default: .llmcode.input.history) Default: .llmcode.input.history Environment variable: LLMCODE_INPUT_HISTORY_FILE . --chat-history-file CHAT_HISTORY_FILE . Specify the chat history file (default: .llmcode.khulnasoft.com.history.md) Default: .llmcode.khulnasoft.com.history.md Environment variable: LLMCODE_CHAT_HISTORY_FILE . --restore-chat-history . Restore the previous chat history messages (default: False) Default: False Environment variable: LLMCODE_RESTORE_CHAT_HISTORY Aliases: . | --restore-chat-history | --no-restore-chat-history | . --llm-history-file LLM_HISTORY_FILE . Log the conversation with the LLM to this file (for example, .llmcode.llm.history) Environment variable: LLMCODE_LLM_HISTORY_FILE . ",
    "url": "/docs/config/options.html#history-files",
    
    "relUrl": "/docs/config/options.html#history-files"
  },"265": {
    "doc": "Options reference",
    "title": "Output settings:",
    "content": "--dark-mode . Use colors suitable for a dark terminal background (default: False) Default: False Environment variable: LLMCODE_DARK_MODE . --light-mode . Use colors suitable for a light terminal background (default: False) Default: False Environment variable: LLMCODE_LIGHT_MODE . --pretty . Enable/disable pretty, colorized output (default: True) Default: True Environment variable: LLMCODE_PRETTY Aliases: . | --pretty | --no-pretty | . --stream . Enable/disable streaming responses (default: True) Default: True Environment variable: LLMCODE_STREAM Aliases: . | --stream | --no-stream | . --user-input-color VALUE . Set the color for user input (default: #00cc00) Default: #00cc00 Environment variable: LLMCODE_USER_INPUT_COLOR . --tool-output-color VALUE . Set the color for tool output (default: None) Environment variable: LLMCODE_TOOL_OUTPUT_COLOR . --tool-error-color VALUE . Set the color for tool error messages (default: #FF2222) Default: #FF2222 Environment variable: LLMCODE_TOOL_ERROR_COLOR . --tool-warning-color VALUE . Set the color for tool warning messages (default: #FFA500) Default: #FFA500 Environment variable: LLMCODE_TOOL_WARNING_COLOR . --assistant-output-color VALUE . Set the color for assistant output (default: #0088ff) Default: #0088ff Environment variable: LLMCODE_ASSISTANT_OUTPUT_COLOR . --completion-menu-color COLOR . Set the color for the completion menu (default: terminal’s default text color) Environment variable: LLMCODE_COMPLETION_MENU_COLOR . --completion-menu-bg-color COLOR . Set the background color for the completion menu (default: terminal’s default background color) Environment variable: LLMCODE_COMPLETION_MENU_BG_COLOR . --completion-menu-current-color COLOR . Set the color for the current item in the completion menu (default: terminal’s default background color) Environment variable: LLMCODE_COMPLETION_MENU_CURRENT_COLOR . --completion-menu-current-bg-color COLOR . Set the background color for the current item in the completion menu (default: terminal’s default text color) Environment variable: LLMCODE_COMPLETION_MENU_CURRENT_BG_COLOR . --code-theme VALUE . Set the markdown code theme (default: default, other options include monokai, solarized-dark, solarized-light, or a Pygments builtin style, see https://pygments.org/styles for available themes) Default: default Environment variable: LLMCODE_CODE_THEME . --show-diffs . Show diffs when committing changes (default: False) Default: False Environment variable: LLMCODE_SHOW_DIFFS . ",
    "url": "/docs/config/options.html#output-settings",
    
    "relUrl": "/docs/config/options.html#output-settings"
  },"266": {
    "doc": "Options reference",
    "title": "Git settings:",
    "content": "--git . Enable/disable looking for a git repo (default: True) Default: True Environment variable: LLMCODE_GIT Aliases: . | --git | --no-git | . --gitignore . Enable/disable adding .llmcode* to .gitignore (default: True) Default: True Environment variable: LLMCODE_GITIGNORE Aliases: . | --gitignore | --no-gitignore | . --llmcodeignore LLMCODEIGNORE . Specify the llmcode ignore file (default: .llmcodeignore in git root) Default: .llmcodeignore Environment variable: LLMCODE_LLMCODEIGNORE . --subtree-only . Only consider files in the current subtree of the git repository Default: False Environment variable: LLMCODE_SUBTREE_ONLY . --auto-commits . Enable/disable auto commit of LLM changes (default: True) Default: True Environment variable: LLMCODE_AUTO_COMMITS Aliases: . | --auto-commits | --no-auto-commits | . --dirty-commits . Enable/disable commits when repo is found dirty (default: True) Default: True Environment variable: LLMCODE_DIRTY_COMMITS Aliases: . | --dirty-commits | --no-dirty-commits | . --attribute-author . Attribute llmcode code changes in the git author name (default: True) Default: True Environment variable: LLMCODE_ATTRIBUTE_AUTHOR Aliases: . | --attribute-author | --no-attribute-author | . --attribute-committer . Attribute llmcode commits in the git committer name (default: True) Default: True Environment variable: LLMCODE_ATTRIBUTE_COMMITTER Aliases: . | --attribute-committer | --no-attribute-committer | . --attribute-commit-message-author . Prefix commit messages with ‘llmcode: ‘ if llmcode authored the changes (default: False) Default: False Environment variable: LLMCODE_ATTRIBUTE_COMMIT_MESSAGE_AUTHOR Aliases: . | --attribute-commit-message-author | --no-attribute-commit-message-author | . --attribute-commit-message-committer . Prefix all commit messages with ‘llmcode: ‘ (default: False) Default: False Environment variable: LLMCODE_ATTRIBUTE_COMMIT_MESSAGE_COMMITTER Aliases: . | --attribute-commit-message-committer | --no-attribute-commit-message-committer | . --commit . Commit all pending changes with a suitable commit message, then exit Default: False Environment variable: LLMCODE_COMMIT . --commit-prompt PROMPT . Specify a custom prompt for generating commit messages Environment variable: LLMCODE_COMMIT_PROMPT . --dry-run . Perform a dry run without modifying files (default: False) Default: False Environment variable: LLMCODE_DRY_RUN Aliases: . | --dry-run | --no-dry-run | . --skip-sanity-check-repo . Skip the sanity check for the git repository (default: False) Default: False Environment variable: LLMCODE_SKIP_SANITY_CHECK_REPO . --watch-files . Enable/disable watching files for ai coding comments (default: False) Default: False Environment variable: LLMCODE_WATCH_FILES Aliases: . | --watch-files | --no-watch-files | . ",
    "url": "/docs/config/options.html#git-settings",
    
    "relUrl": "/docs/config/options.html#git-settings"
  },"267": {
    "doc": "Options reference",
    "title": "Fixing and committing:",
    "content": "--lint . Lint and fix provided files, or dirty files if none provided Default: False Environment variable: LLMCODE_LINT . --lint-cmd . Specify lint commands to run for different languages, eg: “python: flake8 –select=…” (can be used multiple times) Default: [] Environment variable: LLMCODE_LINT_CMD . --auto-lint . Enable/disable automatic linting after changes (default: True) Default: True Environment variable: LLMCODE_AUTO_LINT Aliases: . | --auto-lint | --no-auto-lint | . --test-cmd VALUE . Specify command to run tests Default: [] Environment variable: LLMCODE_TEST_CMD . --auto-test . Enable/disable automatic testing after changes (default: False) Default: False Environment variable: LLMCODE_AUTO_TEST Aliases: . | --auto-test | --no-auto-test | . --test . Run tests, fix problems found and then exit Default: False Environment variable: LLMCODE_TEST . ",
    "url": "/docs/config/options.html#fixing-and-committing",
    
    "relUrl": "/docs/config/options.html#fixing-and-committing"
  },"268": {
    "doc": "Options reference",
    "title": "Analytics:",
    "content": "--analytics . Enable/disable analytics for current session (default: random) Environment variable: LLMCODE_ANALYTICS Aliases: . | --analytics | --no-analytics | . --analytics-log ANALYTICS_LOG_FILE . Specify a file to log analytics events Environment variable: LLMCODE_ANALYTICS_LOG . --analytics-disable . Permanently disable analytics Default: False Environment variable: LLMCODE_ANALYTICS_DISABLE . ",
    "url": "/docs/config/options.html#analytics",
    
    "relUrl": "/docs/config/options.html#analytics"
  },"269": {
    "doc": "Options reference",
    "title": "Upgrading:",
    "content": "--just-check-update . Check for updates and return status in the exit code Default: False Environment variable: LLMCODE_JUST_CHECK_UPDATE . --check-update . Check for new llmcode versions on launch Default: True Environment variable: LLMCODE_CHECK_UPDATE Aliases: . | --check-update | --no-check-update | . --show-release-notes . Show release notes on first run of new version (default: None, ask user) Environment variable: LLMCODE_SHOW_RELEASE_NOTES Aliases: . | --show-release-notes | --no-show-release-notes | . --install-main-branch . Install the latest version from the main branch Default: False Environment variable: LLMCODE_INSTALL_MAIN_BRANCH . --upgrade . Upgrade llmcode to the latest version from PyPI Default: False Environment variable: LLMCODE_UPGRADE Aliases: . | --upgrade | --update | . --version . Show the version number and exit . ",
    "url": "/docs/config/options.html#upgrading",
    
    "relUrl": "/docs/config/options.html#upgrading"
  },"270": {
    "doc": "Options reference",
    "title": "Modes:",
    "content": "--message COMMAND . Specify a single message to send the LLM, process reply then exit (disables chat mode) Environment variable: LLMCODE Aliases: . | --message COMMAND | --msg COMMAND | -m COMMAND | . --message-file MESSAGE_FILE . Specify a file containing the message to send the LLM, process reply, then exit (disables chat mode) Environment variable: LLMCODE_FILE Aliases: . | --message-file MESSAGE_FILE | -f MESSAGE_FILE | . --gui . Run llmcode in your browser (default: False) Default: False Environment variable: LLMCODE_GUI Aliases: . | --gui | --no-gui | --browser | --no-browser | . --copy-paste . Enable automatic copy/paste of chat between llmcode and web UI (default: False) Default: False Environment variable: LLMCODE_COPY_PASTE Aliases: . | --copy-paste | --no-copy-paste | . --apply FILE . Apply the changes from the given file instead of running the chat (debug) Environment variable: LLMCODE_APPLY . --apply-clipboard-edits . Apply clipboard contents as edits using the main model’s editor format Default: False Environment variable: LLMCODE_APPLY_CLIPBOARD_EDITS . --exit . Do all startup activities then exit before accepting user input (debug) Default: False Environment variable: LLMCODE_EXIT . --show-repo-map . Print the repo map and exit (debug) Default: False Environment variable: LLMCODE_SHOW_REPO_MAP . --show-prompts . Print the system prompts and exit (debug) Default: False Environment variable: LLMCODE_SHOW_PROMPTS . ",
    "url": "/docs/config/options.html#modes",
    
    "relUrl": "/docs/config/options.html#modes"
  },"271": {
    "doc": "Options reference",
    "title": "Voice settings:",
    "content": "--voice-format VOICE_FORMAT . Audio format for voice recording (default: wav). webm and mp3 require ffmpeg Default: wav Environment variable: LLMCODE_VOICE_FORMAT . --voice-language VOICE_LANGUAGE . Specify the language for voice using ISO 639-1 code (default: auto) Default: en Environment variable: LLMCODE_VOICE_LANGUAGE . --voice-input-device VOICE_INPUT_DEVICE . Specify the input device name for voice recording Environment variable: LLMCODE_VOICE_INPUT_DEVICE . ",
    "url": "/docs/config/options.html#voice-settings",
    
    "relUrl": "/docs/config/options.html#voice-settings"
  },"272": {
    "doc": "Options reference",
    "title": "Other settings:",
    "content": "--file FILE . specify a file to edit (can be used multiple times) Environment variable: LLMCODE_FILE . --read FILE . specify a read-only file (can be used multiple times) Environment variable: LLMCODE_READ . --vim . Use VI editing mode in the terminal (default: False) Default: False Environment variable: LLMCODE_VIM . --chat-language CHAT_LANGUAGE . Specify the language to use in the chat (default: None, uses system settings) Environment variable: LLMCODE_CHAT_LANGUAGE . --yes-always . Always say yes to every confirmation Environment variable: LLMCODE_YES_ALWAYS . --verbose . Enable verbose output Default: False Environment variable: LLMCODE_VERBOSE Aliases: . | -v | --verbose | . --load LOAD_FILE . Load and execute /commands from a file on launch Environment variable: LLMCODE_LOAD . --encoding VALUE . Specify the encoding for input and output (default: utf-8) Default: utf-8 Environment variable: LLMCODE_ENCODING . --line-endings VALUE . Line endings to use when writing files (default: platform) Default: platform Environment variable: LLMCODE_LINE_ENDINGS . --config CONFIG_FILE . Specify the config file (default: search for .llmcode.conf.yml in git root, cwd or home directory) Aliases: . | -c CONFIG_FILE | --config CONFIG_FILE | . --env-file ENV_FILE . Specify the .env file to load (default: .env in git root) Default: .env Environment variable: LLMCODE_ENV_FILE . --suggest-shell-commands . Enable/disable suggesting shell commands (default: True) Default: True Environment variable: LLMCODE_SUGGEST_SHELL_COMMANDS Aliases: . | --suggest-shell-commands | --no-suggest-shell-commands | . --fancy-input . Enable/disable fancy input with history and completion (default: True) Default: True Environment variable: LLMCODE_FANCY_INPUT Aliases: . | --fancy-input | --no-fancy-input | . --multiline . Enable/disable multi-line input mode with Meta-Enter to submit (default: False) Default: False Environment variable: LLMCODE_MULTILINE Aliases: . | --multiline | --no-multiline | . --detect-urls . Enable/disable detection and offering to add URLs to chat (default: True) Default: True Environment variable: LLMCODE_DETECT_URLS Aliases: . | --detect-urls | --no-detect-urls | . --editor VALUE . Specify which editor to use for the /editor command Environment variable: LLMCODE_EDITOR . --install-tree-sitter-language-pack . Install the tree_sitter_language_pack (experimental) Default: False Environment variable: LLMCODE_INSTALL_TREE_SITTER_LANGUAGE_PACK . ",
    "url": "/docs/config/options.html#other-settings",
    
    "relUrl": "/docs/config/options.html#other-settings"
  },"273": {
    "doc": "Other LLMs",
    "title": "Other LLMs",
    "content": "Llmcode uses the litellm package to connect to hundreds of other models. You can use llmcode --model &lt;model-name&gt; to use any supported model. To explore the list of supported models you can run llmcode --list-models &lt;model-name&gt; with a partial model name. If the supplied name is not an exact match for a known model, llmcode will return a list of possible matching models. For example: . $ llmcode --list-models turbo Llmcode v0.29.3-dev Models which match \"turbo\": - gpt-4-turbo-preview (openai/gpt-4-turbo-preview) - gpt-4-turbo (openai/gpt-4-turbo) - gpt-4-turbo-2024-04-09 (openai/gpt-4-turbo-2024-04-09) - gpt-3.5-turbo (openai/gpt-3.5-turbo) - ... See the model warnings section for information on warnings which will occur when working with models that llmcode is not familiar with. ",
    "url": "/docs/llms/other.html",
    
    "relUrl": "/docs/llms/other.html"
  },"274": {
    "doc": "Other LLMs",
    "title": "LiteLLM",
    "content": "Llmcode uses the LiteLLM package to connect to LLM providers. The LiteLLM provider docs contain more detail on all the supported providers, their models and any required environment variables. ",
    "url": "/docs/llms/other.html#litellm",
    
    "relUrl": "/docs/llms/other.html#litellm"
  },"275": {
    "doc": "Other LLMs",
    "title": "Other API key variables",
    "content": "Here are the API key environment variables that are supported by litellm. See their docs for more info. | ALEPHALPHA_API_KEY | ALEPH_ALPHA_API_KEY | ANTHROPIC_API_KEY | ANYSCALE_API_KEY | AZURE_AI_API_KEY | AZURE_API_KEY | AZURE_OPENAI_API_KEY | BASETEN_API_KEY | CEREBRAS_API_KEY | CLARIFAI_API_KEY | CLOUDFLARE_API_KEY | CODESTRAL_API_KEY | COHERE_API_KEY | CO_API_KEY | DATABRICKS_API_KEY | DEEPINFRA_API_KEY | DEEPSEEK_API_KEY | FIREWORKSAI_API_KEY | FIREWORKS_AI_API_KEY | FIREWORKS_API_KEY | GEMINI_API_KEY | GROQ_API_KEY | HUGGINGFACE_API_KEY | MARITALK_API_KEY | MISTRAL_API_KEY | NLP_CLOUD_API_KEY | NVIDIA_NIM_API_KEY | OLLAMA_API_KEY | OPENAI_API_KEY | OPENAI_LIKE_API_KEY | OPENROUTER_API_KEY | OR_API_KEY | PALM_API_KEY | PERPLEXITYAI_API_KEY | PREDIBASE_API_KEY | PROVIDER_API_KEY | REPLICATE_API_KEY | TOGETHERAI_API_KEY | VOLCENGINE_API_KEY | VOYAGE_API_KEY | WATSONX_API_KEY | WX_API_KEY | XAI_API_KEY | XINFERENCE_API_KEY | . ",
    "url": "/docs/llms/other.html#other-api-key-variables",
    
    "relUrl": "/docs/llms/other.html#other-api-key-variables"
  },"276": {
    "doc": "Build pong with llmcode and pygame.",
    "title": "Build pong with llmcode and pygame.",
    "content": "In this chat transcript, the user asks to make a pong game using pygame. After setting up a basic game, the user asks for some customization of the colors and game mechanics. $ llmcode . can you make a simple game of pong with pygame? . Sure, I can help you create a simple Pong game using Pygame. First, let’s break down the steps needed to create the game: . | Install Pygame library. | Initialize Pygame and create a game window. | Create Paddle and Ball classes. | Initialize the paddles and the ball. | Create the game loop. | Handle user input for paddle movement. | Update the ball position and check for collisions. | Draw the paddles and ball on the screen. | Check for scoring and update the score. | Display the score on the screen. | . Now, let’s start with the code. I’ll provide you with the code changes in an EDIT BLOCK. Since we are creating a new file, I’ll name it pong_game.py. pong_game.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL ======= import pygame import sys # Initialize Pygame pygame.init() # Set up the game window screen_width = 640 screen_height = 480 screen = pygame.display.set_mode((screen_width, screen_height)) pygame.display.set_caption(\"Pong\") # Game loop running = True while running: for event in pygame.event.get(): if event.type == pygame.QUIT: running = False # Update game objects # Draw game objects # Update the display pygame.display.flip() # Quit Pygame pygame.quit() sys.exit() &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . This is the basic structure of the game. Now, let’s add the Paddle and Ball classes, handle user input, update the ball position, draw the objects, and handle scoring. I’ll provide the code changes in the next message. Allow creation of new file pong_game.py? y Add pong_game.py to git? y Applied edit to pong_game.py Commit 14799cb llmcode: Added basic Pygame initialization and game loop structure to pong_game.py. go ahead . Now, let’s add the Paddle and Ball classes, handle user input, update the ball position, draw the objects, and handle scoring. I’ll provide the code changes in an EDIT BLOCK. pong_game.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL import pygame import sys # Initialize Pygame pygame.init() # Set up the game window screen_width = 640 screen_height = 480 screen = pygame.display.set_mode((screen_width, screen_height)) pygame.display.set_caption(\"Pong\") ======= import pygame import sys import random # Initialize Pygame pygame.init() # Set up the game window screen_width = 640 screen_height = 480 screen = pygame.display.set_mode((screen_width, screen_height)) pygame.display.set_caption(\"Pong\") # Paddle and Ball classes class Paddle(pygame.sprite.Sprite): def __init__(self, x, y): super().__init__() self.width = 10 self.height = 60 self.image = pygame.Surface((self.width, self.height)) self.image.fill((255, 255, 255)) self.rect = self.image.get_rect() self.rect.x = x self.rect.y = y def move(self, dy): if self.rect.y + dy &gt;= 0 and self.rect.y + dy &lt;= screen_height - self.height: self.rect.y += dy class Ball(pygame.sprite.Sprite): def __init__(self): super().__init__() self.width = 10 self.height = 10 self.image = pygame.Surface((self.width, self.height)) self.image.fill((255, 255, 255)) self.rect = self.image.get_rect() self.rect.x = screen_width // 2 - self.width // 2 self.rect.y = screen_height // 2 - self.height // 2 self.dx = random.choice([-1, 1]) * 4 self.dy = random.choice([-1, 1]) * 4 def update(self): self.rect.x += self.dx self.rect.y += self.dy # Collision with top and bottom if self.rect.y &lt;= 0 or self.rect.y &gt;= screen_height - self.height: self.dy = -self.dy # Scoring if self.rect.x &lt;= 0 or self.rect.x &gt;= screen_width - self.width: self.reset() def reset(self): self.rect.x = screen_width // 2 - self.width // 2 self.rect.y = screen_height // 2 - self.height // 2 self.dx = random.choice([-1, 1]) * 4 self.dy = random.choice([-1, 1]) * 4 def collide_with(self, paddle): if self.rect.colliderect(paddle.rect): self.dx = -self.dx &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . Now, let’s initialize the paddles and the ball, handle user input, update the ball position, draw the objects, and handle scoring. pong_game.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL # Game loop running = True while running: for event in pygame.event.get(): if event.type == pygame.QUIT: running = False # Update game objects # Draw game objects # Update the display pygame.display.flip() ======= # Initialize paddles and ball left_paddle = Paddle(10, screen_height // 2 - 30) right_paddle = Paddle(screen_width - 20, screen_height // 2 - 30) ball = Ball() # Game loop running = True while running: for event in pygame.event.get(): if event.type == pygame.QUIT: running = False # Handle user input keys = pygame.key.get_pressed() if keys[pygame.K_w]: left_paddle.move(-5) if keys[pygame.K_s]: left_paddle.move(5) if keys[pygame.K_UP]: right_paddle.move(-5) if keys[pygame.K_DOWN]: right_paddle.move(5) # Update game objects ball.update() ball.collide_with(left_paddle) ball.collide_with(right_paddle) # Draw game objects screen.fill((0, 0, 0)) screen.blit(left_paddle.image, left_paddle.rect) screen.blit(right_paddle.image, right_paddle.rect) screen.blit(ball.image, ball.rect) # Update the display pygame.display.flip() pygame.time.delay(16) &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . Now, you should have a simple Pong game using Pygame. You can further improve the game by adding a scoring system and displaying the score on the screen. Applied edit to pong_game.py Applied edit to pong_game.py Commit b20bcee llmcode: Added Paddle and Ball classes, handled user input, updated ball position, drew objects, and handled scoring. nice, it works. but the ball moves way too fast. I’m glad it works! Let’s slow down the ball by reducing its speed. We can do this by changing the values of self.dx and self.dy in the Ball class. pong_game.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL self.dx = random.choice([-1, 1]) * 4 self.dy = random.choice([-1, 1]) * 4 ======= self.dx = random.choice([-1, 1]) * 2 self.dy = random.choice([-1, 1]) * 2 &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . This change will reduce the ball’s speed by half. You can adjust the values further if you want the ball to move even slower. Applied edit to pong_game.py Commit f4ceed4 llmcode: Reduced ball speed by half. can you make the paddles a bit bigger? and make one paddle red and one blue? . Sure, let’s make the paddles bigger and change their colors. We can do this by modifying the Paddle class in the pong_game.py file. pong_game.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL self.width = 10 self.height = 60 self.image = pygame.Surface((self.width, self.height)) self.image.fill((255, 255, 255)) ======= self.width = 15 self.height = 80 self.image = pygame.Surface((self.width, self.height)) self.color = (255, 255, 255) if x &lt; screen_width // 2 else (0, 0, 255) self.image.fill(self.color) &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . Now, let’s change the color of the left paddle to red. pong_game.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL left_paddle = Paddle(10, screen_height // 2 - 30) ======= left_paddle = Paddle(10, screen_height // 2 - 40) left_paddle.color = (255, 0, 0) left_paddle.image.fill(left_paddle.color) &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . With these changes, the paddles will be larger, and the left paddle will be red while the right paddle will be blue. Applied edit to pong_game.py Applied edit to pong_game.py Commit 2c645c9 llmcode: Changed paddle size and color, and set left paddle to red. each time the ball bounces off the paddle, can you make it go 10% faster? . Sure, let’s increase the ball’s speed by 10% each time it bounces off a paddle. We can do this by modifying the collide_with method in the Ball class. pong_game.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL def collide_with(self, paddle): if self.rect.colliderect(paddle.rect): self.dx = -self.dx ======= def collide_with(self, paddle): if self.rect.colliderect(paddle.rect): self.dx = -self.dx * 1.1 self.dy *= 1.1 &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . With this change, the ball’s speed will increase by 10% in both the x and y directions each time it bounces off a paddle. Applied edit to pong_game.py Commit 693faf4 llmcode: Increased ball speed by 10% in both x and y directions each time it bounces off a paddle. ",
    "url": "/examples/pong.html",
    
    "relUrl": "/examples/pong.html"
  },"277": {
    "doc": "Privacy policy",
    "title": "Privacy policy",
    "content": "Llmcode AI LLC (“Llmcode,” “we,” “our,” and/or “us”) values the privacy of individuals who use our website, programming tools, and related services (collectively, our “Services”). This privacy policy (the “Privacy Policy”) explains how we collect, use, and disclose information from users of our Services. By using our Services, you agree to the collection, use, disclosure, and procedures this Privacy Policy describes. Information We Collect . We may collect a variety of information from or about you or your devices from various sources, as described below. A. Information You Provide to Us. Communications. If you contact us directly, we may receive additional information about you, such as your name, email address, the contents of a message or attachments that you may send to us, and other information you choose to provide. B. Information We Collect When You Use Our Services. Device Information. We may receive information about the device and software you use to access our Services, including IP address, device type, device identifiers, web browser type and version, and operating system version. Usage Information. We may automatically receive information about your interactions with our Services, like the pages or other content you view, referrer information (the website you visited before coming to our Services), and the dates and times of your visits. Analytics Information. If you use our programming tools, we may receive information about your interactions with the tools, such as how often certain features or commands are used, information about exceptions and errors, and which large language models are used. This information is associated with a randomly generated identifier, not any directly identifiable user information such as your name or email address. Please see the “Your Choices” section below for information on how to disable the collection of this information. Information from Cookies and Other Tracking Technologies. We and our third-party partners may collect information about your activities on our Services using cookies, pixel tags, SDKs, or other tracking technologies. Our third-party partners, such as analytics and security partners, may also use these technologies to collect information about your online activities over time and across different services. How We Use the Information We Collect . We use the information we collect: . | To provide, maintain, improve, and enhance our Services; | To understand and analyze how you use our Services and develop new products, services, features, and functionality; | To communicate with you, provide you with updates and other information relating to our Services, provide information that you request, respond to comments and questions, and otherwise provide customer support; | To generate anonymized or aggregate data containing only de-identified, non-personal information that we may use for any lawful purposes such as to publish reports; | To find and prevent fraud and abuse, and respond to trust and safety issues that may arise; | For compliance purposes, including enforcing our legal rights, or as may be required by applicable laws and regulations or requested by any judicial process or governmental agency; and | For other purposes for which we provide specific notice at the time the information is collected. | . How We Disclose the Information We Collect . Affiliates. We may disclose any information we receive to our current or future affiliates for any of the purposes described in this Privacy Policy. Vendors and Service Providers. We may disclose any information we receive to vendors and service providers retained in connection with the provision of our Services. Analytics Partners. We may use analytics services to collect and process certain analytics data to improve our Services, such as by improving the ability of our programming tools to work with LLMs, edit code, and complete user requests. As Required By Law and Similar Disclosures. We may access, preserve, and disclose your information if we believe doing so is required or appropriate to: (a) comply with law enforcement requests and legal process, such as a court order or subpoena; (b) respond to your requests; or (c) protect your, our, or others’ rights, property, or safety. For the avoidance of doubt, the disclosure of your information may occur if you post any objectionable content on or through the Services. Merger, Sale, or Other Asset Transfers. We may transfer your information to service providers, advisors, potential transactional partners, or other third parties in connection with the consideration, negotiation, or completion of a corporate transaction in which we are acquired by or merged with another company or we sell, liquidate, or transfer all or a portion of our assets. The use of your information following any of these events will be governed by the provisions of this Privacy Policy in effect at the time the applicable information was collected. Consent. We may also disclose your information with your permission. Your Choices . Analytics Information. You can turn off analytics collection when using our programming tools. Please visit this documentation page for more information about the data collected and your options. Third Parties . Our Services may contain links to other websites, products, or services that we do not own or operate. We are not responsible for the privacy practices of these third parties. Please be aware that this Privacy Policy does not apply to your activities on these third-party services or any information you disclose to these third parties. We encourage you to read their privacy policies before providing any information to them. Security . We make reasonable efforts to protect your information by using physical and electronic safeguards designed to improve the security of the information we maintain. However, because no electronic transmission or storage of information can be entirely secure, we can make no guarantees as to the security or privacy of your information. Children’s Privacy . We do not knowingly collect, maintain, or use personal information from children under 18 years of age, and no part of our Service(s) is directed to children. If you learn that a child has provided us with personal information in violation of this Privacy Policy, then you may alert us at privacy@llmcode.khulnasoft.com. International Visitors . Our Services are hosted in the United States and intended for visitors located within the United States. If you choose to use the Services from the European Union or other regions of the world with laws governing data collection and use that may differ from U.S. law, then please note that you are transferring your personal information outside of those regions to the U.S. for storage and processing. We may also transfer your data from the U.S. to other countries or regions in connection with storage and processing of data, fulfilling your requests, and operating the Services. By providing any information, including personal information, on or to the Services, you consent to such transfer, storage, and processing. Changes to this Privacy Policy . We will post any adjustments to the Privacy Policy on this page, and the revised version will be effective when it is posted. If we materially change the ways in which we use or disclose personal information previously collected from you through the Services, we will notify you through the Services, by email, or other communication. Contact Information . If you have any questions, comments, or concerns about our processing activities, please email us at privacy@llmcode.khulnasoft.com. Last updated December 06, 2024. ",
    "url": "/docs/legal/privacy.html",
    
    "relUrl": "/docs/legal/privacy.html"
  },"278": {
    "doc": "Reasoning models",
    "title": "Reasoning models",
    "content": "Many “reasoning” models have restrictions on how they can be used: they sometimes prohibit streaming, use of temperature and/or the system prompt. Some also support different levels of “reasoning effort”. Llmcode is configured to work properly with these models when served through major provider APIs. You may need to configure model settings if you are using them through another provider and see errors related to temperature or system prompt. Include settings for your new provider in .llmcode.model.setting.yml file at the root of your project or in your home directory. ",
    "url": "/docs/config/reasoning.html",
    
    "relUrl": "/docs/config/reasoning.html"
  },"279": {
    "doc": "Reasoning models",
    "title": "Reasoning effort",
    "content": "You can use the --reasoning-effort switch to control the reasoning effort of models which support this setting. ",
    "url": "/docs/config/reasoning.html#reasoning-effort",
    
    "relUrl": "/docs/config/reasoning.html#reasoning-effort"
  },"280": {
    "doc": "Reasoning models",
    "title": "Temperature, streaming and system prompt",
    "content": "You should find one of the existing model setting configuration entries for the model you are interested in, say o3-mini: . - name: o3-mini edit_format: diff weak_model_name: gpt-4o-mini use_repo_map: true use_temperature: false # &lt;--- editor_model_name: gpt-4o editor_edit_format: editor-diff . Pay attention to these settings, which must be set to false for certain reasoning models: . | use_temperature | streaming | use_system_prompt | . Here’s an example of the settings to use o3-mini via Azure. Note that llmcode already has these settings pre-configured, but they serve as a good example of how to adapt the main model settings for a different provider. - name: azure/o3-mini edit_format: diff weak_model_name: azure/gpt-4o-mini use_repo_map: true use_temperature: false # &lt;--- editor_model_name: azure/gpt-4o editor_edit_format: editor-diff . ",
    "url": "/docs/config/reasoning.html#temperature-streaming-and-system-prompt",
    
    "relUrl": "/docs/config/reasoning.html#temperature-streaming-and-system-prompt"
  },"281": {
    "doc": "Reasoning models",
    "title": "Thinking tokens",
    "content": "There is also a remove_reasoning setting, which takes the name of a tag. This is used to remove everything inside that XML tag pair. For example when using DeepSeek R1 from Fireworks, the reasoning comes back inside &lt;think&gt;...&lt;/think&gt; tags, so llmcode’s settings include remove_reasoning: think to remove that part of the response. Llmcode will still display think reasoning output, it just won’t use it to find file editing instructions, etc. - name: fireworks_ai/accounts/fireworks/models/deepseek-r1 edit_format: diff weak_model_name: fireworks_ai/accounts/fireworks/models/deepseek-v3 use_repo_map: true extra_params: max_tokens: 160000 use_temperature: false editor_model_name: fireworks_ai/accounts/fireworks/models/deepseek-v3 editor_edit_format: editor-diff remove_reasoning: think # &lt;--- . ",
    "url": "/docs/config/reasoning.html#thinking-tokens",
    
    "relUrl": "/docs/config/reasoning.html#thinking-tokens"
  },"282": {
    "doc": "Refactoring leaderboard",
    "title": "Refactoring leaderboard",
    "content": "Llmcode’s refactoring benchmark asks the LLM to refactor 89 large methods from large python classes. This is a more challenging benchmark, which tests the model’s ability to output long chunks of code without skipping sections or making mistakes. It was developed to provoke and measure GPT-4 Turbo’s “lazy coding” habit. The refactoring benchmark requires a large context window to work with large source files. Therefore, results are available for fewer models. | Model | Percent completed correctly | Percent using correct edit format | Command | Edit format | . | claude-3-5-sonnet-20241022 | 92.1% | 91.0% | llmcode --sonnet | diff | . | o1-preview | 75.3% | 57.3% | llmcode --model o1-preview | diff | . | claude-3-opus-20240229 | 72.3% | 79.5% | llmcode --opus | diff | . | claude-3.5-sonnet-20240620 | 64.0% | 76.4% | llmcode --sonnet | diff | . | gpt-4o | 62.9% | 53.9% | llmcode | diff | . | gpt-4-1106-preview | 50.6% | 39.3% | llmcode --model gpt-4-1106-preview | udiff | . | gpt-4o-2024-08-06 | 49.4% | 89.9% | llmcode --model openai/gpt-4o-2024-08-06 | diff | . | gemini/gemini-1.5-pro-latest | 49.4% | 7.9% | llmcode --model gemini/gemini-1.5-pro-latest | diff-fenced | . | o1-mini | 44.9% | 29.2% | llmcode --model o1-mini | diff | . | gpt-4-turbo-2024-04-09 (udiff) | 34.1% | 30.7% | llmcode --gpt-4-turbo | udiff | . | gpt-4-0125-preview | 33.7% | 47.2% | llmcode --model gpt-4-0125-preview | udiff | . | DeepSeek Coder V2 0724 (deprecated) | 32.6% | 59.6% | llmcode --model deepseek/deepseek-coder | diff | . | DeepSeek Chat V2.5 | 31.5% | 67.4% | llmcode --deepseek | diff | . | gpt-4-turbo-2024-04-09 (diff) | 21.4% | 6.8% | llmcode --model gpt-4-turbo-2024-04-09 | diff | . By Paul Gauthier, last updated January 16, 2025. ",
    "url": "/docs/leaderboards/refactor.html",
    
    "relUrl": "/docs/leaderboards/refactor.html"
  },"283": {
    "doc": "Replit",
    "title": "Replit",
    "content": "Replit . To use llmcode with pipx on replit, you can run these commands in the replit shell: . pip install pipx pipx run llmcode ...normal llmcode args... If you install llmcode with pipx on replit and try and run it as just llmcode it will crash with a missing libstdc++.so.6 library. ",
    "url": "/docs/install/replit.html",
    
    "relUrl": "/docs/install/replit.html"
  },"284": {
    "doc": "Repository map",
    "title": "Repository map",
    "content": ". Llmcode uses a concise map of your whole git repository that includes the most important classes and functions along with their types and call signatures. This helps llmcode understand the code it’s editing and how it relates to the other parts of the codebase. The repo map also helps llmcode write new code that respects and utilizes existing libraries, modules and abstractions found elsewhere in the codebase. ",
    "url": "/docs/repomap.html",
    
    "relUrl": "/docs/repomap.html"
  },"285": {
    "doc": "Repository map",
    "title": "Using a repo map to provide context",
    "content": "Llmcode sends a repo map to the LLM along with each change request from the user. The repo map contains a list of the files in the repo, along with the key symbols which are defined in each file. It shows how each of these symbols are defined, by including the critical lines of code for each definition. Here’s a part of the repo map of llmcode’s repo, for base_coder.py and commands.py : . llmcode/coders/base_coder.py: ⋮... │class Coder: │ abs_fnames = None ⋮... │ @classmethod │ def create( │ self, │ main_model, │ edit_format, │ io, │ skip_model_availabily_check=False, │ **kwargs, ⋮... │ def abs_root_path(self, path): ⋮... │ def run(self, with_message=None): ⋮... llmcode/commands.py: ⋮... │class Commands: │ voice = None │ ⋮... │ def get_commands(self): ⋮... │ def get_command_completions(self, cmd_name, partial): ⋮... │ def run(self, inp): ⋮... Mapping out the repo like this provides some key benefits: . | The LLM can see classes, methods and function signatures from everywhere in the repo. This alone may give it enough context to solve many tasks. For example, it can probably figure out how to use the API exported from a module just based on the details shown in the map. | If it needs to see more code, the LLM can use the map to figure out which files it needs to look at. The LLM can ask to see these specific files, and llmcode will offer to add them to the chat context. | . ",
    "url": "/docs/repomap.html#using-a-repo-map-to-provide-context",
    
    "relUrl": "/docs/repomap.html#using-a-repo-map-to-provide-context"
  },"286": {
    "doc": "Repository map",
    "title": "Optimizing the map",
    "content": "Of course, for large repositories even just the repo map might be too large for the LLM’s context window. Llmcode solves this problem by sending just the most relevant portions of the repo map. It does this by analyzing the full repo map using a graph ranking algorithm, computed on a graph where each source file is a node and edges connect files which have dependencies. Llmcode optimizes the repo map by selecting the most important parts of the codebase which will fit into the active token budget. The token budget is influenced by the --map-tokens switch, which defaults to 1k tokens. Llmcode adjusts the size of the repo map dynamically based on the state of the chat. It will usually stay within that setting’s value. But it does expand the repo map significantly at times, especially when no files have been added to the chat and llmcode needs to understand the entire repo as best as possible. The sample map shown above doesn’t contain every class, method and function from those files. It only includes the most important identifiers, the ones which are most often referenced by other portions of the code. These are the key pieces of context that the LLM needs to know to understand the overall codebase. ",
    "url": "/docs/repomap.html#optimizing-the-map",
    
    "relUrl": "/docs/repomap.html#optimizing-the-map"
  },"287": {
    "doc": "Repository map",
    "title": "More info",
    "content": "Please check the repo map article on llmcode’s blog for more information on llmcode’s repository map and how it is constructed. ",
    "url": "/docs/repomap.html#more-info",
    
    "relUrl": "/docs/repomap.html#more-info"
  },"288": {
    "doc": "Scripting llmcode",
    "title": "Scripting llmcode",
    "content": "You can script llmcode via the command line or python. ",
    "url": "/docs/scripting.html",
    
    "relUrl": "/docs/scripting.html"
  },"289": {
    "doc": "Scripting llmcode",
    "title": "Command line",
    "content": "Llmcode takes a --message argument, where you can give it a natural language instruction. It will do that one thing, apply the edits to the files and then exit. So you could do: . llmcode --message \"make a script that prints hello\" hello.js . Or you can write simple shell scripts to apply the same instruction to many files: . for FILE in *.py ; do llmcode --message \"add descriptive docstrings to all the functions\" $FILE done . Use llmcode --help to see all the command line options, but these are useful for scripting: . --stream, --no-stream Enable/disable streaming responses (default: True) [env var: LLMCODE_STREAM] --message COMMAND, --msg COMMAND, -m COMMAND Specify a single message to send GPT, process reply then exit (disables chat mode) [env var: LLMCODE] --message-file MESSAGE_FILE, -f MESSAGE_FILE Specify a file containing the message to send GPT, process reply, then exit (disables chat mode) [env var: LLMCODE_FILE] --yes Always say yes to every confirmation [env var: LLMCODE_YES] --auto-commits, --no-auto-commits Enable/disable auto commit of GPT changes (default: True) [env var: LLMCODE_AUTO_COMMITS] --dirty-commits, --no-dirty-commits Enable/disable commits when repo is found dirty (default: True) [env var: LLMCODE_DIRTY_COMMITS] --dry-run, --no-dry-run Perform a dry run without modifying files (default: False) [env var: LLMCODE_DRY_RUN] --commit Commit all pending changes with a suitable commit message, then exit [env var: LLMCODE_COMMIT] . ",
    "url": "/docs/scripting.html#command-line",
    
    "relUrl": "/docs/scripting.html#command-line"
  },"290": {
    "doc": "Scripting llmcode",
    "title": "Python",
    "content": "You can also script llmcode from python: . from llmcode.coders import Coder from llmcode.models import Model # This is a list of files to add to the chat fnames = [\"greeting.py\"] model = Model(\"gpt-4-turbo\") # Create a coder object coder = Coder.create(main_model=model, fnames=fnames) # This will execute one instruction on those files and then return coder.run(\"make a script that prints hello world\") # Send another instruction coder.run(\"make it say goodbye\") # You can run in-chat \"/\" commands too coder.run(\"/tokens\") . See the Coder.create() and Coder.init() methods for all the supported arguments. It can also be helpful to set the equivalent of --yes by doing this: . from llmcode.io import InputOutput io = InputOutput(yes=True) # ... coder = Coder.create(model=model, fnames=fnames, io=io) . The python scripting API is not officially supported or documented, and could change in future releases without providing backwards compatibility. ",
    "url": "/docs/scripting.html#python",
    
    "relUrl": "/docs/scripting.html#python"
  },"291": {
    "doc": "Semantic search &amp; replace code with llmcode",
    "title": "Semantic search &amp; replace code with llmcode",
    "content": "In this chat transcript, the user asks to modify a series of function calls. Notice that it isn’t replacing exact strings. Making the change requires dealing with various formatting and semantic differences in the calls that it is updating. Some edits were elided from this transcript for brevity. $ llmcode llmcode/coder.py Added llmcode/coder.py to the chat . replace all the self.console.print() calls that contain [red] with calls to self.io.tool_error() with [red] removed from the string . llmcode/coder.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL self.console.print( f\"[red]This tool will almost certainly fail to work with {main_model}\" ) ======= self.io.tool_error(f\"This tool will almost certainly fail to work with {main_model}\") &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . llmcode/coder.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL self.console.print(\"[red]Files are not in a git repo.\") ======= self.io.tool_error(\"Files are not in a git repo.\") &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . llmcode/coder.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL self.console.print(\"[red]Malformed ORIGINAL/UPDATE blocks, retrying...\") self.console.print(\"[red]\", Text(err)) ======= self.io.tool_error(\"Malformed ORIGINAL/UPDATE blocks, retrying...\") self.io.tool_error(Text(err)) &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . Please note that I couldn’t find the self.io.tool_error() method in the provided code. You may need to create this method in the appropriate class or provide the correct method name to replace the self.console.print() calls. Applied edit to llmcode/coder.py Applied edit to llmcode/coder.py Applied edit to llmcode/coder.py Commit 919cfc2 llmcode: Replaced all self.console.print() calls with self.io.tool_error() calls with [red] removed from the string. ",
    "url": "/examples/semantic-search-replace.html",
    
    "relUrl": "/examples/semantic-search-replace.html"
  },"292": {
    "doc": "Using /help",
    "title": "Using /help",
    "content": "Type /help &lt;question&gt; and llmcode will respond with helpful information. You can ask questions about using llmcode, customizing settings, troubleshooting, using LLMs, etc. Behind the scenes, llmcode has indexed all of its documentation. It uses the question to find relevant docs and share them with the LLM to do retrieval augmented generation (RAG) The LLM also has the full context of your ongoing llmcode chat, so it can help with the exact issue you might be facing. Llmcode v0.42.1-dev Models: claude-3-5-sonnet with diff edit format Repo-map: using 1024 tokens Use /help &lt;question&gt; to ask for help, run with –help to see cmd line args . /help is the .env file global or for a git repo? . Based on the provided documentation, here’s the answer to your question about the .env file in llmcode: . The .env file in llmcode can be both global and specific to a git repository. Llmcode looks for .env files in multiple locations, with the following priority order: . | Your home directory (global) | The root of your git repo (repo-specific) | The current directory | A file specified with the --env-file &lt;filename&gt; parameter | . If multiple .env files exist in these locations, they are loaded in the order listed above, with files loaded last taking priority. This means you can have a global .env file in your home directory with general settings, and then override or add to those settings with a repo-specific .env file in your git repository root. Here are some relevant documentation links that you might find helpful: . | https://llmcode.khulnasoft.com/docs/config.html | https://llmcode.khulnasoft.com/docs/config/dotenv.html | . The ability to use both global and repo-specific .env files allows for flexible configuration, where you can set default values globally and override them on a per-project basis as needed. /help can i use local LLMs? . Yes, you can use local LLMs with Llmcode. Here’s a summary of the options: . | Ollama: Llmcode supports using local models through Ollama. | OpenAI-compatible API: Llmcode can access local models that provide an OpenAI-compatible API. | Other local models: Llmcode uses the LiteLLM package to connect to various LLM providers, which may include options for running models locally. | . However, it’s important to note that Llmcode works best with more capable models. Less capable models may have difficulty properly returning “code edits,” which could result in Llmcode not being able to edit your files and commit changes effectively. For more detailed information on using local LLMs and connecting to various models, you might want to check out these relevant documentation pages: . | https://llmcode.khulnasoft.com/docs/llms.html | https://llmcode.khulnasoft.com/docs/llms/ollama.html | https://llmcode.khulnasoft.com/docs/llms/openai-compat.html | . Keep in mind that the performance and capabilities of local LLMs may vary depending on the specific model and your hardware resources. ",
    "url": "/docs/troubleshooting/support.html",
    
    "relUrl": "/docs/troubleshooting/support.html"
  },"293": {
    "doc": "Using /help",
    "title": "Getting more help",
    "content": "If you need more help, please check our GitHub issues and file a new issue if your problem isn’t discussed. Or drop into our Discord to chat with us. When reporting problems, it is very helpful if you can provide: . | Llmcode version | LLM model you are using | . Including the “announcement” lines that llmcode prints at startup is an easy way to share this helpful info. Llmcode v0.37.1-dev Models: gpt-4o with diff edit format, weak model gpt-3.5-turbo Git repo: .git with 243 files Repo-map: using 1024 tokens . Use /help &lt;question&gt; to ask for help about using llmcode, customizing settings, troubleshooting, using LLMs, etc. ",
    "url": "/docs/troubleshooting/support.html#getting-more-help",
    
    "relUrl": "/docs/troubleshooting/support.html#getting-more-help"
  },"294": {
    "doc": "Tips",
    "title": "Tips",
    "content": " ",
    "url": "/docs/usage/tips.html",
    
    "relUrl": "/docs/usage/tips.html"
  },"295": {
    "doc": "Tips",
    "title": "Just add the files that need to be changed to the chat",
    "content": "Take a moment and think about which files will need to be changed. Llmcode can often figure out which files to edit all by itself, but the most efficient approach is for you to add the files to the chat. ",
    "url": "/docs/usage/tips.html#just-add-the-files-that-need-to-be-changed-to-the-chat",
    
    "relUrl": "/docs/usage/tips.html#just-add-the-files-that-need-to-be-changed-to-the-chat"
  },"296": {
    "doc": "Tips",
    "title": "Don’t add lots of files to the chat",
    "content": "Just add the files you think need to be edited. Too much irrelevant code will distract and confuse the LLM. Llmcode uses a map of your entire git repo so is usually aware of relevant classes/functions/methods elsewhere in your code base. It’s ok to add 1-2 highly relevant files that don’t need to be edited, but be selective. ",
    "url": "/docs/usage/tips.html#dont-add-lots-of-files-to-the-chat",
    
    "relUrl": "/docs/usage/tips.html#dont-add-lots-of-files-to-the-chat"
  },"297": {
    "doc": "Tips",
    "title": "Break your goal down into bite sized steps",
    "content": "Do them one at a time. Adjust the files added to the chat as you go: /drop files that don’t need any more changes, /add files that need changes for the next step. ",
    "url": "/docs/usage/tips.html#break-your-goal-down-into-bite-sized-steps",
    
    "relUrl": "/docs/usage/tips.html#break-your-goal-down-into-bite-sized-steps"
  },"298": {
    "doc": "Tips",
    "title": "For complex changes, discuss a plan first",
    "content": "Use the /ask command to make a plan with llmcode. Once you are happy with the approach, just say “go ahead” without the /ask prefix. ",
    "url": "/docs/usage/tips.html#for-complex-changes-discuss-a-plan-first",
    
    "relUrl": "/docs/usage/tips.html#for-complex-changes-discuss-a-plan-first"
  },"299": {
    "doc": "Tips",
    "title": "If llmcode gets stuck",
    "content": ". | Use /clear to discard the chat history and make a fresh start. | Can you /drop any extra files? | Use /ask to discuss a plan before llmcode starts editing code. | Use the /model command to switch to a different model and try again. Switching between GPT-4o and Sonnet will often get past problems. | If llmcode is hopelessly stuck, just code the next step yourself and try having llmcode code some more after that. Take turns and pair program with llmcode. | . ",
    "url": "/docs/usage/tips.html#if-llmcode-gets-stuck",
    
    "relUrl": "/docs/usage/tips.html#if-llmcode-gets-stuck"
  },"300": {
    "doc": "Tips",
    "title": "Creating new files",
    "content": "If you want llmcode to create a new file, add it to the repository first with /add &lt;file&gt;. This way llmcode knows this file exists and will write to it. Otherwise, llmcode might write the changes to an existing file. This can happen even if you ask for a new file, as LLMs tend to focus a lot on the existing information in their contexts. ",
    "url": "/docs/usage/tips.html#creating-new-files",
    
    "relUrl": "/docs/usage/tips.html#creating-new-files"
  },"301": {
    "doc": "Tips",
    "title": "Fixing bugs and errors",
    "content": "If your code is throwing an error, use the /run command to share the error output with the llmcode. Or just paste the errors into the chat. Let the llmcode figure out how to fix the bug. If test are failing, use the /test command to run tests and share the error output with the llmcode. ",
    "url": "/docs/usage/tips.html#fixing-bugs-and-errors",
    
    "relUrl": "/docs/usage/tips.html#fixing-bugs-and-errors"
  },"302": {
    "doc": "Tips",
    "title": "Providing docs",
    "content": "LLMs know about a lot of standard tools and libraries, but may get some of the fine details wrong about API versions and function arguments. You can provide up-to-date documentation in a few ways: . | Paste doc snippets into the chat. | Include a URL to docs in your chat message and llmcode will scrape and read it. For example: Add a submit button like this https://ui.shadcn.com/docs/components/button. | Use the /read command to read doc files into the chat from anywhere on your filesystem. | If you have coding conventions or standing instructions you want llmcode to follow, consider using a conventions file. | . ",
    "url": "/docs/usage/tips.html#providing-docs",
    
    "relUrl": "/docs/usage/tips.html#providing-docs"
  },"303": {
    "doc": "Tips",
    "title": "Interrupting &amp; inputting",
    "content": "Use Control-C to interrupt llmcode if it isn’t providing a useful response. The partial response remains in the conversation, so you can refer to it when you reply with more information or direction. You can send long, multi-line messages in the chat in a few ways: . | Paste a multi-line message directly into the chat. | Enter { alone on the first line to start a multiline message and } alone on the last line to end it. | Or, start with {tag (where “tag” is any sequence of letters/numbers) and end with tag}. This is useful when you need to include closing braces } in your message. | . | Use Meta-ENTER to start a new line without sending the message (Esc+ENTER in some environments). | Use /paste to paste text from the clipboard into the chat. | Use the /editor command to open your editor to create the next chat message. See editor configuration docs for more info. | Use multiline-mode, which swaps the function of Meta-Enter and Enter, so that Enter inserts a newline, and Meta-Enter submits your command. To enable multiline mode: . | Use the /multiline-mode command to toggle it during a session. | Use the --multiline switch. | . | . Example with a tag: . {python def hello(): print(\"Hello}\") # Note: contains a brace python} . People often ask for SHIFT-ENTER to be a soft-newline. Unfortunately there is no portable way to detect that keystroke in terminals. ",
    "url": "/docs/usage/tips.html#interrupting--inputting",
    
    "relUrl": "/docs/usage/tips.html#interrupting--inputting"
  },"304": {
    "doc": "Token limits",
    "title": "Token limits",
    "content": "Every LLM has limits on how many tokens it can process for each request: . | The model’s context window limits how many total tokens of input and output it can process. | Each model has limit on how many output tokens it can produce. | . Llmcode will report an error if a model responds indicating that it has exceeded a token limit. The error will include suggested actions to try and avoid hitting token limits. Here’s an example error: . Model gpt-3.5-turbo has hit a token limit! Input tokens: 768 of 16385 Output tokens: 4096 of 4096 -- exceeded output limit! Total tokens: 4864 of 16385 To reduce output tokens: - Ask for smaller changes in each request. - Break your code into smaller source files. - Try using a stronger model like DeepSeek V3 or Sonnet that can return diffs. For more info: https://llmcode.khulnasoft.com/docs/token-limits.html . Llmcode never enforces token limits, it only reports token limit errors from the API provider. The token counts that llmcode reports are estimates. ",
    "url": "/docs/troubleshooting/token-limits.html",
    
    "relUrl": "/docs/troubleshooting/token-limits.html"
  },"305": {
    "doc": "Token limits",
    "title": "Input tokens &amp; context window size",
    "content": "The most common problem is trying to send too much data to a model, overflowing its context window. Technically you can exhaust the context window if the input is too large or if the input plus output are too large. Strong models like GPT-4o and Sonnet have quite large context windows, so this sort of error is typically only an issue when working with weaker models. The easiest solution is to try and reduce the input tokens by removing files from the chat. It’s best to only add the files that llmcode will need to edit to complete your request. | Use /tokens to see token usage. | Use /drop to remove unneeded files from the chat session. | Use /clear to clear the chat history. | Break your code into smaller source files. | . ",
    "url": "/docs/troubleshooting/token-limits.html#input-tokens--context-window-size",
    
    "relUrl": "/docs/troubleshooting/token-limits.html#input-tokens--context-window-size"
  },"306": {
    "doc": "Token limits",
    "title": "Output token limits",
    "content": "Most models have quite small output limits, often as low as 4k tokens. If you ask llmcode to make a large change that affects a lot of code, the LLM may hit output token limits as it tries to send back all the changes. To avoid hitting output token limits: . | Ask for smaller changes in each request. | Break your code into smaller source files. | Use a strong model like gpt-4o, sonnet or DeepSeek V3 that can return diffs. | Use a model that supports infinite output. | . ",
    "url": "/docs/troubleshooting/token-limits.html#output-token-limits",
    
    "relUrl": "/docs/troubleshooting/token-limits.html#output-token-limits"
  },"307": {
    "doc": "Token limits",
    "title": "Other causes",
    "content": "Sometimes token limit errors are caused by non-compliant API proxy servers or bugs in the API server you are using to host a local model. Llmcode has been well tested when directly connecting to major LLM provider cloud APIs. For serving local models, Ollama is known to work well with llmcode. Try using llmcode without an API proxy server or directly with one of the recommended cloud APIs and see if your token limit problems resolve. ",
    "url": "/docs/troubleshooting/token-limits.html#other-causes",
    
    "relUrl": "/docs/troubleshooting/token-limits.html#other-causes"
  },"308": {
    "doc": "Token limits",
    "title": "More help",
    "content": "If you need more help, please check our GitHub issues and file a new issue if your problem isn’t discussed. Or drop into our Discord to chat with us. When reporting problems, it is very helpful if you can provide: . | Llmcode version | LLM model you are using | . Including the “announcement” lines that llmcode prints at startup is an easy way to share this helpful info. Llmcode v0.37.1-dev Models: gpt-4o with diff edit format, weak model gpt-3.5-turbo Git repo: .git with 243 files Repo-map: using 1024 tokens . Use /help &lt;question&gt; to ask for help about using llmcode, customizing settings, troubleshooting, using LLMs, etc. ",
    "url": "/docs/troubleshooting/token-limits.html#more-help",
    
    "relUrl": "/docs/troubleshooting/token-limits.html#more-help"
  },"309": {
    "doc": "Troubleshooting",
    "title": "Troubleshooting",
    "content": "Below are some approaches for troubleshooting problems with llmcode. If you need more help, please check our GitHub issues and file a new issue if your problem isn’t discussed. Or drop into our Discord to chat with us. When reporting problems, it is very helpful if you can provide: . | Llmcode version | LLM model you are using | . Including the “announcement” lines that llmcode prints at startup is an easy way to share this helpful info. Llmcode v0.37.1-dev Models: gpt-4o with diff edit format, weak model gpt-3.5-turbo Git repo: .git with 243 files Repo-map: using 1024 tokens . Use /help &lt;question&gt; to ask for help about using llmcode, customizing settings, troubleshooting, using LLMs, etc. ",
    "url": "/docs/troubleshooting.html",
    
    "relUrl": "/docs/troubleshooting.html"
  },"310": {
    "doc": "Tutorial videos",
    "title": "Tutorial videos",
    "content": "Here are some tutorial videos made by llmcode users: . | Using Architect/Editor mode – AICodeKing | Using llmcode to incrementally build a non-trivial app – IndyDevDan | Llmcode and Replit on mobile with your voice – Paul Couvert | Llmcode is the OG AI Coding King (Mermaid Diagram AI Agent) – IndyDevDan | Installing llmcode in replit and making a Trello clone – Paul Couvert | Step-by-Step Development Environment Setup for AI-Assisted Coding – Coding the Future With AI | Generate FULL-STACK Apps with Claude 3.5 Sonnet – AICodeKing | Creating Games with AI from Start-To-End – AICodeKing | Claude 3.5 and llmcode: Use AI Assistants to Build AI Apps – Coding the Future With AI | Develop a Full-stack App Without Writing ANY Code – WorldofAI | Generate Games with AI (w/ Local LLMs) – AICodeKing | Llmcode tips and Example use – techfren | Llmcode and Claude 3.5: Develop a Full-stack App Without Writing ANY Code! – Coding the Future With AI | Generate application with just one prompt using Llmcode – AICodeKing | Llmcode : the production ready AI coding assistant you’ve been waiting for – Learn Code With JV | Holy Grail: FREE Coding Assistant That Can Build From EXISTING CODE BASE – Matthew Berman | Llmcode: This AI Coder Can Create AND Update Git Codebases – Ian Wootten | . Thanks to all these great creators for taking the time to share their experiences coding with llmcode! . ",
    "url": "/docs/usage/tutorials.html",
    
    "relUrl": "/docs/usage/tutorials.html"
  },"311": {
    "doc": "Unified diffs make GPT-4 Turbo 3X less lazy",
    "title": "Unified diffs make GPT-4 Turbo 3X less lazy",
    "content": ". Llmcode now asks GPT-4 Turbo to use unified diffs to edit your code. This dramatically improves GPT-4 Turbo’s performance on a challenging new benchmark and significantly reduces its bad habit of “lazy” coding, where it writes code with comments like “…add logic here…”. Llmcode’s new “laziness” benchmark suite is designed to both provoke and quantify lazy coding. It consists of 89 python refactoring tasks which tend to make GPT-4 Turbo write lazy comments like “…include original method body…”. This new laziness benchmark produced the following results with gpt-4-1106-preview: . | GPT-4 Turbo only scored 20% as a baseline using llmcodede’s existing “SEARCH/REPLACE block” edit format. It outputs “lazy comments” on 12 of the tasks. | Llmcode’s new unified diff edit format raised the score to 61%. Using this format reduced laziness by 3X, with GPT-4 Turbo only using lazy comments on 4 of the tasks. | It’s worse to add a prompt that says the user is blind, has no hands, will tip $2000 and fears truncated code trauma. Widely circulated “emotional appeal” folk remedies produced worse benchmark scores for both the baseline SEARCH/REPLACE and new unified diff editing formats. | . The older gpt-4-0613 also did better on the laziness benchmark using unified diffs: . | The June GPT-4’s baseline was 26% using llmcodede’s existing “SEARCH/REPLACE block” edit format. | Llmcode’s new unified diff edit format raised June GPT-4’s score to 59%. | The benchmark was designed to use large files, and 28% of them are too large to fit in June GPT-4’s 8k context window. This puts a hard ceiling of 72% on how well the June model could possibly score. | . With unified diffs, GPT acts more like it’s writing textual data intended to be read by a program, not talking to a person. Diffs are usually consumed by the patch program, which is fairly rigid. This seems to encourage rigor, making GPT less likely to leave informal editing instructions in comments or be lazy about writing all the needed code. Llmcode’s new unified diff editing format outperforms other solutions I evaluated by a wide margin. I explored many other approaches including: prompts about being tireless and diligent, OpenAI’s function/tool calling capabilities, numerous variations on llmcodede’s existing editing formats, line number based formats and other diff-like formats. The results shared here reflect an extensive investigation and benchmark evaluations of many approaches. The rest of this article will describe llmcodede’s new editing format and refactoring benchmark. It will highlight some key design decisions, and evaluate their significance using ablation experiments. ",
    "url": "/docs/unified-diffs.html#unified-diffs-make-gpt-4-turbo-3x-less-lazy",
    
    "relUrl": "/docs/unified-diffs.html#unified-diffs-make-gpt-4-turbo-3x-less-lazy"
  },"312": {
    "doc": "Unified diffs make GPT-4 Turbo 3X less lazy",
    "title": "Unified diff editing format",
    "content": "The design and implementation of llmcodede’s new unified diff editing format helped clarify some general principles for GPT-4 code editing: . | FAMILIAR - Choose an edit format that GPT is already familiar with. | SIMPLE - Choose a simple format that avoids escaping, syntactic overhead and brittle specifiers like line numbers or line counts. | HIGH LEVEL - Encourage GPT to structure edits as new versions of substantive code blocks (functions, methods, etc), not as a series of surgical/minimal changes to individual lines of code. | FLEXIBLE - Strive to be maximally flexible when interpreting GPT’s edit instructions. | . A helpful shortcut here is to have empathy for GPT, and imagine you are the one being asked to specify code edits. Would you want to hand type a properly escaped json data structure to invoke surgical insert, delete, replace operations on specific code line numbers? Do you want to use a brittle format, where any mistake causes an error that discards all your work? . GPT is quantitatively better at code editing when you reduce the burden of formatting edits by using a familiar, simple, high level and flexible editing format. Choose a familiar editing format . Unified diffs are perhaps the most common way to show code edits, because it’s the default output format of git diff: . --- a/greeting.py +++ b/greeting.py @@ -1,5 +1,5 @@ def main(args): # show a greeting - print(\"Hello!\") + print(\"Goodbye!\") return . Choosing such a popular format means that GPT has seen many examples in its training data. It’s been trained to generate text that conforms to the unified diff syntax. Use a simple editing format . Llmcode’s previous benchmark results made it clear that simple editing formats work best. Even though OpenAI provides extensive support for structured formats like json and function calls, GPT is worse at editing code if you use them. I repeated these and other similar benchmarks against GPT-4 Turbo, and again reached these same conclusions. Informally, this is probably because stuffing source code into JSON is complicated and error prone. Wrapping the python code print(\"On Windows use \\\"C:\\\\\\\"\") as valid json is pretty painful and error prone. Due to escaping issues GPT’s code is often syntactically incorrect when it’s unpacked from JSON, or the JSON decode just fails entirely. On the other hand, the core of the unified diff format is very simple. You include a hunk of the file that needs to be changed, with every line prefixed by a character to indicate unchanged, new or deleted lines. A unified diff looks pretty much like the code it is modifying. The one complicated piece is the line numbers found at the start of each hunk. They look something like this: @@ -2,4 +3,5 @@. GPT is terrible at working with source code line numbers. This is a general observation about any use of line numbers in editing formats, backed up by many quantitative benchmark experiments. You’ve probably ignored the line numbers in every diff you’ve seen, because the diffs usually still make sense without them. Llmcode tells GPT not to include line numbers, and just interprets each hunk from the unified diffs as a search and replace operation: . This diff: . @@ ... @@ def main(args): # show a greeting - print(\"Hello!\") + print(\"Goodbye!\") return . Means we need to search the file for the space and minus - lines: . def main(args): # show a greeting print(\"Hello!\") return . And replace them with the space and plus + lines: . def main(args): # show a greeting print(\"Goodbye!\") return . Simple, right? . Encourage high level edits . The example unified diffs we’ve seen so far have all been single line changes, which makes them pretty easy to read and understand. Consider this slightly more complex change, which renames the variable n to number: . @@ ... @@ -def factorial(n): +def factorial(number): - if n == 0: + if number == 0: return 1 else: - return n * factorial(n-1) + return number * factorial(number-1) . The following “high level diff” of the same change is not as succinct as the minimal diff above, but it is much easier to see two different coherent versions of the factorial() function. @@ ... @@ -def factorial(n): - if n == 0: - return 1 - else: - return n * factorial(n-1) +def factorial(number): + if number == 0: + return 1 + else: + return number * factorial(number-1) . Llmcode’s system prompt encourages GPT to produce these high level diffs. This makes GPT better at producing correct diffs, which can be successfully applied to the original file. Experiments without “high level diff” prompting produce a 30-50% increase in editing errors, where diffs fail to apply or apply incorrectly and produce invalid code. When a patch fails, llmcodede needs to ask GPT for a corrected version of the diff. This takes time, costs tokens and sometimes fails to produce a successful edit even after multiple retries. There are probably a couple of reasons why high level diffs help: . | It’s easier to produce diffs that both correctly match the original code and correctly produce the intended new code. There is less risk of GPT getting confused, compared to generating a series of surgical edits that interleave lines of old and new code. | High level hunks often contain more lines than a surgical hunk, so they are less likely to accidentally match unrelated parts of the code. This is helpful because GPT can’t reliably give us line numbers to specify exactly where in the file to make changes. | . Be flexible when applying edits . GPT frequently makes imperfect diffs that won’t apply cleanly. They exhibit a variety of problems: . | GPT forgets things like comments, docstrings, blank lines, etc. Or it skips over some code that it doesn’t intend to change. | GPT forgets the leading plus + character to mark novel lines that it wants to add to the file. It incorrectly includes them with a leading space as if they were already there. | GPT outdents all of the code, removing all the leading white space which is shared across the lines. So a chunk of deeply indented code is shown in a diff with only the leading white space that changes between the lines in the chunk. | GPT jumps ahead to show edits to a different part of the file without starting a new hunk with a @@ ... @@ divider. | . As an example of the first issue, consider this source code: . import sys def main(args): # show a greeting print(\"Hello!\") return main(sys.argv[1:]) . The diff below is missing the “show a greeting” comment line, and represents a common type of mistake GPT might make. When we search for the minus - lines, we won’t find them in the original file because of the missing comment. @@ ... @@ -def main(args): - print(\"Hello!\") - return +def main(args): + print(\"Goodbye!\") + return . Llmcode tries to be very flexible when applying diffs, in order to handle defects. If a hunk doesn’t apply cleanly, llmcodede uses a number of strategies: . | Normalize the hunk, by taking the minus - and space lines as one version of the hunk and the space and plus + lines as a second version and doing an actual unified diff on them. | Try and discover new lines that GPT is trying to add but which it forgot to mark with plus + markers. This is done by diffing the minus - and space lines back against the original file. | Try and apply the hunk using “relative leading white space”, so we can match and patch correctly even if the hunk has been uniformly indented or outdented. | Break a large hunk apart into an overlapping sequence of smaller hunks, which each contain only one contiguous run of plus + and minus - lines. Try and apply each of these sub-hunks independently. | Vary the size and offset of the “context window” of space lines from the hunk that are used to localize the edit to a specific part of the file. | Combine the above mechanisms to progressively become more permissive about how to apply the hunk. | . These flexible patching strategies are critical, and removing them radically increases the number of hunks which fail to apply. Experiments where flexible patching is disabled show a 9X increase in editing errors on llmcodede’s original Exercism benchmark. ",
    "url": "/docs/unified-diffs.html#unified-diff-editing-format",
    
    "relUrl": "/docs/unified-diffs.html#unified-diff-editing-format"
  },"313": {
    "doc": "Unified diffs make GPT-4 Turbo 3X less lazy",
    "title": "Refactoring benchmark",
    "content": "Llmcode has long used a benchmark suite based on 133 Exercism python exercises. But these are mostly small coding problems, usually requiring only a few dozen lines of code. GPT-4 Turbo is typically only lazy on 2-3 of these exercises: the ones with the most code and which involve refactoring. Based on this observation, I set out to build a benchmark based on refactoring a non-trivial amount of code found in fairly large files. To do this, I used python’s ast module to analyze 9 popular open source python repositories to identify challenging refactoring tasks. The goal was to find: . | Source files that contain classes with non-trivial methods, having 100-250+ AST nodes in their implementation. | Focus on methods that are part of a larger class, which has at least twice as much code as the method itself. | Select methods that don’t use their self parameter, so they can be trivially refactored out of the class. | . We can then turn each of these source files into a task for the benchmark, where we ask GPT to do something like: . Refactor the _set_csrf_cookie method in the CsrfViewMiddleware class to be a stand alone, top level function. Name the new function _set_csrf_cookie, exactly the same name as the existing method. Update any existing self._set_csrf_cookie calls to work with the new _set_csrf_cookie function. A simple python AST scanning script found 89 suitable files and packaged them up as benchmark tasks. Each task has a test that checks if the refactor was performed roughly correctly: . | The updated source file must parse as valid python, to detect misapplied edits which produce invalid code. | The target method must now exist as a top-level function in the file. | This new top-level function must contain approximately the same number of AST nodes as the original class method. This ensures that GPT didn’t elide code and replace it with comments. | The original class must still be present in the file, and it must be smaller by about the number of AST nodes in the method which was removed. This helps confirm that the method was removed from the class, without other significant modifications. | . To be clear, this is not a rigorous test that the refactor was performed correctly. But it does serve as a basic sanity check that the refactor was essentially done as a cut &amp; paste, without eliding any code as comments. And it correlates well with other laziness metrics gathered during benchmarking like the introduction of new comments that contain “…”. The result is a pragmatic benchmark suite that provokes, detects and quantifies GPT coding laziness. ",
    "url": "/docs/unified-diffs.html#refactoring-benchmark",
    
    "relUrl": "/docs/unified-diffs.html#refactoring-benchmark"
  },"314": {
    "doc": "Unified diffs make GPT-4 Turbo 3X less lazy",
    "title": "Conclusions and future work",
    "content": "Based on the refactor benchmark results, llmcodede’s new unified diff format seems to dramatically increase GPT-4 Turbo’s skill at more complex coding tasks. It also seems very effective at reducing the lazy coding which has been widely noted as a problem with GPT-4 Turbo. Unified diffs was one of the very first edit formats I tried when originally building llmcodede. I think a lot of other AI coding assistant projects have also tried going down this path. It seems like any naive or direct use of structured diff formats is pretty much doomed to failure. But the techniques described here and incorporated into llmcodede provide a highly effective way to harness GPT’s knowledge of unified diffs. There could be significant benefits to fine tuning models on llmcodede’s simple, high level style of unified diffs. Dropping line numbers from the hunk headers and focusing on diffs of semantically coherent chunks of code seems to be an important part of successful GPT code editing (besides the relentless focus on flexibly applying edits). Most LLMs will have already seen plenty of unified diffs in their normal training data, and so should be amenable to fining tuning towards this particular diff style. ",
    "url": "/docs/unified-diffs.html#conclusions-and-future-work",
    
    "relUrl": "/docs/unified-diffs.html#conclusions-and-future-work"
  },"315": {
    "doc": "Unified diffs make GPT-4 Turbo 3X less lazy",
    "title": "Unified diffs make GPT-4 Turbo 3X less lazy",
    "content": " ",
    "url": "/docs/unified-diffs.html",
    
    "relUrl": "/docs/unified-diffs.html"
  },"316": {
    "doc": "Automatically update docs with llmcode",
    "title": "Automatically update docs with llmcode",
    "content": "In this chat transcript, the user asks to automatically update the Usage docs based on the latest version of the main() function in the code. $ llmcode ./README.md llmcode/main.py Added README.md to the chat Added llmcode/main.py to the chat . the cmd line args in main() have been updated. update the readme to accurately describe them. README.md &lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL - `--history-file HISTORY_FILE`: Specify the chat input history file (default: .llmcode.history). Override the default with the environment variable `LLMCODE_HISTORY_FILE`. - `--model MODEL`: Specify the model to use for the main chat (default: gpt-4). Override the default with the environment variable `LLMCODE_MODEL`. - `-3`: Use gpt-3.5-turbo model for the main chat (basically won't work). No environment variable override. - `--no-pretty`: Disable pretty, colorized output. Override the default with the environment variable `LLMCODE_PRETTY` (default: 1 for enabled, 0 for disabled). - `--show-diffs`: Show diffs when committing changes (default: False). Override the default with the environment variable `LLMCODE_SHOW_DIFFS` (default: 0 for False, 1 for True). - `--no-auto-commits`: Disable auto commit of changes. Override the default with the environment variable `LLMCODE_AUTO_COMMITS` (default: 1 for enabled, 0 for disabled). ======= - `--input-history-file INPUT_HISTORY_FILE`: Specify the chat input history file (default: .llmcode.input.history). Override the default with the environment variable `LLMCODE_INPUT_HISTORY_FILE`. - `--chat-history-file CHAT_HISTORY_FILE`: Specify the chat history file (default: .llmcode.khulnasoft.com.history.md). Override the default with the environment variable `LLMCODE_CHAT_HISTORY_FILE`. - `--model MODEL`: Specify the model to use for the main chat (default: gpt-4). Override the default with the environment variable `LLMCODE_MODEL`. - `-3`: Use gpt-3.5-turbo model for the main chat (basically won't work). No environment variable override. - `--no-pretty`: Disable pretty, colorized output. Override the default with the environment variable `LLMCODE_PRETTY` (default: 1 for enabled, 0 for disabled). - `--apply FILE`: Apply the changes from the given file instead of running the chat (debug). - `--no-auto-commits`: Disable auto commit of changes. Override the default with the environment variable `LLMCODE_AUTO_COMMITS` (default: 1 for enabled, 0 for disabled). - `--show-diffs`: Show diffs when committing changes (default: False). Override the default with the environment variable `LLMCODE_SHOW_DIFFS` (default: 0 for False, 1 for True). - `--yes`: Always say yes to every confirmation (default: False). &gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED . Applied edit to README.md Commit 024f45e llmcode: Updated command line arguments in main() and updated README.md to accurately describe them. ",
    "url": "/examples/update-docs.html",
    
    "relUrl": "/examples/update-docs.html"
  },"317": {
    "doc": "Usage",
    "title": "Usage",
    "content": "Run llmcode with the source code files you want to edit. These files will be “added to the chat session”, so that llmcode can see their contents and edit them for you. They can be existing files or the name of files you want llmcode to create for you. llmcode &lt;file1&gt; &lt;file2&gt; ... At the llmcode &gt; prompt, ask for code changes and llmcode will edit those files to accomplish your request. $ llmcode factorial.py Llmcode v0.37.1-dev Models: gpt-4o with diff edit format, weak model gpt-3.5-turbo Git repo: .git with 258 files Repo-map: using 1024 tokens Use /help to see in-chat commands, run with --help to see cmd line args ─────────────────────────────────────────────────────────────────────── &gt; Make a program that asks for a number and prints its factorial ... Use /help &lt;question&gt; to ask for help about using llmcode, customizing settings, troubleshooting, using LLMs, etc. ",
    "url": "/docs/usage.html",
    
    "relUrl": "/docs/usage.html"
  },"318": {
    "doc": "Usage",
    "title": "Adding files",
    "content": "To edit files, you need to “add them to the chat”. Do this by naming them on the llmcode command line. Or, you can use the in-chat /add command to add files. Only add the files that need to be edited for your task. Don’t add a bunch of extra files. If you add too many files, the LLM can get overwhelmed and confused (and it costs more tokens). Llmcode will automatically pull in content from related files so that it can understand the rest of your code base. You can use llmcode without adding any files, and it will try to figure out which files need to be edited based on your requests. You’ll get the best results if you think about which files need to be edited. Add just those files to the chat. Llmcode will include relevant context from the rest of your repo. ",
    "url": "/docs/usage.html#adding-files",
    
    "relUrl": "/docs/usage.html#adding-files"
  },"319": {
    "doc": "Usage",
    "title": "LLMs",
    "content": "Llmcode works best with Claude 3.5 Sonnet, DeepSeek R1 &amp; Chat V3, OpenAI o1, o3-mini &amp; GPT-4o. Llmcode can connect to almost any LLM, including local models. # o3-mini $ llmcode --model o3-mini --api-key openai=&lt;key&gt; # Claude 3.5 Sonnet $ llmcode --model sonnet --api-key anthropic=&lt;key&gt; . Or you can run llmcode --model XXX to launch llmcode with another model. During your chat you can switch models with the in-chat /model command. ",
    "url": "/docs/usage.html#llms",
    
    "relUrl": "/docs/usage.html#llms"
  },"320": {
    "doc": "Usage",
    "title": "Making changes",
    "content": "Ask llmcode to make changes to your code. It will show you some diffs of the changes it is making to complete you request. Llmcode will git commit all of its changes, so they are easy to track and undo. You can always use the /undo command to undo AI changes that you don’t like. ",
    "url": "/docs/usage.html#making-changes",
    
    "relUrl": "/docs/usage.html#making-changes"
  },"321": {
    "doc": "Vertex AI",
    "title": "Vertex AI",
    "content": "Llmcode can connect to models provided by Google Vertex AI. You will need to install the gcloud CLI and login with a GCP account or service account with permission to use the Vertex AI API. With your chosen login method, the gcloud CLI should automatically set the GOOGLE_APPLICATION_CREDENTIALS environment variable which points to the credentials file. To configure Llmcode to use the Vertex AI API, you need to set VERTEXAI_PROJECT (the GCP project ID) and VERTEXAI_LOCATION (the GCP region) environment variables for Llmcode. Note that Claude on Vertex AI is only available in certain GCP regions, check the model card for your model to see which regions are supported. Example .env file: . VERTEXAI_PROJECT=my-project VERTEXAI_LOCATION=us-east5 . Then you can run llmcode with the --model command line switch, like this: . llmcode --model vertex_ai/claude-3-5-sonnet@20240620 . Or you can use the yaml config to set the model to any of the models supported by Vertex AI. Example .llmcode.conf.yml file: . model: vertex_ai/claude-3-5-sonnet@20240620 . ",
    "url": "/docs/llms/vertex.html",
    
    "relUrl": "/docs/llms/vertex.html"
  },"322": {
    "doc": "Voice-to-code with llmcode",
    "title": "Voice-to-code with llmcode",
    "content": "Speak with llmcode about your code! Request new features, test cases or bug fixes using your voice and let llmcode do the work of editing the files in your local git repo. As with all of llmcode’s capabilities, you can use voice-to-code with an existing repo or to start a new project. Voice support fits quite naturally into llmcode’s AI pair programming chat interface. Now you can fluidly switch between voice and text chat when you ask llmcode to edit your code. ",
    "url": "/docs/usage/voice.html",
    
    "relUrl": "/docs/usage/voice.html"
  },"323": {
    "doc": "Voice-to-code with llmcode",
    "title": "How to use voice-to-code",
    "content": "Use the in-chat /voice command to start recording, and press ENTER when you’re done speaking. Your voice coding instructions will be transcribed, as if you had typed them into the llmcode chat session. See the installation instructions for information on how to enable the /voice command. Llmcode v0.11.2-dev Added app.py to the chat. /voice . Recording, press ENTER when done... 3.5sec . “&nbsp;add a factorial endpoint that uses math factorial&nbsp;” Add a factorial endpoint that uses math.factorial. To add a factorial endpoint to the Flask application, we need to modify the app.py file. Here are the steps: . | Add a new route /fact/&lt;int:x&gt; to the Flask application. | In the function for this route, call math.factorial(x) and return the result as a string. | . Here is the edit block for these changes: . app.py &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD if __name__ == '__main__': print(\"Starting...\") app.run() ======= @app.route('/fact/&lt;int:x&gt;') def factorial(x): result = math.factorial(x) return str(result) if __name__ == '__main__': print(\"Starting...\") app.run() &gt;&gt;&gt;&gt;&gt;&gt;&gt; updated . Applied edit to app.py Commit ef9e3e7 llmcode: Add a factorial endpoint that uses math.factorial. ",
    "url": "/docs/usage/voice.html#how-to-use-voice-to-code",
    
    "relUrl": "/docs/usage/voice.html#how-to-use-voice-to-code"
  },"324": {
    "doc": "Model warnings",
    "title": "Model warnings",
    "content": " ",
    "url": "/docs/llms/warnings.html",
    
    "relUrl": "/docs/llms/warnings.html"
  },"325": {
    "doc": "Model warnings",
    "title": "Unknown context window size and token costs",
    "content": "Model foobar: Unknown context window size and costs, using sane defaults. If you specify a model that llmcode has never heard of, you will get this warning. This means llmcode doesn’t know the context window size and token costs for that model. Llmcode will use an unlimited context window and assume the model is free, so this is not usually a significant problem. See the docs on configuring advanced model settings for details on how to remove this warning. You can probably ignore the unknown context window size and token costs warning. ",
    "url": "/docs/llms/warnings.html#unknown-context-window-size-and-token-costs",
    
    "relUrl": "/docs/llms/warnings.html#unknown-context-window-size-and-token-costs"
  },"326": {
    "doc": "Model warnings",
    "title": "Did you mean?",
    "content": "If llmcode isn’t familiar with the model you’ve specified, it will suggest similarly named models. This helps in the case where you made a typo or mistake when specifying the model name. Model gpt-5o: Unknown context window size and costs, using sane defaults. Did you mean one of these? - gpt-4o . ",
    "url": "/docs/llms/warnings.html#did-you-mean",
    
    "relUrl": "/docs/llms/warnings.html#did-you-mean"
  },"327": {
    "doc": "Model warnings",
    "title": "Missing environment variables",
    "content": "You need to set the listed environment variables. Otherwise you will get error messages when you start chatting with the model. Model azure/gpt-4-turbo: Missing these environment variables: - AZURE_API_BASE - AZURE_API_VERSION - AZURE_API_KEY . On Windows, if you just set these environment variables using setx you may need to restart your terminal or command prompt for the changes to take effect. ",
    "url": "/docs/llms/warnings.html#missing-environment-variables",
    
    "relUrl": "/docs/llms/warnings.html#missing-environment-variables"
  },"328": {
    "doc": "Model warnings",
    "title": "Unknown which environment variables are required",
    "content": "Model gpt-5: Unknown which environment variables are required. Llmcode is unable verify the environment because it doesn’t know which variables are required for the model. If required variables are missing, you may get errors when you attempt to chat with the model. You can look in the llmcode’s LLM documentation or the litellm documentation to see if the required variables are listed there. ",
    "url": "/docs/llms/warnings.html#unknown-which-environment-variables-are-required",
    
    "relUrl": "/docs/llms/warnings.html#unknown-which-environment-variables-are-required"
  },"329": {
    "doc": "Model warnings",
    "title": "Model warnings",
    "content": " ",
    "url": "/docs/troubleshooting/warnings.html",
    
    "relUrl": "/docs/troubleshooting/warnings.html"
  },"330": {
    "doc": "Model warnings",
    "title": "Unknown context window size and token costs",
    "content": "Model foobar: Unknown context window size and costs, using sane defaults. If you specify a model that llmcode has never heard of, you will get this warning. This means llmcode doesn’t know the context window size and token costs for that model. Llmcode will use an unlimited context window and assume the model is free, so this is not usually a significant problem. See the docs on configuring advanced model settings for details on how to remove this warning. You can probably ignore the unknown context window size and token costs warning. ",
    "url": "/docs/troubleshooting/warnings.html#unknown-context-window-size-and-token-costs",
    
    "relUrl": "/docs/troubleshooting/warnings.html#unknown-context-window-size-and-token-costs"
  },"331": {
    "doc": "Model warnings",
    "title": "Did you mean?",
    "content": "If llmcode isn’t familiar with the model you’ve specified, it will suggest similarly named models. This helps in the case where you made a typo or mistake when specifying the model name. Model gpt-5o: Unknown context window size and costs, using sane defaults. Did you mean one of these? - gpt-4o . ",
    "url": "/docs/troubleshooting/warnings.html#did-you-mean",
    
    "relUrl": "/docs/troubleshooting/warnings.html#did-you-mean"
  },"332": {
    "doc": "Model warnings",
    "title": "Missing environment variables",
    "content": "You need to set the listed environment variables. Otherwise you will get error messages when you start chatting with the model. Model azure/gpt-4-turbo: Missing these environment variables: - AZURE_API_BASE - AZURE_API_VERSION - AZURE_API_KEY . On Windows, if you just set these environment variables using setx you may need to restart your terminal or command prompt for the changes to take effect. ",
    "url": "/docs/troubleshooting/warnings.html#missing-environment-variables",
    
    "relUrl": "/docs/troubleshooting/warnings.html#missing-environment-variables"
  },"333": {
    "doc": "Model warnings",
    "title": "Unknown which environment variables are required",
    "content": "Model gpt-5: Unknown which environment variables are required. Llmcode is unable verify the environment because it doesn’t know which variables are required for the model. If required variables are missing, you may get errors when you attempt to chat with the model. You can look in the llmcode’s LLM documentation or the litellm documentation to see if the required variables are listed there. ",
    "url": "/docs/troubleshooting/warnings.html#unknown-which-environment-variables-are-required",
    
    "relUrl": "/docs/troubleshooting/warnings.html#unknown-which-environment-variables-are-required"
  },"334": {
    "doc": "Model warnings",
    "title": "More help",
    "content": "If you need more help, please check our GitHub issues and file a new issue if your problem isn’t discussed. Or drop into our Discord to chat with us. When reporting problems, it is very helpful if you can provide: . | Llmcode version | LLM model you are using | . Including the “announcement” lines that llmcode prints at startup is an easy way to share this helpful info. Llmcode v0.37.1-dev Models: gpt-4o with diff edit format, weak model gpt-3.5-turbo Git repo: .git with 243 files Repo-map: using 1024 tokens . Use /help &lt;question&gt; to ask for help about using llmcode, customizing settings, troubleshooting, using LLMs, etc. ",
    "url": "/docs/troubleshooting/warnings.html#more-help",
    
    "relUrl": "/docs/troubleshooting/warnings.html#more-help"
  },"335": {
    "doc": "Llmcode in your IDE",
    "title": "Llmcode in your IDE",
    "content": "Llmcode browser UI demo video ",
    "url": "/docs/usage/watch.html",
    
    "relUrl": "/docs/usage/watch.html"
  },"336": {
    "doc": "Llmcode in your IDE",
    "title": "AI comments",
    "content": "If you run llmcode with --watch-files, it will watch all files in your repo and look for any AI coding instructions you add using your favorite IDE or text editor. Specifically, llmcode looks for one-liner comments (# … or // …) that either start or end with AI, AI! or AI? like these: . # Make a snake game. AI! # What is the purpose of this method AI? . Or in // comment languages… . // Write a protein folding prediction engine. AI! . Llmcode will take note of all the comments that start or end with AI. Comments that include AI! with an exclamation point or AI? with a question mark are special. They trigger llmcode to take action to collect all the AI comments and use them as your instructions. | AI! triggers llmcode to make changes to your code. | AI? triggers llmcode to answer your question. | . See the demo video above that shows llmcode working with AI comments in VSCode. ",
    "url": "/docs/usage/watch.html#ai-comments",
    
    "relUrl": "/docs/usage/watch.html#ai-comments"
  },"337": {
    "doc": "Llmcode in your IDE",
    "title": "Example",
    "content": "For example, if you included this AI comment in your code: . function factorial(n) // Implement this. AI! . Then llmcode would update the file and implement the function: . function factorial(n) { if (n === 0 || n === 1) { return 1; } else { return n * factorial(n - 1); } } . ",
    "url": "/docs/usage/watch.html#example",
    
    "relUrl": "/docs/usage/watch.html#example"
  },"338": {
    "doc": "Llmcode in your IDE",
    "title": "Comment styles",
    "content": "Llmcode only watches for these types of one-liner comments: . # Python and bash style // Javascript style -- SQL style . Llmcode will look for those comment types in all files. You can use them into any code file you’re editing, even if they aren’t the correct comment syntax for that language. ",
    "url": "/docs/usage/watch.html#comment-styles",
    
    "relUrl": "/docs/usage/watch.html#comment-styles"
  },"339": {
    "doc": "Llmcode in your IDE",
    "title": "Multiple uses",
    "content": "This capability is quite flexible and powerful, and can be used in many ways. In-context instructions . You can add an AI comment in the function you want changed, explaining the change request in-context right where you want the changes. app.get('/sqrt/:n', (req, res) =&gt; { const n = parseFloat(req.params.n); // Add error handling for NaN and less than zero. AI! const result = math.sqrt(n); res.json({ result: result }); }); . Multiple comments . You can add multiple AI comments without the !, before triggering llmcode with a final AI!. Also keep in mind that you can spread the AI comments across multiple files, if you want to coordinate changes in multiple places. Just use AI! last, to trigger llmcode. @app.route('/factorial/&lt;int:n&gt;') def factorial(n): if n &lt; 0: return jsonify(error=\"Factorial is not defined for negative numbers\"), 400 # AI: Refactor this code... result = 1 for i in range(1, n + 1): result *= i # ... into to a compute_factorial() function. AI! return jsonify(result=result) . Long form instructions . You can add a block of comments, with longer instructions. Just be sure to start or end one of the lines with AI or AI! to draw llmcode’s attention to the block. # Make these changes: AI! # - Add a proper main() function # - Use Click to process cmd line args # - Accept --host and --port args # - Print a welcome message that includes the listening url if __name__ == \"__main__\": app.run(debug=True) . Add a file to the llmcode chat . Rather than using /add to add a file inside the llmcode chat, you can simply put an #AI comment in it and save the file. You can undo/remove the comment immediately if you like, the file will still be added to the llmcode chat. ",
    "url": "/docs/usage/watch.html#multiple-uses",
    
    "relUrl": "/docs/usage/watch.html#multiple-uses"
  },"340": {
    "doc": "Llmcode in your IDE",
    "title": "Also use llmcode chat in the terminal",
    "content": "It can be really helpful to get a change started with AI comments. But sometimes you want to build on or refine those changes. You can of course continue to do that with AI comments, but it can sometimes be effective to switch over to the llmcode terminal chat. The chat has the history of the AI comments you just made, so you can continue on naturally from there. You can also use the normal llmcode chat in your terminal to work with many of llmcode’s more advanced features: . | Use /undo to revert changes you don’t like. Although you may also be able to use your IDE’s undo function to step back in the file history. | Use chat modes to ask questions or get help. | Manage the chat context with /tokens, /clear, /drop, /reset. Adding an AI comment will add the file to the chat. Periodically, you may want remove extra context that is no longer needed. | Fix lint and test errors. | Run shell commands. | Etc. | . ",
    "url": "/docs/usage/watch.html#also-use-llmcode-chat-in-the-terminal",
    
    "relUrl": "/docs/usage/watch.html#also-use-llmcode-chat-in-the-terminal"
  },"341": {
    "doc": "Llmcode in your IDE",
    "title": "You can be lazy",
    "content": "The examples above all show AI comments with full sentences, proper capitalization, punctuation, etc. This was done to help explain how AI comments work, but is not needed in practice. Most LLMs are perfectly capable of dealing with ambiguity and inferring implied intent. This often allows you to be quite lazy with your AI comments. In particular, you can start and end comments with lowercase ai and ai!, but you can also be much more terse with the request itself. Below are simpler versions of some of the examples given above. When the context clearly implies the needed action, ai! might be all you need. For example, to implement a factorial function in a program full of other math functions either of these approaches would probably work: . function factorial(n) // ai! . Or… . // add factorial() ai! . Rather than a long, explicit comment like “Add error handling for NaN and less than zero,” you can let llmcode infer more about the request. This simpler comment may be sufficient: . app.get('/sqrt/:n', (req, res) =&gt; { const n = parseFloat(req.params.n); // add error handling ai! const result = math.sqrt(n); res.json({ result: result }); }); . Similarly, this refactor probably could have been requested with fewer words, like this: . @app.route('/factorial/&lt;int:n&gt;') def factorial(n): if n &lt; 0: return jsonify(error=\"Factorial is not defined for negative numbers\"), 400 # ai refactor... result = 1 for i in range(1, n + 1): result *= i # ... to compute_factorial() ai! return jsonify(result=result) . As you use llmcode with your chosen LLM, you can develop a sense for how explicit you need to make your AI comments. ",
    "url": "/docs/usage/watch.html#you-can-be-lazy",
    
    "relUrl": "/docs/usage/watch.html#you-can-be-lazy"
  },"342": {
    "doc": "Llmcode in your IDE",
    "title": "Behind the scenes",
    "content": "Llmcode sends your AI comments to the LLM with the repo map and all the other code context you’ve added to the chat. It also pulls out and highlights the AI comments with specific context, showing the LLM exactly how they fit into the code base. The \"AI\" comments below marked with █ can be found in the code files I've shared with you. They contain your instructions. Make the requested changes. Be sure to remove all these \"AI\" comments from the code! todo_app.py: ⋮... │class TodoList: ⋮... │ def __init__(self): │ \"\"\"Initialize an empty todo list\"\"\" ⋮... │ │ def list_tasks(self): │ \"\"\"Display all tasks\"\"\" █ # Implement this. AI! │ │def main(): │ todo = TodoList() │ ⋮... Credits . This feature was inspired by the way Override watches for file changes to find prompts embedded within //&gt; a specific set of delimiters &lt;//. ",
    "url": "/docs/usage/watch.html#behind-the-scenes",
    
    "relUrl": "/docs/usage/watch.html#behind-the-scenes"
  },"343": {
    "doc": "xAI",
    "title": "xAI",
    "content": "You’ll need a xAI API key. To use xAI: . python -m pip install -U llmcode export XAI_API_KEY=&lt;key&gt; # Mac/Linux setx XAI_API_KEY &lt;key&gt; # Windows, restart shell after setx llmcode --model xai/grok-beta # List models available from xAI llmcode --list-models xai/ . ",
    "url": "/docs/llms/xai.html",
    
    "relUrl": "/docs/llms/xai.html"
  }
}
