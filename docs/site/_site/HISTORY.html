

<!DOCTYPE html>

<html lang="en-US">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge">

  <link rel="stylesheet" href="/assets/css/just-the-docs-default.css">

  <link rel="stylesheet" href="/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet">

  <style id="jtd-nav-activation">
  

.site-nav > ul.nav-list:first-child > li:not(:nth-child(10)) > a,
.site-nav > ul.nav-list:first-child > li > ul > li > a,
.site-nav > ul.nav-list:first-child > li > ul > li > ul > li > a {
  background-image: none;
}

.site-nav > ul.nav-list:not(:first-child) a,
.site-nav li.external a {
  background-image: none;
}

.site-nav > ul.nav-list:first-child > li:nth-child(10) > a {
  font-weight: 600;
  text-decoration: none;
}.site-nav > ul.nav-list:first-child > li:nth-child(10) > button svg {
  transform: rotate(-90deg);
}.site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(10) > ul.nav-list {
  display: block;
}
  </style>

  

  
    <script src="/assets/js/vendor/lunr.min.js"></script>
  

  <script src="/assets/js/just-the-docs.js"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1">

  



  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Release history | llmcode</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="Release history" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Release notes and stats on llmcode writing its own code." />
<meta property="og:description" content="Release notes and stats on llmcode writing its own code." />
<link rel="canonical" href="http://0.0.0.0:4000/HISTORY.html" />
<meta property="og:url" content="http://0.0.0.0:4000/HISTORY.html" />
<meta property="og:site_name" content="llmcode" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Release history" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Release notes and stats on llmcode writing its own code.","headline":"Release history","url":"http://0.0.0.0:4000/HISTORY.html"}</script>
<!-- End Jekyll SEO tag -->


  
<meta property="og:image" content="http://0.0.0.0:4000/assets/blame.jpg">
<meta property="twitter:image" content="http://0.0.0.0:4000/assets/blame.jpg">

<link rel="alternate" type="application/rss+xml" title="RSS Feed" href="http://0.0.0.0:4000/feed.xml">
<link rel="preconnect" href="https://fonts.gstatic.com">
<link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style" type="text/css" crossorigin>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="theme-color" content="#157878">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/favicon-16x16.png">
<link rel="apple-touch-icon" sizes="180x180" href="/assets/icons/apple-touch-icon.png">
<link rel="manifest" href="/assets/icons/site.webmanifest">
<link rel="mask-icon" href="/assets/icons/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">




</head>

<body>
  <a class="skip-to-main" href="#main-content">Skip to main content</a>
  <svg xmlns="http://www.w3.org/2000/svg" class="d-none">
  <symbol id="svg-link" viewBox="0 0 24 24">
  <title>Link</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link">
    <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
  </svg>
</symbol>

  <symbol id="svg-menu" viewBox="0 0 24 24">
  <title>Menu</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu">
    <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line>
  </svg>
</symbol>

  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
  <title>Expand</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right">
    <polyline points="9 18 15 12 9 6"></polyline>
  </svg>
</symbol>

  <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE -->
<symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link">
  <title id="svg-external-link-title">(external link)</title>
  <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line>
</symbol>

  
    <symbol id="svg-doc" viewBox="0 0 24 24">
  <title>Document</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file">
    <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline>
  </svg>
</symbol>

    <symbol id="svg-search" viewBox="0 0 24 24">
  <title>Search</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search">
    <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line>
  </svg>
</symbol>

  
  
    <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md -->
<symbol id="svg-copy" viewBox="0 0 16 16">
  <title>Copy</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16">
    <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/>
    <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/>
  </svg>
</symbol>
<symbol id="svg-copied" viewBox="0 0 16 16">
  <title>Copied</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16">
    <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/>
    <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/>
  </svg>
</symbol>

  
</svg>

  <div class="side-bar">
  <div class="site-header" role="banner">
    <a href="/" class="site-title lh-tight">
  llmcode

</a>
    <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false">
      <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg>
    </button>
  </div>

  <nav aria-label="Main" id="site-nav" class="site-nav">
  
  
    <ul class="nav-list"><li class="nav-list-item"><a href="/" class="nav-list-link">Home</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Installation category" aria-pressed="false">
      <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg>
    </button><a href="/docs/install.html" class="nav-list-link">Installation</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/install/optional.html" class="nav-list-link">Optional steps</a></li><li class="nav-list-item"><a href="/docs/install/docker.html" class="nav-list-link">Llmcode with docker</a></li><li class="nav-list-item"><a href="/docs/install/codespaces.html" class="nav-list-link">GitHub Codespaces</a></li><li class="nav-list-item"><a href="/docs/install/replit.html" class="nav-list-link">Replit</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Usage category" aria-pressed="false">
      <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg>
    </button><a href="/docs/usage.html" class="nav-list-link">Usage</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/usage/tips.html" class="nav-list-link">Tips</a></li><li class="nav-list-item"><a href="/docs/usage/commands.html" class="nav-list-link">In-chat commands</a></li><li class="nav-list-item"><a href="/docs/usage/modes.html" class="nav-list-link">Chat modes</a></li><li class="nav-list-item"><a href="/docs/usage/tutorials.html" class="nav-list-link">Tutorial videos</a></li><li class="nav-list-item"><a href="/docs/usage/voice.html" class="nav-list-link">Voice-to-code with llmcode</a></li><li class="nav-list-item"><a href="/docs/usage/images-urls.html" class="nav-list-link">Images &amp; web pages</a></li><li class="nav-list-item"><a href="/docs/usage/caching.html" class="nav-list-link">Prompt caching</a></li><li class="nav-list-item"><a href="/docs/usage/watch.html" class="nav-list-link">Llmcode in your IDE</a></li><li class="nav-list-item"><a href="/docs/usage/browser.html" class="nav-list-link">Llmcode in your browser</a></li><li class="nav-list-item"><a href="/docs/usage/conventions.html" class="nav-list-link">Specifying coding conventions</a></li><li class="nav-list-item"><a href="/docs/usage/copypaste.html" class="nav-list-link">Copy/paste with web chat</a></li><li class="nav-list-item"><a href="/docs/usage/lint-test.html" class="nav-list-link">Linting and testing</a></li><li class="nav-list-item"><a href="/docs/usage/not-code.html" class="nav-list-link">Editing config &amp; text files</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Connecting to LLMs category" aria-pressed="false">
      <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg>
    </button><a href="/docs/llms.html" class="nav-list-link">Connecting to LLMs</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/llms/openai.html" class="nav-list-link">OpenAI</a></li><li class="nav-list-item"><a href="/docs/llms/anthropic.html" class="nav-list-link">Anthropic</a></li><li class="nav-list-item"><a href="/docs/llms/gemini.html" class="nav-list-link">Gemini</a></li><li class="nav-list-item"><a href="/docs/llms/groq.html" class="nav-list-link">GROQ</a></li><li class="nav-list-item"><a href="/docs/llms/lm-studio.html" class="nav-list-link">LM Studio</a></li><li class="nav-list-item"><a href="/docs/llms/xai.html" class="nav-list-link">xAI</a></li><li class="nav-list-item"><a href="/docs/llms/azure.html" class="nav-list-link">Azure</a></li><li class="nav-list-item"><a href="/docs/llms/cohere.html" class="nav-list-link">Cohere</a></li><li class="nav-list-item"><a href="/docs/llms/deepseek.html" class="nav-list-link">DeepSeek</a></li><li class="nav-list-item"><a href="/docs/llms/ollama.html" class="nav-list-link">Ollama</a></li><li class="nav-list-item"><a href="/docs/llms/openai-compat.html" class="nav-list-link">OpenAI compatible APIs</a></li><li class="nav-list-item"><a href="/docs/llms/openrouter.html" class="nav-list-link">OpenRouter</a></li><li class="nav-list-item"><a href="/docs/llms/vertex.html" class="nav-list-link">Vertex AI</a></li><li class="nav-list-item"><a href="/docs/llms/bedrock.html" class="nav-list-link">Amazon Bedrock</a></li><li class="nav-list-item"><a href="/docs/llms/other.html" class="nav-list-link">Other LLMs</a></li><li class="nav-list-item"><a href="/docs/llms/warnings.html" class="nav-list-link">Model warnings</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Configuration category" aria-pressed="false">
      <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg>
    </button><a href="/docs/config.html" class="nav-list-link">Configuration</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/config/api-keys.html" class="nav-list-link">API Keys</a></li><li class="nav-list-item"><a href="/docs/config/options.html" class="nav-list-link">Options reference</a></li><li class="nav-list-item"><a href="/docs/config/llmcode_conf.html" class="nav-list-link">YAML config file</a></li><li class="nav-list-item"><a href="/docs/config/dotenv.html" class="nav-list-link">Config with .env</a></li><li class="nav-list-item"><a href="/docs/config/editor.html" class="nav-list-link">Editor configuration</a></li><li class="nav-list-item"><a href="/docs/config/reasoning.html" class="nav-list-link">Reasoning models</a></li><li class="nav-list-item"><a href="/docs/config/adv-model-settings.html" class="nav-list-link">Advanced model settings</a></li><li class="nav-list-item"><a href="/docs/config/model-aliases.html" class="nav-list-link">Model Aliases</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Troubleshooting category" aria-pressed="false">
      <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg>
    </button><a href="/docs/troubleshooting.html" class="nav-list-link">Troubleshooting</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/troubleshooting/edit-errors.html" class="nav-list-link">File editing problems</a></li><li class="nav-list-item"><a href="/docs/troubleshooting/warnings.html" class="nav-list-link">Model warnings</a></li><li class="nav-list-item"><a href="/docs/troubleshooting/token-limits.html" class="nav-list-link">Token limits</a></li><li class="nav-list-item"><a href="/docs/troubleshooting/imports.html" class="nav-list-link">Dependency versions</a></li><li class="nav-list-item"><a href="/docs/troubleshooting/llmcode-not-found.html" class="nav-list-link">Llmcode not found</a></li><li class="nav-list-item"><a href="/docs/troubleshooting/models-and-keys.html" class="nav-list-link">Models and API keys</a></li><li class="nav-list-item"><a href="/docs/troubleshooting/support.html" class="nav-list-link">Using /help</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Example chat transcripts category" aria-pressed="false">
      <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg>
    </button><a href="/examples/README.html" class="nav-list-link">Example chat transcripts</a><ul class="nav-list"><li class="nav-list-item"><a href="/examples/hello-world-flask.html" class="nav-list-link">Create a simple flask app with llmcode</a></li><li class="nav-list-item"><a href="/examples/2048-game.html" class="nav-list-link">Modify an open source 2048 game with llmcode</a></li><li class="nav-list-item"><a href="/examples/complex-change.html" class="nav-list-link">A complex multi-file change, with debugging</a></li><li class="nav-list-item"><a href="/examples/add-test.html" class="nav-list-link">Create a “black box” test case</a></li><li class="nav-list-item"><a href="/examples/update-docs.html" class="nav-list-link">Automatically update docs with llmcode</a></li><li class="nav-list-item"><a href="/examples/pong.html" class="nav-list-link">Build pong with llmcode and pygame.</a></li><li class="nav-list-item"><a href="/examples/css-exercises.html" class="nav-list-link">Complete a css exercise with llmcode</a></li><li class="nav-list-item"><a href="/examples/census.html" class="nav-list-link">Download, analyze and plot US Census data</a></li><li class="nav-list-item"><a href="/examples/asciinema.html" class="nav-list-link">Editing an asciinema cast file with llmcode</a></li><li class="nav-list-item"><a href="/examples/hello.html" class="nav-list-link">Hello llmcode!</a></li><li class="nav-list-item"><a href="/examples/no-color.html" class="nav-list-link">Honor the NO_COLOR environment variable</a></li><li class="nav-list-item"><a href="/examples/chat-transcript-css.html" class="nav-list-link">Improve css styling of chat transcripts</a></li><li class="nav-list-item"><a href="/examples/semantic-search-replace.html" class="nav-list-link">Semantic search &amp; replace code with llmcode</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in More info category" aria-pressed="false">
      <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg>
    </button><a href="/docs/more-info.html" class="nav-list-link">More info</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/git.html" class="nav-list-link">Git integration</a></li><li class="nav-list-item"><a href="/docs/languages.html" class="nav-list-link">Supported languages</a></li><li class="nav-list-item"><a href="/docs/repomap.html" class="nav-list-link">Repository map</a></li><li class="nav-list-item"><a href="/docs/scripting.html" class="nav-list-link">Scripting llmcode</a></li><li class="nav-list-item"><a href="/docs/more/infinite-output.html" class="nav-list-link">Infinite output</a></li><li class="nav-list-item"><a href="/docs/more/edit-formats.html" class="nav-list-link">Edit formats</a></li><li class="nav-list-item"><a href="/docs/more/analytics.html" class="nav-list-link">Analytics</a></li><li class="nav-list-item"><a href="/docs/legal/privacy.html" class="nav-list-link">Privacy policy</a></li></ul></li><li class="nav-list-item"><a href="/docs/faq.html" class="nav-list-link">FAQ</a></li><li class="nav-list-item"><a href="/HISTORY.html" class="nav-list-link">Release history</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Llmcode LLM Leaderboards category" aria-pressed="false">
      <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg>
    </button><a href="/docs/leaderboards/" class="nav-list-link">Llmcode LLM Leaderboards</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/leaderboards/edit.html" class="nav-list-link">Code editing leaderboard</a></li><li class="nav-list-item"><a href="/docs/leaderboards/refactor.html" class="nav-list-link">Refactoring leaderboard</a></li><li class="nav-list-item"><a href="/docs/leaderboards/by-release-date.html" class="nav-list-link">Scores by release date</a></li><li class="nav-list-item"><a href="/docs/leaderboards/notes.html" class="nav-list-link">Benchmark notes</a></li><li class="nav-list-item"><a href="/docs/leaderboards/contrib.html" class="nav-list-link">Contributing results</a></li></ul></li><li class="nav-list-item"><a href="/blog/" class="nav-list-link">Llmcode blog</a></li></ul>

  <ul class="nav-list"><li class="nav-list-item external">
          <a href="https://github.com/KhulnaSoft/llmcode" class="nav-list-link external"
            
          >
            GitHub
            <svg viewBox="0 0 24 24" aria-labelledby="svg-external-link-title"><use xlink:href="#svg-external-link"></use></svg>
          </a>
        </li><li class="nav-list-item external">
          <a href="https://discord.gg/Tv2uQnR88V" class="nav-list-link external"
            
          >
            Discord
            <svg viewBox="0 0 24 24" aria-labelledby="svg-external-link-title"><use xlink:href="#svg-external-link"></use></svg>
          </a>
        </li></ul>
</nav>


  
  
    <footer class="site-footer">
    Llmcode is AI pair programming in your terminal.
    Llmcode is on
    <a href="https://github.com/KhulnaSoft/llmcode">GitHub</a>
    and
    <a href="https://discord.gg/Tv2uQnR88V">Discord</a>.
</footer>

  
</div>

  <div class="main" id="top">
    <div id="main-header" class="main-header">
  
    

<div class="search" role="search">
  <div class="search-input-wrap">
    <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search llmcode" aria-label="Search llmcode" autocomplete="off">
    <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label>
  </div>
  <div id="search-results" class="search-results"></div>
</div>

  
  
  
    <nav aria-label="Auxiliary" class="aux-nav">
  <ul class="aux-nav-list">
    
      <li class="aux-nav-list-item">
        <a href="https://github.com/KhulnaSoft/llmcode" class="site-button"
          
        >
          GitHub
        </a>
      </li>
    
      <li class="aux-nav-list-item">
        <a href="https://discord.gg/Tv2uQnR88V" class="site-button"
          
        >
          Discord
        </a>
      </li>
    
      <li class="aux-nav-list-item">
        <a href="/blog/" class="site-button"
          
        >
          Blog
        </a>
      </li>
    
  </ul>
</nav>

  
</div>

    <div class="main-content-wrap">
      
      <div id="main-content" class="main-content">
        <main>
          
            <h1 id="release-history">
  
  
    <a href="#release-history" class="anchor-heading" aria-labelledby="release-history"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Release history
  
  
</h1>
    

<div class="chart-container">
    <canvas id="blameChart" style="margin-top: 20px"></canvas>
</div>
<div class="chart-container">
    <canvas id="linesChart" style="margin-top: 20px"></canvas>
</div>

<style>
.chart-container {
    position: relative;
    width: 100%;
    height: 300px;
}
</style>

<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>

<script src="https://cdn.jsdelivr.net/npm/moment"></script>

<script src="https://cdn.jsdelivr.net/npm/chartjs-adapter-moment"></script>

<script>
document.addEventListener('DOMContentLoaded', function () {
    var blameCtx = document.getElementById('blameChart').getContext('2d');
    var linesCtx = document.getElementById('linesChart').getContext('2d');
    
    var labels = ['v0.6.0','v0.7.0','v0.8.0','v0.9.0','v0.10.0','v0.11.0','v0.12.0','v0.13.0','v0.14.0','v0.15.0','v0.16.0','v0.17.0','v0.18.0','v0.19.0','v0.20.0','v0.21.0','v0.22.0','v0.23.0','v0.24.0','v0.25.0','v0.26.0','v0.27.0','v0.28.0','v0.29.0','v0.30.0','v0.31.0','v0.32.0','v0.33.0','v0.34.0','v0.35.0','v0.36.0','v0.37.0','v0.38.0','v0.39.0','v0.40.0','v0.41.0','v0.42.0','v0.43.0','v0.44.0','v0.45.0','v0.46.0','v0.47.0','v0.48.0','v0.49.0','v0.50.0','v0.51.0','v0.52.0','v0.53.0','v0.54.0','v0.55.0','v0.56.0','v0.57.0','v0.58.0','v0.59.0','v0.60.0','v0.61.0','v0.62.0','v0.63.0','v0.64.0','v0.65.0','v0.66.0','v0.67.0','v0.68.0','v0.69.0','v0.70.0','v0.71.0','v0.72.0','v0.73.0','v0.74.0','v0.75.0',];
    
    var blameData = {
        labels: labels,
        datasets: [{
            label: 'Llmcode\'s percent of new code by release',
            data: [{ x: 'v0.6.0', y: 29.08, lines: 41 },{ x: 'v0.7.0', y: 15.99, lines: 224 },{ x: 'v0.8.0', y: 8.21, lines: 142 },{ x: 'v0.9.0', y: 17.67, lines: 144 },{ x: 'v0.10.0', y: 11.34, lines: 33 },{ x: 'v0.11.0', y: 6.62, lines: 49 },{ x: 'v0.12.0', y: 4.71, lines: 24 },{ x: 'v0.13.0', y: 4.32, lines: 23 },{ x: 'v0.14.0', y: 0.55, lines: 1 },{ x: 'v0.15.0', y: 11.28, lines: 38 },{ x: 'v0.16.0', y: 1.76, lines: 16 },{ x: 'v0.17.0', y: 5.35, lines: 13 },{ x: 'v0.18.0', y: 39.3, lines: 90 },{ x: 'v0.19.0', y: 0.72, lines: 13 },{ x: 'v0.20.0', y: 11.38, lines: 38 },{ x: 'v0.21.0', y: 19.32, lines: 17 },{ x: 'v0.22.0', y: 0.0, lines: 0 },{ x: 'v0.23.0', y: 1.16, lines: 2 },{ x: 'v0.24.0', y: 6.68, lines: 25 },{ x: 'v0.25.0', y: 5.79, lines: 7 },{ x: 'v0.26.0', y: 0.0, lines: 0 },{ x: 'v0.27.0', y: 0.0, lines: 0 },{ x: 'v0.28.0', y: 0.0, lines: 0 },{ x: 'v0.29.0', y: 5.69, lines: 35 },{ x: 'v0.30.0', y: 0.0, lines: 0 },{ x: 'v0.31.0', y: 0.16, lines: 2 },{ x: 'v0.32.0', y: 3.29, lines: 8 },{ x: 'v0.33.0', y: 0.0, lines: 0 },{ x: 'v0.34.0', y: 0.0, lines: 0 },{ x: 'v0.35.0', y: 6.8, lines: 17 },{ x: 'v0.36.0', y: 15.38, lines: 92 },{ x: 'v0.37.0', y: 19.06, lines: 113 },{ x: 'v0.38.0', y: 9.53, lines: 53 },{ x: 'v0.39.0', y: 15.56, lines: 47 },{ x: 'v0.40.0', y: 6.42, lines: 21 },{ x: 'v0.41.0', y: 4.74, lines: 11 },{ x: 'v0.42.0', y: 2.29, lines: 7 },{ x: 'v0.43.0', y: 10.61, lines: 40 },{ x: 'v0.44.0', y: 27.02, lines: 157 },{ x: 'v0.45.0', y: 44.93, lines: 93 },{ x: 'v0.46.0', y: 52.87, lines: 313 },{ x: 'v0.47.0', y: 56.98, lines: 355 },{ x: 'v0.48.0', y: 45.67, lines: 269 },{ x: 'v0.49.0', y: 59.83, lines: 429 },{ x: 'v0.50.0', y: 65.23, lines: 182 },{ x: 'v0.51.0', y: 56.95, lines: 582 },{ x: 'v0.52.0', y: 67.74, lines: 485 },{ x: 'v0.53.0', y: 62.36, lines: 434 },{ x: 'v0.54.0', y: 67.4, lines: 184 },{ x: 'v0.55.0', y: 52.82, lines: 759 },{ x: 'v0.56.0', y: 56.23, lines: 149 },{ x: 'v0.57.0', y: 70.27, lines: 390 },{ x: 'v0.58.0', y: 44.68, lines: 600 },{ x: 'v0.59.0', y: 68.72, lines: 123 },{ x: 'v0.60.0', y: 57.2, lines: 139 },{ x: 'v0.61.0', y: 67.04, lines: 781 },{ x: 'v0.62.0', y: 77.78, lines: 56 },{ x: 'v0.63.0', y: 55.21, lines: 350 },{ x: 'v0.64.0', y: 73.55, lines: 865 },{ x: 'v0.65.0', y: 81.19, lines: 544 },{ x: 'v0.66.0', y: 86.17, lines: 841 },{ x: 'v0.67.0', y: 67.86, lines: 437 },{ x: 'v0.68.0', y: 71.57, lines: 428 },{ x: 'v0.69.0', y: 67.87, lines: 207 },{ x: 'v0.70.0', y: 74.22, lines: 875 },{ x: 'v0.71.0', y: 60.36, lines: 236 },{ x: 'v0.72.0', y: 48.76, lines: 138 },{ x: 'v0.73.0', y: 69.44, lines: 284 },{ x: 'v0.74.0', y: 77.14, lines: 604 },{ x: 'v0.75.0', y: 46.31, lines: 163 },],
            backgroundColor: 'rgba(54, 162, 235, 0.8)',
            borderColor: 'rgba(54, 162, 235, 1)',
            borderWidth: 1
        }]
    };

    var linesData = {
        labels: labels,
        datasets: [{
            label: 'Llmcode',
            data: [{ x: 'v0.6.0', y: 41 },{ x: 'v0.7.0', y: 224 },{ x: 'v0.8.0', y: 142 },{ x: 'v0.9.0', y: 144 },{ x: 'v0.10.0', y: 33 },{ x: 'v0.11.0', y: 49 },{ x: 'v0.12.0', y: 24 },{ x: 'v0.13.0', y: 23 },{ x: 'v0.14.0', y: 1 },{ x: 'v0.15.0', y: 38 },{ x: 'v0.16.0', y: 16 },{ x: 'v0.17.0', y: 13 },{ x: 'v0.18.0', y: 90 },{ x: 'v0.19.0', y: 13 },{ x: 'v0.20.0', y: 38 },{ x: 'v0.21.0', y: 17 },{ x: 'v0.22.0', y: 0 },{ x: 'v0.23.0', y: 2 },{ x: 'v0.24.0', y: 25 },{ x: 'v0.25.0', y: 7 },{ x: 'v0.26.0', y: 0 },{ x: 'v0.27.0', y: 0 },{ x: 'v0.28.0', y: 0 },{ x: 'v0.29.0', y: 35 },{ x: 'v0.30.0', y: 0 },{ x: 'v0.31.0', y: 2 },{ x: 'v0.32.0', y: 8 },{ x: 'v0.33.0', y: 0 },{ x: 'v0.34.0', y: 0 },{ x: 'v0.35.0', y: 17 },{ x: 'v0.36.0', y: 92 },{ x: 'v0.37.0', y: 113 },{ x: 'v0.38.0', y: 53 },{ x: 'v0.39.0', y: 47 },{ x: 'v0.40.0', y: 21 },{ x: 'v0.41.0', y: 11 },{ x: 'v0.42.0', y: 7 },{ x: 'v0.43.0', y: 40 },{ x: 'v0.44.0', y: 157 },{ x: 'v0.45.0', y: 93 },{ x: 'v0.46.0', y: 313 },{ x: 'v0.47.0', y: 355 },{ x: 'v0.48.0', y: 269 },{ x: 'v0.49.0', y: 429 },{ x: 'v0.50.0', y: 182 },{ x: 'v0.51.0', y: 582 },{ x: 'v0.52.0', y: 485 },{ x: 'v0.53.0', y: 434 },{ x: 'v0.54.0', y: 184 },{ x: 'v0.55.0', y: 759 },{ x: 'v0.56.0', y: 149 },{ x: 'v0.57.0', y: 390 },{ x: 'v0.58.0', y: 600 },{ x: 'v0.59.0', y: 123 },{ x: 'v0.60.0', y: 139 },{ x: 'v0.61.0', y: 781 },{ x: 'v0.62.0', y: 56 },{ x: 'v0.63.0', y: 350 },{ x: 'v0.64.0', y: 865 },{ x: 'v0.65.0', y: 544 },{ x: 'v0.66.0', y: 841 },{ x: 'v0.67.0', y: 437 },{ x: 'v0.68.0', y: 428 },{ x: 'v0.69.0', y: 207 },{ x: 'v0.70.0', y: 875 },{ x: 'v0.71.0', y: 236 },{ x: 'v0.72.0', y: 138 },{ x: 'v0.73.0', y: 284 },{ x: 'v0.74.0', y: 604 },{ x: 'v0.75.0', y: 163 },],
            backgroundColor: 'rgba(54, 162, 235, 0.8)',
            borderColor: 'rgba(54, 162, 235, 1)',
            borderWidth: 1
        },
        {
            label: 'Human',
            data: [{ x: 'v0.6.0', y: 100 },{ x: 'v0.7.0', y: 1177 },{ x: 'v0.8.0', y: 1587 },{ x: 'v0.9.0', y: 671 },{ x: 'v0.10.0', y: 258 },{ x: 'v0.11.0', y: 691 },{ x: 'v0.12.0', y: 486 },{ x: 'v0.13.0', y: 510 },{ x: 'v0.14.0', y: 181 },{ x: 'v0.15.0', y: 299 },{ x: 'v0.16.0', y: 894 },{ x: 'v0.17.0', y: 230 },{ x: 'v0.18.0', y: 139 },{ x: 'v0.19.0', y: 1799 },{ x: 'v0.20.0', y: 296 },{ x: 'v0.21.0', y: 71 },{ x: 'v0.22.0', y: 54 },{ x: 'v0.23.0', y: 171 },{ x: 'v0.24.0', y: 349 },{ x: 'v0.25.0', y: 114 },{ x: 'v0.26.0', y: 35 },{ x: 'v0.27.0', y: 194 },{ x: 'v0.28.0', y: 4 },{ x: 'v0.29.0', y: 580 },{ x: 'v0.30.0', y: 213 },{ x: 'v0.31.0', y: 1247 },{ x: 'v0.32.0', y: 235 },{ x: 'v0.33.0', y: 21 },{ x: 'v0.34.0', y: 54 },{ x: 'v0.35.0', y: 233 },{ x: 'v0.36.0', y: 506 },{ x: 'v0.37.0', y: 480 },{ x: 'v0.38.0', y: 503 },{ x: 'v0.39.0', y: 255 },{ x: 'v0.40.0', y: 306 },{ x: 'v0.41.0', y: 221 },{ x: 'v0.42.0', y: 299 },{ x: 'v0.43.0', y: 337 },{ x: 'v0.44.0', y: 424 },{ x: 'v0.45.0', y: 114 },{ x: 'v0.46.0', y: 279 },{ x: 'v0.47.0', y: 268 },{ x: 'v0.48.0', y: 320 },{ x: 'v0.49.0', y: 288 },{ x: 'v0.50.0', y: 97 },{ x: 'v0.51.0', y: 440 },{ x: 'v0.52.0', y: 231 },{ x: 'v0.53.0', y: 262 },{ x: 'v0.54.0', y: 89 },{ x: 'v0.55.0', y: 678 },{ x: 'v0.56.0', y: 116 },{ x: 'v0.57.0', y: 165 },{ x: 'v0.58.0', y: 743 },{ x: 'v0.59.0', y: 56 },{ x: 'v0.60.0', y: 104 },{ x: 'v0.61.0', y: 384 },{ x: 'v0.62.0', y: 16 },{ x: 'v0.63.0', y: 284 },{ x: 'v0.64.0', y: 311 },{ x: 'v0.65.0', y: 126 },{ x: 'v0.66.0', y: 135 },{ x: 'v0.67.0', y: 207 },{ x: 'v0.68.0', y: 170 },{ x: 'v0.69.0', y: 98 },{ x: 'v0.70.0', y: 304 },{ x: 'v0.71.0', y: 155 },{ x: 'v0.72.0', y: 145 },{ x: 'v0.73.0', y: 125 },{ x: 'v0.74.0', y: 179 },{ x: 'v0.75.0', y: 189 },],
            backgroundColor: 'rgba(200, 200, 200, 0.8)',
            borderColor: 'rgba(200, 200, 200, 1)',
            borderWidth: 1
        }]
    };

    var blameChart = new Chart(blameCtx, {
        type: 'bar',
        data: blameData,
        options: {
            maintainAspectRatio: false,
            scales: {
                x: {
                    type: 'category',
                    title: {
                        display: true,
                        text: 'Version'
                    },
                    ticks: {
                        maxRotation: 45,
                        minRotation: 45
                    }
                },
                y: {
                    title: {
                        display: true,
                        text: 'Percent of new code'
                    },
                    beginAtZero: true
                }
            },
            plugins: {
                legend: {
                    display: false
                },
                tooltip: {
                    callbacks: {
                        label: function(context) {
                            var label = 'Llmcode\'s contribution';
                            var value = context.parsed.y || 0;
                            var lines = context.raw.lines || 0;
                            return `${label}: ${Math.round(value)}% (${lines} lines)`;
                        }
                    }
                },
                title: {
                    display: true,
                    text: 'Percent of new code written by llmcode, by release',
                    font: {
                        size: 16
                    }
                }
            }
        }
    });

    var linesChart = new Chart(linesCtx, {
        type: 'bar',
        data: linesData,
        options: {
            maintainAspectRatio: false,
            scales: {
                x: {
                    type: 'category',
                    stacked: true,
                    title: {
                        display: true,
                        text: 'Version'
                    },
                    ticks: {
                        maxRotation: 45,
                        minRotation: 45
                    }
                },
                y: {
                    stacked: true,
                    title: {
                        display: true,
                        text: 'Lines of new code'
                    },
                    beginAtZero: true
                }
            },
            plugins: {
                legend: {
                    display: true,
                    position: 'chartArea',
                    reverse: true
                },
                tooltip: {
                    callbacks: {
                        label: function(context) {
                            var label = context.dataset.label;
                            var value = context.parsed.y || 0;
                            return `${label}: ${value}`;
                        }
                    }
                },
                title: {
                    display: true,
                    text: 'Lines of new code, by release',
                    font: {
                        size: 16
                    }
                }
            }
        }
    });
});
</script>

<p>The above 
<a href="/docs/faq.html#how-are-the-llmcode-wrote-xx-of-code-stats-computed">stats are based on the git commit history</a>
of the llmcode repo.</p>
<h2 id="release-notes">
  
  
    <a href="#release-notes" class="anchor-heading" aria-labelledby="release-notes"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Release notes
  
  
</h2>
    

<!--[[[cog
# This page is a copy of HISTORY.md, adding the front matter above.
text = open("HISTORY.md").read()
text = text.replace("# Release history", "")
cog.out(text)
]]]-->
<h3 id="main-branch">
  
  
    <a href="#main-branch" class="anchor-heading" aria-labelledby="main-branch"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> main branch
  
  
</h3>
    

<ul>
  <li>Added support for Claude 3.7 Sonnet models on OpenRouter, Bedrock and Vertex AI.</li>
  <li>Llmcode wrote 47% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0751">
  
  
    <a href="#llmcode-v0751" class="anchor-heading" aria-labelledby="llmcode-v0751"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.75.1
  
  
</h3>
    

<ul>
  <li>Added support for <code class="language-plaintext highlighter-rouge">openrouter/anthropic/claude-3.7-sonnet</code></li>
</ul>
<h3 id="llmcode-v0750">
  
  
    <a href="#llmcode-v0750" class="anchor-heading" aria-labelledby="llmcode-v0750"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.75.0
  
  
</h3>
    

<ul>
  <li>Basic support for Claude 3.7 Sonnet
    <ul>
      <li>Use <code class="language-plaintext highlighter-rouge">--model sonnet</code> to use the new 3.7</li>
      <li>Thinking support coming soon.</li>
    </ul>
  </li>
  <li>Bugfix to <code class="language-plaintext highlighter-rouge">/editor</code> command.</li>
  <li>Llmcode wrote 46% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0743">
  
  
    <a href="#llmcode-v0743" class="anchor-heading" aria-labelledby="llmcode-v0743"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.74.3
  
  
</h3>
    

<ul>
  <li>Downgrade streamlit dependency to avoid threading bug.</li>
  <li>Added support for tree-sitter language pack.</li>
  <li>Added openrouter/o3-mini-high model configuration.</li>
  <li>Added build.gradle.kts to special files for Kotlin project support, by Lucas Shadler.</li>
</ul>
<h3 id="llmcode-v0742">
  
  
    <a href="#llmcode-v0742" class="anchor-heading" aria-labelledby="llmcode-v0742"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.74.2
  
  
</h3>
    

<ul>
  <li>Prevent more than one cache warming thread from becoming active.</li>
  <li>Fixed continuation prompt “. “ for multiline input.</li>
  <li>Added HCL (Terraform) syntax support, by Warren Krewenki.</li>
</ul>
<h3 id="llmcode-v0741">
  
  
    <a href="#llmcode-v0741" class="anchor-heading" aria-labelledby="llmcode-v0741"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.74.1
  
  
</h3>
    

<ul>
  <li>Have o1 &amp; o3-mini generate markdown by sending the magic “Formatting re-enabled.” string.</li>
  <li>Bugfix for multi-line inputs, which should not include the “. “ continuation prompt.</li>
</ul>
<h3 id="llmcode-v0740">
  
  
    <a href="#llmcode-v0740" class="anchor-heading" aria-labelledby="llmcode-v0740"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.74.0
  
  
</h3>
    

<ul>
  <li>Dynamically changes the Ollama context window to hold the current chat.</li>
  <li>Better support for o3-mini, DeepSeek V3 &amp; R1, o1-mini, o1 especially via third-party API providers.</li>
  <li>Remove <code class="language-plaintext highlighter-rouge">&lt;think&gt;</code> tags from R1 responses for commit messages (and other weak model uses).</li>
  <li>Can now specify <code class="language-plaintext highlighter-rouge">use_temperature: &lt;float&gt;</code> in model settings, not just true/false.</li>
  <li>The full docker container now includes <code class="language-plaintext highlighter-rouge">boto3</code> for Bedrock.</li>
  <li>Docker containers now set <code class="language-plaintext highlighter-rouge">HOME=/app</code> which is the normal project mount-point, to persist <code class="language-plaintext highlighter-rouge">~/.llmcode</code>.</li>
  <li>Bugfix to prevent creating incorrect filenames like <code class="language-plaintext highlighter-rouge">python</code>, <code class="language-plaintext highlighter-rouge">php</code>, etc.</li>
  <li>Bugfix for <code class="language-plaintext highlighter-rouge">--timeout</code></li>
  <li>Bugfix so that <code class="language-plaintext highlighter-rouge">/model</code> now correctly reports that the weak model is not changed.</li>
  <li>Bugfix so that multi-line mode persists through ^C at confirmation prompts.</li>
  <li>Watch files now fully ignores top-level directories named in ignore files, to reduce the chance of hitting OS watch limits. Helpful to ignore giant subtrees like <code class="language-plaintext highlighter-rouge">node_modules</code>.</li>
  <li>Fast startup with more providers and when model metadata provided in local files.</li>
  <li>Improved .gitignore handling:
    <ul>
      <li>Honor ignores already in effect regardless of how they’ve been configured.</li>
      <li>Check for .env only when the file exists.</li>
    </ul>
  </li>
  <li>Yes/No prompts now accept All/Skip as alias for Y/N even when not processing a group of confirmations.</li>
  <li>Llmcode wrote 77% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0730">
  
  
    <a href="#llmcode-v0730" class="anchor-heading" aria-labelledby="llmcode-v0730"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.73.0
  
  
</h3>
    

<ul>
  <li>Full support for o3-mini: <code class="language-plaintext highlighter-rouge">llmcode --model o3-mini</code></li>
  <li>New <code class="language-plaintext highlighter-rouge">--reasoning-effort</code> argument: low, medium, high.</li>
  <li>Improved handling of context window size limits, with better messaging and Ollama-specific guidance.</li>
  <li>Added support for removing model-specific reasoning tags from responses with <code class="language-plaintext highlighter-rouge">remove_reasoning: tagname</code> model setting.</li>
  <li>Auto-create parent directories when creating new files, by xqyz.</li>
  <li>Support for R1 free on OpenRouter: <code class="language-plaintext highlighter-rouge">--model openrouter/deepseek/deepseek-r1:free</code></li>
  <li>Llmcode wrote 69% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0723">
  
  
    <a href="#llmcode-v0723" class="anchor-heading" aria-labelledby="llmcode-v0723"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.72.3
  
  
</h3>
    

<ul>
  <li>Enforce user/assistant turn order to avoid R1 errors, by miradnanali.</li>
  <li>Case-insensitive model name matching while preserving original case.</li>
</ul>
<h3 id="llmcode-v0722">
  
  
    <a href="#llmcode-v0722" class="anchor-heading" aria-labelledby="llmcode-v0722"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.72.2
  
  
</h3>
    
<ul>
  <li>Harden against user/assistant turn order problems which cause R1 errors.</li>
</ul>
<h3 id="llmcode-v0721">
  
  
    <a href="#llmcode-v0721" class="anchor-heading" aria-labelledby="llmcode-v0721"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.72.1
  
  
</h3>
    
<ul>
  <li>Fix model metadata for <code class="language-plaintext highlighter-rouge">openrouter/deepseek/deepseek-r1</code></li>
</ul>
<h3 id="llmcode-v0720">
  
  
    <a href="#llmcode-v0720" class="anchor-heading" aria-labelledby="llmcode-v0720"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.72.0
  
  
</h3>
    
<ul>
  <li>Support for DeepSeek R1.
    <ul>
      <li>Use shortcut: <code class="language-plaintext highlighter-rouge">--model r1</code></li>
      <li>Also via OpenRouter: <code class="language-plaintext highlighter-rouge">--model openrouter/deepseek/deepseek-r1</code></li>
    </ul>
  </li>
  <li>Added Kotlin syntax support to repo map, by Paul Walker.</li>
  <li>Added <code class="language-plaintext highlighter-rouge">--line-endings</code> for file writing, by Titusz Pan.</li>
  <li>Added examples_as_sys_msg=True for GPT-4o models, improves benchmark scores.</li>
  <li>Bumped all dependencies, to pick up litellm support for o1 system messages.</li>
  <li>Bugfix for turn taking when reflecting lint/test errors.</li>
  <li>Llmcode wrote 52% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0711">
  
  
    <a href="#llmcode-v0711" class="anchor-heading" aria-labelledby="llmcode-v0711"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.71.1
  
  
</h3>
    

<ul>
  <li>Fix permissions issue in Docker images.</li>
  <li>Added read-only file announcements.</li>
  <li>Bugfix: ASCII fallback for unicode errors.</li>
  <li>Bugfix: integer indices for list slicing in repomap calculations.</li>
</ul>
<h3 id="llmcode-v0710">
  
  
    <a href="#llmcode-v0710" class="anchor-heading" aria-labelledby="llmcode-v0710"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.71.0
  
  
</h3>
    

<ul>
  <li>Prompts to help DeepSeek work better when alternating between <code class="language-plaintext highlighter-rouge">/ask</code> and <code class="language-plaintext highlighter-rouge">/code</code>.</li>
  <li>Streaming pretty LLM responses is smoother and faster for long replies.</li>
  <li>Streaming automatically turns of for model that don’t support it
    <ul>
      <li>Can now switch to/from <code class="language-plaintext highlighter-rouge">/model o1</code> and a streaming model</li>
    </ul>
  </li>
  <li>Pretty output remains enabled even when editing files with triple-backtick fences</li>
  <li>Bare <code class="language-plaintext highlighter-rouge">/ask</code>, <code class="language-plaintext highlighter-rouge">/code</code> and <code class="language-plaintext highlighter-rouge">/architect</code> commands now switch the chat mode.</li>
  <li>Increased default size of the repomap.</li>
  <li>Increased max chat history tokens limit from 4k to 8k.</li>
  <li>Turn off fancy input and watch files if terminal is dumb.</li>
  <li>Added support for custom voice format and input device settings.</li>
  <li>Disabled Streamlit email prompt, by apaz-cli.</li>
  <li>Docker container runs as non-root user.</li>
  <li>Fixed lint command handling of nested spaced strings, by Aaron Weisberg.</li>
  <li>Added token count feedback when adding command output to chat.</li>
  <li>Improved error handling for large audio files with automatic format conversion.</li>
  <li>Improved handling of git repo index errors, by Krazer.</li>
  <li>Improved unicode handling in console output with ASCII fallback.</li>
  <li>Added AssertionError, AttributeError to git error handling.</li>
  <li>Llmcode wrote 60% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0700">
  
  
    <a href="#llmcode-v0700" class="anchor-heading" aria-labelledby="llmcode-v0700"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.70.0
  
  
</h3>
    

<ul>
  <li>Full support for o1 models.</li>
  <li>Watch files now honors <code class="language-plaintext highlighter-rouge">--subtree-only</code>, and only watches that subtree.</li>
  <li>Improved prompting for watch files, to work more reliably with more models.</li>
  <li>New install methods via uv, including one-liners.</li>
  <li>Support for openrouter/deepseek/deepseek-chat model.</li>
  <li>Better error handling when interactive commands are attempted via <code class="language-plaintext highlighter-rouge">/load</code> or <code class="language-plaintext highlighter-rouge">--load</code>.</li>
  <li>Display read-only files with abs path if its shorter than rel path.</li>
  <li>Ask 10% of users to opt-in to analytics.</li>
  <li>Bugfix for auto-suggest.</li>
  <li>Gracefully handle unicode errors in git path names.</li>
  <li>Llmcode wrote 74% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0691">
  
  
    <a href="#llmcode-v0691" class="anchor-heading" aria-labelledby="llmcode-v0691"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.69.1
  
  
</h3>
    

<ul>
  <li>Fix for gemini model names in model metadata.</li>
  <li>Show hints about AI! and AI? when user makes AI comments.</li>
  <li>Support for running without git installed.</li>
  <li>Improved environment variable setup messages on Windows.</li>
</ul>
<h3 id="llmcode-v0690">
  
  
    <a href="#llmcode-v0690" class="anchor-heading" aria-labelledby="llmcode-v0690"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.69.0
  
  
</h3>
    

<ul>
  <li><a href="https://llmcode.khulnasoft.com/docs/usage/watch.html">Watch files</a> improvements:
    <ul>
      <li>Use <code class="language-plaintext highlighter-rouge"># ... AI?</code> comments to trigger llmcode and ask questions about your code.</li>
      <li>Now watches <em>all</em> files, not just certain source files.</li>
      <li>Use <code class="language-plaintext highlighter-rouge"># AI comments</code>, <code class="language-plaintext highlighter-rouge">// AI comments</code>, or <code class="language-plaintext highlighter-rouge">-- AI comments</code> to give llmcode instructions in any text file.</li>
    </ul>
  </li>
  <li>Full support for Gemini Flash 2.0 Exp:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">llmcode --model flash</code> or <code class="language-plaintext highlighter-rouge">llmcode --model gemini/gemini-2.0-flash-exp</code></li>
    </ul>
  </li>
  <li><a href="https://llmcode.khulnasoft.com/docs/usage/commands.html#entering-multi-line-chat-messages">New <code class="language-plaintext highlighter-rouge">--multiline</code> flag and <code class="language-plaintext highlighter-rouge">/multiline-mode</code> command</a> makes ENTER a soft newline and META-ENTER send the message, by @miradnanali.</li>
  <li><code class="language-plaintext highlighter-rouge">/copy-context &lt;instructions&gt;</code> now takes optional “instructions” when <a href="https://llmcode.khulnasoft.com/docs/usage/copypaste.html#copy-llmcodes-code-context-to-your-clipboard-paste-into-the-web-ui">copying code context to the clipboard</a>.</li>
  <li>Improved clipboard error handling with helpful requirements install info.</li>
  <li>Ask 5% of users if they want to opt-in to analytics.</li>
  <li><code class="language-plaintext highlighter-rouge">/voice</code> now lets you edit the transcribed text before sending.</li>
  <li>Disabled auto-complete in Y/N prompts.</li>
  <li>Llmcode wrote 68% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0680">
  
  
    <a href="#llmcode-v0680" class="anchor-heading" aria-labelledby="llmcode-v0680"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.68.0
  
  
</h3>
    

<ul>
  <li><a href="https://llmcode.khulnasoft.com/docs/usage/copypaste.html">Llmcode works with LLM web chat UIs</a>.
    <ul>
      <li>New <code class="language-plaintext highlighter-rouge">--copy-paste</code> mode.</li>
      <li>New <code class="language-plaintext highlighter-rouge">/copy-context</code> command.</li>
    </ul>
  </li>
  <li><a href="https://llmcode.khulnasoft.com/docs/config/llmcode_conf.html#storing-llm-keys">Set API keys and other environment variables for all providers from command line or yaml conf file</a>.
    <ul>
      <li>New <code class="language-plaintext highlighter-rouge">--api-key provider=key</code> setting.</li>
      <li>New <code class="language-plaintext highlighter-rouge">--set-env VAR=value</code> setting.</li>
    </ul>
  </li>
  <li>Added bash and zsh support to <code class="language-plaintext highlighter-rouge">--watch-files</code>.</li>
  <li>Better error messages when missing dependencies for Gemini and Bedrock models.</li>
  <li>Control-D now properly exits the program.</li>
  <li>Don’t count token costs when API provider returns a hard error.</li>
  <li>Bugfix so watch files works with files that don’t have tree-sitter support.</li>
  <li>Bugfix so o1 models can be used as weak model.</li>
  <li>Updated shell command prompt.</li>
  <li>Added docstrings for all Coders.</li>
  <li>Reorganized command line arguments with improved help messages and grouping.</li>
  <li>Use the exact <code class="language-plaintext highlighter-rouge">sys.python</code> for self-upgrades.</li>
  <li>Added experimental Gemini models.</li>
  <li>Llmcode wrote 71% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0670">
  
  
    <a href="#llmcode-v0670" class="anchor-heading" aria-labelledby="llmcode-v0670"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.67.0
  
  
</h3>
    

<ul>
  <li><a href="https://llmcode.khulnasoft.com/docs/usage/watch.html">Use llmcode in your IDE or editor</a>.
    <ul>
      <li>Run <code class="language-plaintext highlighter-rouge">llmcode --watch-files</code> and it will watch for instructions you add to your source files.</li>
      <li>One-liner <code class="language-plaintext highlighter-rouge"># ...</code> or <code class="language-plaintext highlighter-rouge">// ...</code> comments that start or end with “AI” are instructions to llmcode.</li>
      <li>When llmcode sees “AI!” it reads and follows all the instructions in AI comments.</li>
    </ul>
  </li>
  <li>Support for new Amazon Bedrock Nova models.</li>
  <li>When <code class="language-plaintext highlighter-rouge">/run</code> or <code class="language-plaintext highlighter-rouge">/test</code> have non-zero exit codes, pre-fill “Fix that” into the next message prompt.</li>
  <li><code class="language-plaintext highlighter-rouge">/diff</code> now invokes <code class="language-plaintext highlighter-rouge">git diff</code> to use your preferred diff tool.</li>
  <li>Added Ctrl-Z support for process suspension.</li>
  <li>Spinner now falls back to ASCII art if fancy symbols throw unicode errors.</li>
  <li><code class="language-plaintext highlighter-rouge">--read</code> now expands <code class="language-plaintext highlighter-rouge">~</code> home dirs.</li>
  <li>Enabled exception capture in analytics.</li>
  <li><a href="https://llmcode.khulnasoft.com/HISTORY.html">Llmcode wrote 61% of the code in this release.</a></li>
</ul>
<h3 id="llmcode-v0660">
  
  
    <a href="#llmcode-v0660" class="anchor-heading" aria-labelledby="llmcode-v0660"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.66.0
  
  
</h3>
    

<ul>
  <li>PDF support for Sonnet and Gemini models.</li>
  <li>Added <code class="language-plaintext highlighter-rouge">--voice-input-device</code> to select audio input device for voice recording, by @preynal.</li>
  <li>Added <code class="language-plaintext highlighter-rouge">--timeout</code> option to configure API call timeouts.</li>
  <li>Set cwd to repo root when running shell commands.</li>
  <li>Added Ctrl-Up/Down keyboard shortcuts for per-message history navigation.</li>
  <li>Improved error handling for failed .gitignore file operations.</li>
  <li>Improved error handling for input history file permissions.</li>
  <li>Improved error handling for analytics file access.</li>
  <li>Removed spurious warning about disabling pretty in VSCode.</li>
  <li>Removed broken support for Dart.</li>
  <li>Bugfix when scraping URLs found in chat messages.</li>
  <li>Better handling of <strong>version</strong> import errors.</li>
  <li>Improved <code class="language-plaintext highlighter-rouge">/drop</code> command to support substring matching for non-glob patterns.</li>
  <li>Llmcode wrote 82% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0651">
  
  
    <a href="#llmcode-v0651" class="anchor-heading" aria-labelledby="llmcode-v0651"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.65.1
  
  
</h3>
    

<ul>
  <li>Bugfix to <code class="language-plaintext highlighter-rouge">--alias</code>.</li>
</ul>
<h3 id="llmcode-v0650">
  
  
    <a href="#llmcode-v0650" class="anchor-heading" aria-labelledby="llmcode-v0650"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.65.0
  
  
</h3>
    

<ul>
  <li>Added <code class="language-plaintext highlighter-rouge">--alias</code> config to define <a href="https://llmcode.khulnasoft.com/docs/config/model-aliases.html">custom model aliases</a>.</li>
  <li>Added <code class="language-plaintext highlighter-rouge">--[no-]detect-urls</code> flag to disable detecting and offering to scrape URLs found in the chat.</li>
  <li>Ollama models now default to an 8k context window.</li>
  <li>Added <a href="https://llmcode.khulnasoft.com/docs/languages.html">RepoMap support for Dart language</a> by @malkoG.</li>
  <li>Ask 2.5% of users if they want to opt-in to <a href="https://llmcode.khulnasoft.com/docs/more/analytics.html">analytics</a>.</li>
  <li>Skip suggesting files that share names with files already in chat.</li>
  <li><code class="language-plaintext highlighter-rouge">/editor</code> returns and prefill the file content into the prompt, so you can use <code class="language-plaintext highlighter-rouge">/editor</code> to compose messages that start with <code class="language-plaintext highlighter-rouge">/commands</code>, etc.</li>
  <li>Enhanced error handling for analytics.</li>
  <li>Improved handling of UnknownEditFormat exceptions with helpful documentation links.</li>
  <li>Bumped dependencies to pick up grep-ast 0.4.0 for Dart language support.</li>
  <li>Llmcode wrote 81% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0641">
  
  
    <a href="#llmcode-v0641" class="anchor-heading" aria-labelledby="llmcode-v0641"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.64.1
  
  
</h3>
    

<ul>
  <li>Disable streaming for o1 on OpenRouter.</li>
</ul>
<h3 id="llmcode-v0640">
  
  
    <a href="#llmcode-v0640" class="anchor-heading" aria-labelledby="llmcode-v0640"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.64.0
  
  
</h3>
    

<ul>
  <li>Added <a href="https://llmcode.khulnasoft.com/docs/usage/commands.html"><code class="language-plaintext highlighter-rouge">/editor</code> command</a> to open system editor for writing prompts, by @thehunmonkgroup.</li>
  <li>Full support for <code class="language-plaintext highlighter-rouge">gpt-4o-2024-11-20</code>.</li>
  <li>Stream o1 models by default.</li>
  <li><code class="language-plaintext highlighter-rouge">/run</code> and suggested shell commands are less mysterious and now confirm that they “Added XX lines of output to the chat.”</li>
  <li>Ask 1% of users if they want to opt-in to <a href="https://llmcode.khulnasoft.com/docs/more/analytics.html">analytics</a>.</li>
  <li>Added support for <a href="https://llmcode.khulnasoft.com/docs/usage/commands.html#entering-multi-line-chat-messages">optional multiline input tags</a> with matching closing tags.</li>
  <li>Improved <a href="https://llmcode.khulnasoft.com/docs/config/adv-model-settings.html#global-extra-params">model settings configuration</a> with support for global <code class="language-plaintext highlighter-rouge">extra_params</code> for <code class="language-plaintext highlighter-rouge">litellm.completion()</code>.</li>
  <li>Architect mode now asks to add files suggested by the LLM.</li>
  <li>Fixed bug in fuzzy model name matching.</li>
  <li>Added Timeout exception to handle API provider timeouts.</li>
  <li>Added <code class="language-plaintext highlighter-rouge">--show-release-notes</code> to control release notes display on first run of new version.</li>
  <li>Save empty dict to cache file on model metadata download failure, to delay retry.</li>
  <li>Improved error handling and code formatting.</li>
  <li>Llmcode wrote 74% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0632">
  
  
    <a href="#llmcode-v0632" class="anchor-heading" aria-labelledby="llmcode-v0632"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.63.2
  
  
</h3>
    

<ul>
  <li>Fixed bug in fuzzy model name matching when litellm provider info is missing.</li>
  <li>Modified model metadata file loading to allow override of resource file.</li>
  <li>Allow recursive loading of dirs using <code class="language-plaintext highlighter-rouge">--read</code>.</li>
  <li>Updated dependency versions to pick up litellm fix for ollama models.</li>
  <li>Added exponential backoff retry when writing files to handle editor file locks.</li>
  <li>Updated Qwen 2.5 Coder 32B model configuration.</li>
</ul>
<h3 id="llmcode-v0631">
  
  
    <a href="#llmcode-v0631" class="anchor-heading" aria-labelledby="llmcode-v0631"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.63.1
  
  
</h3>
    

<ul>
  <li>Fixed bug in git ignored file handling.</li>
  <li>Improved error handling for git operations.</li>
</ul>
<h3 id="llmcode-v0630">
  
  
    <a href="#llmcode-v0630" class="anchor-heading" aria-labelledby="llmcode-v0630"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.63.0
  
  
</h3>
    

<ul>
  <li>Support for Qwen 2.5 Coder 32B.</li>
  <li><code class="language-plaintext highlighter-rouge">/web</code> command just adds the page to the chat, without triggering an LLM response.</li>
  <li>Improved prompting for the user’s preferred chat language.</li>
  <li>Improved handling of LiteLLM exceptions.</li>
  <li>Bugfix for double-counting tokens when reporting cache stats.</li>
  <li>Bugfix for the LLM creating new files.</li>
  <li>Other small bug fixes.</li>
  <li>Llmcode wrote 55% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0620">
  
  
    <a href="#llmcode-v0620" class="anchor-heading" aria-labelledby="llmcode-v0620"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.62.0
  
  
</h3>
    

<ul>
  <li>Full support for Claude 3.5 Haiku
    <ul>
      <li>Scored 75% on <a href="https://llmcode.khulnasoft.com/docs/leaderboards/">llmcode’s code editing leaderboard</a>.</li>
      <li>Almost as good as Sonnet at much lower cost.</li>
      <li>Launch with <code class="language-plaintext highlighter-rouge">--haiku</code> to use it.</li>
    </ul>
  </li>
  <li>Easily apply file edits from ChatGPT, Claude or other web apps
    <ul>
      <li>Chat with ChatGPT or Claude via their web app.</li>
      <li>Give it your source files and ask for the changes you want.</li>
      <li>Use the web app’s “copy response” button to copy the entire reply from the LLM.</li>
      <li>Run <code class="language-plaintext highlighter-rouge">llmcode --apply-clipboard-edits file-to-edit.js</code>.</li>
      <li>Llmcode will edit your file with the LLM’s changes.</li>
    </ul>
  </li>
  <li>Bugfix for creating new files.</li>
  <li>Llmcode wrote 84% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0610">
  
  
    <a href="#llmcode-v0610" class="anchor-heading" aria-labelledby="llmcode-v0610"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.61.0
  
  
</h3>
    

<ul>
  <li>Load and save llmcode slash-commands to files:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">/save &lt;fname&gt;</code> command will make a file of <code class="language-plaintext highlighter-rouge">/add</code> and <code class="language-plaintext highlighter-rouge">/read-only</code> commands that recreate the current file context in the chat.</li>
      <li><code class="language-plaintext highlighter-rouge">/load &lt;fname&gt;</code> will replay the commands in the file.</li>
      <li>You can use <code class="language-plaintext highlighter-rouge">/load</code> to run any arbitrary set of slash-commands, not just <code class="language-plaintext highlighter-rouge">/add</code> and <code class="language-plaintext highlighter-rouge">/read-only</code>.</li>
      <li>Use <code class="language-plaintext highlighter-rouge">--load &lt;fname&gt;</code> to run a list of commands on launch, before the interactive chat begins.</li>
    </ul>
  </li>
  <li>Anonymous, opt-in <a href="https://llmcode.khulnasoft.com/docs/more/analytics.html">analytics</a> with no personal data sharing.</li>
  <li>Llmcode follows litellm’s <code class="language-plaintext highlighter-rouge">supports_vision</code> attribute to enable image support for models.</li>
  <li>Bugfix for when diff mode flexibly handles the model using the wrong filename.</li>
  <li>Displays filenames in sorted order for <code class="language-plaintext highlighter-rouge">/add</code> and <code class="language-plaintext highlighter-rouge">/read-only</code>.</li>
  <li>New <code class="language-plaintext highlighter-rouge">--no-fancy-input</code> switch disables prompt toolkit input, now still available with <code class="language-plaintext highlighter-rouge">--no-pretty</code>.</li>
  <li>Override browser config with <code class="language-plaintext highlighter-rouge">--no-browser</code> or <code class="language-plaintext highlighter-rouge">--no-gui</code>.</li>
  <li>Offer to open documentation URLs when errors occur.</li>
  <li>Properly support all o1 models, regardless of provider.</li>
  <li>Improved layout of filenames above input prompt.</li>
  <li>Better handle corrupted repomap tags cache.</li>
  <li>Improved handling of API errors, especially when accessing the weak model.</li>
  <li>Llmcode wrote 68% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0601">
  
  
    <a href="#llmcode-v0601" class="anchor-heading" aria-labelledby="llmcode-v0601"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.60.1
  
  
</h3>
    

<ul>
  <li>Enable image support for Sonnet 10/22.</li>
  <li>Display filenames in sorted order.</li>
</ul>
<h3 id="llmcode-v0600">
  
  
    <a href="#llmcode-v0600" class="anchor-heading" aria-labelledby="llmcode-v0600"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.60.0
  
  
</h3>
    

<ul>
  <li>Full support for Sonnet 10/22, the new SOTA model on llmcode’s code editing benchmark.
    <ul>
      <li>Llmcode uses Sonnet 10/22 by default.</li>
    </ul>
  </li>
  <li>Improved formatting of added and read-only files above chat prompt, by @jbellis.</li>
  <li>Improved support for o1 models by more flexibly parsing their nonconforming code edit replies.</li>
  <li>Corrected diff edit format prompt that only the first match is replaced.</li>
  <li>Stronger whole edit format prompt asking for clean file names.</li>
  <li>Now offers to add <code class="language-plaintext highlighter-rouge">.env</code> to the <code class="language-plaintext highlighter-rouge">.gitignore</code> file.</li>
  <li>Ships with a small model metadata json file to handle models not yet updated in litellm.</li>
  <li>Model settings for o1 models on azure.</li>
  <li>Bugfix to properly include URLs in <code class="language-plaintext highlighter-rouge">/help</code> RAG results.</li>
  <li>Llmcode wrote 49% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0591">
  
  
    <a href="#llmcode-v0591" class="anchor-heading" aria-labelledby="llmcode-v0591"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.59.1
  
  
</h3>
    

<ul>
  <li>Check for obsolete <code class="language-plaintext highlighter-rouge">yes: true</code> in yaml config, show helpful error.</li>
  <li>Model settings for openrouter/anthropic/claude-3.5-sonnet:beta</li>
</ul>
<h3 id="llmcode-v0590">
  
  
    <a href="#llmcode-v0590" class="anchor-heading" aria-labelledby="llmcode-v0590"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.59.0
  
  
</h3>
    

<ul>
  <li>Improvements to <code class="language-plaintext highlighter-rouge">/read-only</code>:
    <ul>
      <li>Now supports shell-style auto-complete of the full file system.</li>
      <li>Still auto-completes the full paths of the repo files like <code class="language-plaintext highlighter-rouge">/add</code>.</li>
      <li>Now supports globs like <code class="language-plaintext highlighter-rouge">src/**/*.py</code></li>
    </ul>
  </li>
  <li>Renamed <code class="language-plaintext highlighter-rouge">--yes</code> to <code class="language-plaintext highlighter-rouge">--yes-always</code>.
    <ul>
      <li>Now uses <code class="language-plaintext highlighter-rouge">LLMCODE_YES_ALWAYS</code> env var and <code class="language-plaintext highlighter-rouge">yes-always:</code> yaml key.</li>
      <li>Existing YAML and .env files will need to be updated.</li>
      <li>Can still abbreviate to <code class="language-plaintext highlighter-rouge">--yes</code> on the command line.</li>
    </ul>
  </li>
  <li>Config file now uses standard YAML list syntax with `  - list entries`, one per line.</li>
  <li><code class="language-plaintext highlighter-rouge">/settings</code> now includes the same announcement lines that would print at launch.</li>
  <li>Sanity checks the <code class="language-plaintext highlighter-rouge">--editor-model</code> on launch now, same as main and weak models.</li>
  <li>Added <code class="language-plaintext highlighter-rouge">--skip-sanity-check-repo</code> switch to speedup launch in large repos.</li>
  <li>Bugfix so architect mode handles Control-C properly.</li>
  <li>Repo-map is deterministic now, with improved caching logic.</li>
  <li>Improved commit message prompt.</li>
  <li>Llmcode wrote 77% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0581">
  
  
    <a href="#llmcode-v0581" class="anchor-heading" aria-labelledby="llmcode-v0581"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.58.1
  
  
</h3>
    

<ul>
  <li>Fixed bug where cache warming pings caused subsequent user messages to trigger a tight loop of LLM requests.</li>
</ul>
<h3 id="llmcode-v0580">
  
  
    <a href="#llmcode-v0580" class="anchor-heading" aria-labelledby="llmcode-v0580"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.58.0
  
  
</h3>
    

<ul>
  <li><a href="https://llmcode.khulnasoft.com/2024/09/26/architect.html">Use a pair of Architect/Editor models for improved coding</a>
    <ul>
      <li>Use a strong reasoning model like o1-preview as your Architect.</li>
      <li>Use a cheaper, faster model like gpt-4o as your Editor.</li>
    </ul>
  </li>
  <li>New <code class="language-plaintext highlighter-rouge">--o1-preview</code> and <code class="language-plaintext highlighter-rouge">--o1-mini</code> shortcuts.</li>
  <li>Support for new Gemini 002 models.</li>
  <li>Better support for Qwen 2.5 models.</li>
  <li>Many confirmation questions can be skipped for the rest of the session with “(D)on’t ask again” response.</li>
  <li>Autocomplete for <code class="language-plaintext highlighter-rouge">/read-only</code> supports the entire filesystem.</li>
  <li>New settings for completion menu colors.</li>
  <li>New <code class="language-plaintext highlighter-rouge">/copy</code> command to copy the last LLM response to the clipboard.</li>
  <li>Renamed <code class="language-plaintext highlighter-rouge">/clipboard</code> to <code class="language-plaintext highlighter-rouge">/paste</code>.</li>
  <li>Will now follow HTTP redirects when scraping urls.</li>
  <li>New <code class="language-plaintext highlighter-rouge">--voice-format</code> switch to send voice audio as wav/mp3/webm, by @mbailey.</li>
  <li>ModelSettings takes <code class="language-plaintext highlighter-rouge">extra_params</code> dict to specify any extras to pass to <code class="language-plaintext highlighter-rouge">litellm.completion()</code>.</li>
  <li>Support for cursor shapes when in vim mode.</li>
  <li>Numerous bug fixes.</li>
  <li>Llmcode wrote 53% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0571">
  
  
    <a href="#llmcode-v0571" class="anchor-heading" aria-labelledby="llmcode-v0571"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.57.1
  
  
</h3>
    

<ul>
  <li>Fixed dependency conflict between llmcode[help] and [playwright].</li>
</ul>
<h3 id="llmcode-v0570">
  
  
    <a href="#llmcode-v0570" class="anchor-heading" aria-labelledby="llmcode-v0570"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.57.0
  
  
</h3>
    

<ul>
  <li>Support for OpenAI o1 models:
    <ul>
      <li>o1-preview now works well with diff edit format.</li>
      <li>o1-preview with diff now matches SOTA leaderboard result with whole edit format.</li>
      <li><code class="language-plaintext highlighter-rouge">llmcode --model o1-mini</code></li>
      <li><code class="language-plaintext highlighter-rouge">llmcode --model o1-preview</code></li>
    </ul>
  </li>
  <li>On Windows, <code class="language-plaintext highlighter-rouge">/run</code> correctly uses PowerShell or cmd.exe.</li>
  <li>Support for new 08-2024 Cohere models, by @jalammar.</li>
  <li>Can now recursively add directories with <code class="language-plaintext highlighter-rouge">/read-only</code>.</li>
  <li>User input prompts now fall back to simple <code class="language-plaintext highlighter-rouge">input()</code> if <code class="language-plaintext highlighter-rouge">--no-pretty</code> or a Windows console is not available.</li>
  <li>Improved sanity check of git repo on startup.</li>
  <li>Improvements to prompt cache chunking strategy.</li>
  <li>Removed “No changes made to git tracked files”.</li>
  <li>Numerous bug fixes for corner case crashes.</li>
  <li>Updated all dependency versions.</li>
  <li>Llmcode wrote 70% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0560">
  
  
    <a href="#llmcode-v0560" class="anchor-heading" aria-labelledby="llmcode-v0560"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.56.0
  
  
</h3>
    

<ul>
  <li>Enables prompt caching for Sonnet via OpenRouter by @fry69</li>
  <li>Enables 8k output tokens for Sonnet via VertexAI and DeepSeek V2.5.</li>
  <li>New <code class="language-plaintext highlighter-rouge">/report</code> command to open your browser with a pre-populated GitHub Issue.</li>
  <li>New <code class="language-plaintext highlighter-rouge">--chat-language</code> switch to set the spoken language.</li>
  <li>Now <code class="language-plaintext highlighter-rouge">--[no-]suggest-shell-commands</code> controls both prompting for and offering to execute shell commands.</li>
  <li>Check key imports on launch, provide helpful error message if dependencies aren’t available.</li>
  <li>Renamed <code class="language-plaintext highlighter-rouge">--models</code> to <code class="language-plaintext highlighter-rouge">--list-models</code> by @fry69.</li>
  <li>Numerous bug fixes for corner case crashes.</li>
  <li>Llmcode wrote 56% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0550">
  
  
    <a href="#llmcode-v0550" class="anchor-heading" aria-labelledby="llmcode-v0550"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.55.0
  
  
</h3>
    

<ul>
  <li>Only print the pip command when self updating on Windows, without running it.</li>
  <li>Converted many error messages to warning messages.</li>
  <li>Added <code class="language-plaintext highlighter-rouge">--tool-warning-color</code> setting.</li>
  <li>Blanket catch and handle git errors in any <code class="language-plaintext highlighter-rouge">/command</code>.</li>
  <li>Catch and handle glob errors in <code class="language-plaintext highlighter-rouge">/add</code>, errors writing files.</li>
  <li>Disabled built in linter for typescript.</li>
  <li>Catch and handle terminals which don’t support pretty output.</li>
  <li>Catch and handle playwright and pandoc errors.</li>
  <li>Catch <code class="language-plaintext highlighter-rouge">/voice</code> transcription exceptions, show the WAV file so the user can recover it.</li>
  <li>Llmcode wrote 53% of the code in this release.</li>
</ul>
<h3 id="llmcode-v05412">
  
  
    <a href="#llmcode-v05412" class="anchor-heading" aria-labelledby="llmcode-v05412"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.54.12
  
  
</h3>
    

<ul>
  <li>Switched to <code class="language-plaintext highlighter-rouge">vX.Y.Z.dev</code> version naming.</li>
</ul>
<h3 id="llmcode-v05411">
  
  
    <a href="#llmcode-v05411" class="anchor-heading" aria-labelledby="llmcode-v05411"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.54.11
  
  
</h3>
    

<ul>
  <li>Improved printed pip command output on Windows.</li>
</ul>
<h3 id="llmcode-v05410">
  
  
    <a href="#llmcode-v05410" class="anchor-heading" aria-labelledby="llmcode-v05410"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.54.10
  
  
</h3>
    

<ul>
  <li>Bugfix to test command in platform info.</li>
</ul>
<h3 id="llmcode-v0549">
  
  
    <a href="#llmcode-v0549" class="anchor-heading" aria-labelledby="llmcode-v0549"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.54.9
  
  
</h3>
    

<ul>
  <li>Include important devops files in the repomap.</li>
  <li>Print quoted pip install commands to the user.</li>
  <li>Adopt setuptools_scm to provide dev versions with git hashes.</li>
  <li>Share active test and lint commands with the LLM.</li>
  <li>Catch and handle most errors creating new files, reading existing files.</li>
  <li>Catch and handle most git errors.</li>
  <li>Added –verbose debug output for shell commands.</li>
</ul>
<h3 id="llmcode-v0548">
  
  
    <a href="#llmcode-v0548" class="anchor-heading" aria-labelledby="llmcode-v0548"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.54.8
  
  
</h3>
    

<ul>
  <li>Startup QOL improvements:
    <ul>
      <li>Sanity check the git repo and exit gracefully on problems.</li>
      <li>Pause for confirmation after model sanity check to allow user to review warnings.</li>
    </ul>
  </li>
  <li>Bug fix for shell commands on Windows.</li>
  <li>Do not fuzzy match filenames when LLM is creating a new file, by @ozapinq</li>
  <li>Numerous corner case bug fixes submitted via new crash report -&gt; GitHub Issue feature.</li>
  <li>Crash reports now include python version, OS, etc.</li>
</ul>
<h3 id="llmcode-v0547">
  
  
    <a href="#llmcode-v0547" class="anchor-heading" aria-labelledby="llmcode-v0547"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.54.7
  
  
</h3>
    

<ul>
  <li>Offer to submit a GitHub issue pre-filled with uncaught exception info.</li>
  <li>Bugfix for infinite output.</li>
</ul>
<h3 id="llmcode-v0546">
  
  
    <a href="#llmcode-v0546" class="anchor-heading" aria-labelledby="llmcode-v0546"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.54.6
  
  
</h3>
    

<ul>
  <li>New <code class="language-plaintext highlighter-rouge">/settings</code> command to show active settings.</li>
  <li>Only show cache warming status update if <code class="language-plaintext highlighter-rouge">--verbose</code>.</li>
</ul>
<h3 id="llmcode-v0545">
  
  
    <a href="#llmcode-v0545" class="anchor-heading" aria-labelledby="llmcode-v0545"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.54.5
  
  
</h3>
    

<ul>
  <li>Bugfix for shell commands on Windows.</li>
  <li>Refuse to make git repo in $HOME, warn user.</li>
  <li>Don’t ask again in current session about a file the user has said not to add to the chat.</li>
  <li>Added <code class="language-plaintext highlighter-rouge">--update</code> as an alias for <code class="language-plaintext highlighter-rouge">--upgrade</code>.</li>
</ul>
<h3 id="llmcode-v0544">
  
  
    <a href="#llmcode-v0544" class="anchor-heading" aria-labelledby="llmcode-v0544"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.54.4
  
  
</h3>
    

<ul>
  <li>Bugfix to completions for <code class="language-plaintext highlighter-rouge">/model</code> command.</li>
  <li>Bugfix: revert home dir special case.</li>
</ul>
<h3 id="llmcode-v0543">
  
  
    <a href="#llmcode-v0543" class="anchor-heading" aria-labelledby="llmcode-v0543"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.54.3
  
  
</h3>
    

<ul>
  <li>Dependency <code class="language-plaintext highlighter-rouge">watchdog&lt;5</code> for docker image.</li>
</ul>
<h3 id="llmcode-v0542">
  
  
    <a href="#llmcode-v0542" class="anchor-heading" aria-labelledby="llmcode-v0542"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.54.2
  
  
</h3>
    

<ul>
  <li>When users launch llmcode in their home dir, help them find/create a repo in a subdir.</li>
  <li>Added missing <code class="language-plaintext highlighter-rouge">pexpect</code> dependency.</li>
</ul>
<h3 id="llmcode-v0540">
  
  
    <a href="#llmcode-v0540" class="anchor-heading" aria-labelledby="llmcode-v0540"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.54.0
  
  
</h3>
    

<ul>
  <li>Added model settings for <code class="language-plaintext highlighter-rouge">gemini/gemini-1.5-pro-exp-0827</code> and <code class="language-plaintext highlighter-rouge">gemini/gemini-1.5-flash-exp-0827</code>.</li>
  <li>Shell and <code class="language-plaintext highlighter-rouge">/run</code> commands can now be interactive in environments where a pty is available.</li>
  <li>Optionally share output of suggested shell commands back to the LLM.</li>
  <li>New <code class="language-plaintext highlighter-rouge">--[no-]suggest-shell-commands</code> switch to configure shell commands.</li>
  <li>Performance improvements for autocomplete in large/mono repos.</li>
  <li>New <code class="language-plaintext highlighter-rouge">--upgrade</code> switch to install latest version of llmcode from pypi.</li>
  <li>Bugfix to <code class="language-plaintext highlighter-rouge">--show-prompt</code>.</li>
  <li>Disabled automatic reply to the LLM on <code class="language-plaintext highlighter-rouge">/undo</code> for all models.</li>
  <li>Removed pager from <code class="language-plaintext highlighter-rouge">/web</code> output.</li>
  <li>Llmcode wrote 64% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0530">
  
  
    <a href="#llmcode-v0530" class="anchor-heading" aria-labelledby="llmcode-v0530"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.53.0
  
  
</h3>
    

<ul>
  <li><a href="https://llmcode.khulnasoft.com/docs/usage/caching.html#preventing-cache-expiration">Keep your prompt cache from expiring</a> with <code class="language-plaintext highlighter-rouge">--cache-keepalive-pings</code>.
    <ul>
      <li>Pings the API every 5min to keep the cache warm.</li>
    </ul>
  </li>
  <li>You can now bulk accept/reject a series of add url and run shell confirmations.</li>
  <li>Improved matching of filenames from S/R blocks with files in chat.</li>
  <li>Stronger prompting for Sonnet to make edits in code chat mode.</li>
  <li>Stronger prompting for the LLM to specify full file paths.</li>
  <li>Improved shell command prompting.</li>
  <li>Weak model now uses <code class="language-plaintext highlighter-rouge">extra_headers</code>, to support Anthropic beta features.</li>
  <li>New <code class="language-plaintext highlighter-rouge">--install-main-branch</code> to update to the latest dev version of llmcode.</li>
  <li>Improved error messages on attempt to add not-git subdir to chat.</li>
  <li>Show model metadata info on <code class="language-plaintext highlighter-rouge">--verbose</code>.</li>
  <li>Improved warnings when LLMs env variables aren’t set.</li>
  <li>Bugfix to windows filenames which contain <code class="language-plaintext highlighter-rouge">\_</code>.</li>
  <li>Llmcode wrote 59% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0521">
  
  
    <a href="#llmcode-v0521" class="anchor-heading" aria-labelledby="llmcode-v0521"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.52.1
  
  
</h3>
    

<ul>
  <li>Bugfix for NameError when applying edits.</li>
</ul>
<h3 id="llmcode-v0520">
  
  
    <a href="#llmcode-v0520" class="anchor-heading" aria-labelledby="llmcode-v0520"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.52.0
  
  
</h3>
    

<ul>
  <li>Llmcode now offers to run shell commands:
    <ul>
      <li>Launch a browser to view updated html/css/js.</li>
      <li>Install new dependencies.</li>
      <li>Run DB migrations.</li>
      <li>Run the program to exercise changes.</li>
      <li>Run new test cases.</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">/read</code> and <code class="language-plaintext highlighter-rouge">/drop</code> now expand <code class="language-plaintext highlighter-rouge">~</code> to the home dir.</li>
  <li>Show the active chat mode at llmcode prompt.</li>
  <li>New <code class="language-plaintext highlighter-rouge">/reset</code> command to <code class="language-plaintext highlighter-rouge">/drop</code> files and <code class="language-plaintext highlighter-rouge">/clear</code> chat history.</li>
  <li>New <code class="language-plaintext highlighter-rouge">--map-multiplier-no-files</code> to control repo map size multiplier when no files are in the chat.
    <ul>
      <li>Reduced default multiplier to 2.</li>
    </ul>
  </li>
  <li>Bugfixes and improvements to auto commit sequencing.</li>
  <li>Improved formatting of token reports and confirmation dialogs.</li>
  <li>Default OpenAI model is now <code class="language-plaintext highlighter-rouge">gpt-4o-2024-08-06</code>.</li>
  <li>Bumped dependencies to pickup litellm bugfixes.</li>
  <li>Llmcode wrote 68% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0510">
  
  
    <a href="#llmcode-v0510" class="anchor-heading" aria-labelledby="llmcode-v0510"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.51.0
  
  
</h3>
    

<ul>
  <li>Prompt caching for Anthropic models with <code class="language-plaintext highlighter-rouge">--cache-prompts</code>.
    <ul>
      <li>Caches the system prompt, repo map and <code class="language-plaintext highlighter-rouge">/read-only</code> files.</li>
    </ul>
  </li>
  <li>Repo map recomputes less often in large/mono repos or when caching enabled.
    <ul>
      <li>Use <code class="language-plaintext highlighter-rouge">--map-refresh &lt;always|files|manual|auto&gt;</code> to configure.</li>
    </ul>
  </li>
  <li>Improved cost estimate logic for caching.</li>
  <li>Improved editing performance on Jupyter Notebook <code class="language-plaintext highlighter-rouge">.ipynb</code> files.</li>
  <li>Show which config yaml file is loaded with <code class="language-plaintext highlighter-rouge">--verbose</code>.</li>
  <li>Bumped dependency versions.</li>
  <li>Bugfix: properly load <code class="language-plaintext highlighter-rouge">.llmcode.models.metadata.json</code> data.</li>
  <li>Bugfix: Using <code class="language-plaintext highlighter-rouge">--msg /ask ...</code> caused an exception.</li>
  <li>Bugfix: litellm tokenizer bug for images.</li>
  <li>Llmcode wrote 56% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0501">
  
  
    <a href="#llmcode-v0501" class="anchor-heading" aria-labelledby="llmcode-v0501"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.50.1
  
  
</h3>
    

<ul>
  <li>Bugfix for provider API exceptions.</li>
</ul>
<h3 id="llmcode-v0500">
  
  
    <a href="#llmcode-v0500" class="anchor-heading" aria-labelledby="llmcode-v0500"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.50.0
  
  
</h3>
    

<ul>
  <li>Infinite output for DeepSeek Coder, Mistral models in addition to Anthropic’s models.</li>
  <li>New <code class="language-plaintext highlighter-rouge">--deepseek</code> switch to use DeepSeek Coder.</li>
  <li>DeepSeek Coder uses 8k token output.</li>
  <li>New <code class="language-plaintext highlighter-rouge">--chat-mode &lt;mode&gt;</code> switch to launch in ask/help/code modes.</li>
  <li>New <code class="language-plaintext highlighter-rouge">/code &lt;message&gt;</code> command request a code edit while in <code class="language-plaintext highlighter-rouge">ask</code> mode.</li>
  <li>Web scraper is more robust if page never idles.</li>
  <li>Improved token and cost reporting for infinite output.</li>
  <li>Improvements and bug fixes for <code class="language-plaintext highlighter-rouge">/read</code> only files.</li>
  <li>Switched from <code class="language-plaintext highlighter-rouge">setup.py</code> to <code class="language-plaintext highlighter-rouge">pyproject.toml</code>, by @branchvincent.</li>
  <li>Bug fix to persist files added during <code class="language-plaintext highlighter-rouge">/ask</code>.</li>
  <li>Bug fix for chat history size in <code class="language-plaintext highlighter-rouge">/tokens</code>.</li>
  <li>Llmcode wrote 66% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0491">
  
  
    <a href="#llmcode-v0491" class="anchor-heading" aria-labelledby="llmcode-v0491"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.49.1
  
  
</h3>
    

<ul>
  <li>Bugfix to <code class="language-plaintext highlighter-rouge">/help</code>.</li>
</ul>
<h3 id="llmcode-v0490">
  
  
    <a href="#llmcode-v0490" class="anchor-heading" aria-labelledby="llmcode-v0490"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.49.0
  
  
</h3>
    

<ul>
  <li>Add read-only files to the chat context with <code class="language-plaintext highlighter-rouge">/read</code> and <code class="language-plaintext highlighter-rouge">--read</code>,  including from outside the git repo.</li>
  <li><code class="language-plaintext highlighter-rouge">/diff</code> now shows diffs of all changes resulting from your request, including lint and test fixes.</li>
  <li>New <code class="language-plaintext highlighter-rouge">/clipboard</code> command to paste images or text from the clipboard, replaces <code class="language-plaintext highlighter-rouge">/add-clipboard-image</code>.</li>
  <li>Now shows the markdown scraped when you add a url with <code class="language-plaintext highlighter-rouge">/web</code>.</li>
  <li>When <a href="https://llmcode.khulnasoft.com/docs/scripting.html">scripting llmcode</a> messages can now contain in-chat <code class="language-plaintext highlighter-rouge">/</code> commands.</li>
  <li>Llmcode in docker image now suggests the correct command to update to latest version.</li>
  <li>Improved retries on API errors (was easy to test during Sonnet outage).</li>
  <li>Added <code class="language-plaintext highlighter-rouge">--mini</code> for <code class="language-plaintext highlighter-rouge">gpt-4o-mini</code>.</li>
  <li>Bugfix to keep session cost accurate when using <code class="language-plaintext highlighter-rouge">/ask</code> and <code class="language-plaintext highlighter-rouge">/help</code>.</li>
  <li>Performance improvements for repo map calculation.</li>
  <li><code class="language-plaintext highlighter-rouge">/tokens</code> now shows the active model.</li>
  <li>Enhanced commit message attribution options:
    <ul>
      <li>New <code class="language-plaintext highlighter-rouge">--attribute-commit-message-author</code> to prefix commit messages with ‘llmcode: ‘ if llmcode authored the changes, replaces <code class="language-plaintext highlighter-rouge">--attribute-commit-message</code>.</li>
      <li>New <code class="language-plaintext highlighter-rouge">--attribute-commit-message-committer</code> to prefix all commit messages with ‘llmcode: ‘.</li>
    </ul>
  </li>
  <li>Llmcode wrote 61% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0481">
  
  
    <a href="#llmcode-v0481" class="anchor-heading" aria-labelledby="llmcode-v0481"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.48.1
  
  
</h3>
    

<ul>
  <li>Added <code class="language-plaintext highlighter-rouge">openai/gpt-4o-2024-08-06</code>.</li>
  <li>Worked around litellm bug that removes OpenRouter app headers when using <code class="language-plaintext highlighter-rouge">extra_headers</code>.</li>
  <li>Improved progress indication during repo map processing.</li>
  <li>Corrected instructions for upgrading the docker container to latest llmcode version.</li>
  <li>Removed obsolete 16k token limit on commit diffs, use per-model limits.</li>
</ul>
<h3 id="llmcode-v0480">
  
  
    <a href="#llmcode-v0480" class="anchor-heading" aria-labelledby="llmcode-v0480"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.48.0
  
  
</h3>
    

<ul>
  <li>Performance improvements for large/mono repos.</li>
  <li>Added <code class="language-plaintext highlighter-rouge">--subtree-only</code> to limit llmcode to current directory subtree.
    <ul>
      <li>Should help with large/mono repo performance.</li>
    </ul>
  </li>
  <li>New <code class="language-plaintext highlighter-rouge">/add-clipboard-image</code> to add images to the chat from your clipboard.</li>
  <li>Use <code class="language-plaintext highlighter-rouge">--map-tokens 1024</code> to use repo map with any model.</li>
  <li>Support for Sonnet’s 8k output window.
    <ul>
      <li><a href="https://llmcode.khulnasoft.com/2024/07/01/sonnet-not-lazy.html">Llmcode already supported infinite output from Sonnet.</a></li>
    </ul>
  </li>
  <li>Workaround litellm bug for retrying API server errors.</li>
  <li>Upgraded dependencies, to pick up litellm bug fixes.</li>
  <li>Llmcode wrote 44% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0471">
  
  
    <a href="#llmcode-v0471" class="anchor-heading" aria-labelledby="llmcode-v0471"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.47.1
  
  
</h3>
    

<ul>
  <li>Improvements to conventional commits prompting.</li>
</ul>
<h3 id="llmcode-v0470">
  
  
    <a href="#llmcode-v0470" class="anchor-heading" aria-labelledby="llmcode-v0470"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.47.0
  
  
</h3>
    

<ul>
  <li><a href="https://llmcode.khulnasoft.com/docs/git.html#commit-messages">Commit message</a> improvements:
    <ul>
      <li>Added Conventional Commits guidelines to commit message prompt.</li>
      <li>Added <code class="language-plaintext highlighter-rouge">--commit-prompt</code> to customize the commit message prompt.</li>
      <li>Added strong model as a fallback for commit messages (and chat summaries).</li>
    </ul>
  </li>
  <li><a href="https://llmcode.khulnasoft.com/docs/usage/lint-test.html">Linting</a> improvements:
    <ul>
      <li>Ask before fixing lint errors.</li>
      <li>Improved performance of <code class="language-plaintext highlighter-rouge">--lint</code> on all dirty files in repo.</li>
      <li>Improved lint flow, now doing code edit auto-commit before linting.</li>
      <li>Bugfix to properly handle subprocess encodings (also for <code class="language-plaintext highlighter-rouge">/run</code>).</li>
    </ul>
  </li>
  <li>Improved <a href="https://llmcode.khulnasoft.com/docs/install/docker.html">docker support</a>:
    <ul>
      <li>Resolved permission issues when using <code class="language-plaintext highlighter-rouge">docker run --user xxx</code>.</li>
      <li>New <code class="language-plaintext highlighter-rouge">khulnasoft/llmcode-full</code> docker image, which includes all extras.</li>
    </ul>
  </li>
  <li>Switching to code and ask mode no longer summarizes the chat history.</li>
  <li>Added graph of llmcode’s contribution to each release.</li>
  <li>Generic auto-completions are provided for <code class="language-plaintext highlighter-rouge">/commands</code> without a completion override.</li>
  <li>Fixed broken OCaml tags file.</li>
  <li>Bugfix in <code class="language-plaintext highlighter-rouge">/run</code> add to chat approval logic.</li>
  <li>Llmcode wrote 58% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0461">
  
  
    <a href="#llmcode-v0461" class="anchor-heading" aria-labelledby="llmcode-v0461"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.46.1
  
  
</h3>
    

<ul>
  <li>Downgraded stray numpy dependency back to 1.26.4.</li>
</ul>
<h3 id="llmcode-v0460">
  
  
    <a href="#llmcode-v0460" class="anchor-heading" aria-labelledby="llmcode-v0460"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.46.0
  
  
</h3>
    

<ul>
  <li>New <code class="language-plaintext highlighter-rouge">/ask &lt;question&gt;</code> command to ask about your code, without making any edits.</li>
  <li>New <code class="language-plaintext highlighter-rouge">/chat-mode &lt;mode&gt;</code> command to switch chat modes:
    <ul>
      <li>ask: Ask questions about your code without making any changes.</li>
      <li>code: Ask for changes to your code (using the best edit format).</li>
      <li>help: Get help about using llmcode (usage, config, troubleshoot).</li>
    </ul>
  </li>
  <li>Add <code class="language-plaintext highlighter-rouge">file: CONVENTIONS.md</code> to <code class="language-plaintext highlighter-rouge">.llmcode.conf.yml</code> to always load a specific file.
    <ul>
      <li>Or <code class="language-plaintext highlighter-rouge">file: [file1, file2, file3]</code> to always load multiple files.</li>
    </ul>
  </li>
  <li>Enhanced token usage and cost reporting. Now works when streaming too.</li>
  <li>Filename auto-complete for <code class="language-plaintext highlighter-rouge">/add</code> and <code class="language-plaintext highlighter-rouge">/drop</code> is now case-insensitive.</li>
  <li>Commit message improvements:
    <ul>
      <li>Updated commit message prompt to use imperative tense.</li>
      <li>Fall back to main model if weak model is unable to generate a commit message.</li>
    </ul>
  </li>
  <li>Stop llmcode from asking to add the same url to the chat multiple times.</li>
  <li>Updates and fixes to <code class="language-plaintext highlighter-rouge">--no-verify-ssl</code>:
    <ul>
      <li>Fixed regression that broke it in v0.42.0.</li>
      <li>Disables SSL certificate verification when <code class="language-plaintext highlighter-rouge">/web</code> scrapes websites.</li>
    </ul>
  </li>
  <li>Improved error handling and reporting in <code class="language-plaintext highlighter-rouge">/web</code> scraping functionality</li>
  <li>Fixed syntax error in Elm’s tree-sitter scm file (by @cjoach).</li>
  <li>Handle UnicodeEncodeError when streaming text to the terminal.</li>
  <li>Updated dependencies to latest versions.</li>
  <li>Llmcode wrote 45% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0451">
  
  
    <a href="#llmcode-v0451" class="anchor-heading" aria-labelledby="llmcode-v0451"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.45.1
  
  
</h3>
    

<ul>
  <li>Use 4o-mini as the weak model wherever 3.5-turbo was used.</li>
</ul>
<h3 id="llmcode-v0450">
  
  
    <a href="#llmcode-v0450" class="anchor-heading" aria-labelledby="llmcode-v0450"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.45.0
  
  
</h3>
    

<ul>
  <li>GPT-4o mini scores similar to the original GPT 3.5, using whole edit format.</li>
  <li>Llmcode is better at offering to add files to the chat on Windows.</li>
  <li>Bugfix corner cases for <code class="language-plaintext highlighter-rouge">/undo</code> with new files or new repos.</li>
  <li>Now shows last 4 characters of API keys in <code class="language-plaintext highlighter-rouge">--verbose</code> output.</li>
  <li>Bugfix to precedence of multiple <code class="language-plaintext highlighter-rouge">.env</code> files.</li>
  <li>Bugfix to gracefully handle HTTP errors when installing pandoc.</li>
  <li>Llmcode wrote 42% of the code in this release.</li>
</ul>
<h3 id="llmcode-v0440">
  
  
    <a href="#llmcode-v0440" class="anchor-heading" aria-labelledby="llmcode-v0440"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.44.0
  
  
</h3>
    

<ul>
  <li>Default pip install size reduced by 3-12x.</li>
  <li>Added 3 package extras, which llmcode will offer to install when needed:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">llmcode[help]</code></li>
      <li><code class="language-plaintext highlighter-rouge">llmcode[browser]</code></li>
      <li><code class="language-plaintext highlighter-rouge">llmcode[playwright]</code></li>
    </ul>
  </li>
  <li>Improved regex for detecting URLs in user chat messages.</li>
  <li>Bugfix to globbing logic when absolute paths are included in <code class="language-plaintext highlighter-rouge">/add</code>.</li>
  <li>Simplified output of <code class="language-plaintext highlighter-rouge">--models</code>.</li>
  <li>The <code class="language-plaintext highlighter-rouge">--check-update</code> switch was renamed to <code class="language-plaintext highlighter-rouge">--just-check-updated</code>.</li>
  <li>The <code class="language-plaintext highlighter-rouge">--skip-check-update</code> switch was renamed to <code class="language-plaintext highlighter-rouge">--[no-]check-update</code>.</li>
  <li>Llmcode wrote 29% of the code in this release (157/547 lines).</li>
</ul>
<h3 id="llmcode-v0434">
  
  
    <a href="#llmcode-v0434" class="anchor-heading" aria-labelledby="llmcode-v0434"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.43.4
  
  
</h3>
    

<ul>
  <li>Added scipy back to main requirements.txt.</li>
</ul>
<h3 id="llmcode-v0433">
  
  
    <a href="#llmcode-v0433" class="anchor-heading" aria-labelledby="llmcode-v0433"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.43.3
  
  
</h3>
    

<ul>
  <li>Added build-essentials back to main Dockerfile.</li>
</ul>
<h3 id="llmcode-v0432">
  
  
    <a href="#llmcode-v0432" class="anchor-heading" aria-labelledby="llmcode-v0432"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.43.2
  
  
</h3>
    

<ul>
  <li>Moved HuggingFace embeddings deps into [hf-embed] extra.</li>
  <li>Added [dev] extra.</li>
</ul>
<h3 id="llmcode-v0431">
  
  
    <a href="#llmcode-v0431" class="anchor-heading" aria-labelledby="llmcode-v0431"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.43.1
  
  
</h3>
    

<ul>
  <li>Replace the torch requirement with the CPU only version, because the GPU versions are huge.</li>
</ul>
<h3 id="llmcode-v0430">
  
  
    <a href="#llmcode-v0430" class="anchor-heading" aria-labelledby="llmcode-v0430"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.43.0
  
  
</h3>
    

<ul>
  <li>Use <code class="language-plaintext highlighter-rouge">/help &lt;question&gt;</code> to <a href="https://llmcode.khulnasoft.com/docs/troubleshooting/support.html">ask for help about using llmcode</a>, customizing settings, troubleshooting, using LLMs, etc.</li>
  <li>Allow multiple use of <code class="language-plaintext highlighter-rouge">/undo</code>.</li>
  <li>All config/env/yml/json files now load from home, git root, cwd and named command line switch.</li>
  <li>New <code class="language-plaintext highlighter-rouge">$HOME/.llmcode/caches</code> dir for app-wide expendable caches.</li>
  <li>Default <code class="language-plaintext highlighter-rouge">--model-settings-file</code> is now <code class="language-plaintext highlighter-rouge">.llmcode.model.settings.yml</code>.</li>
  <li>Default <code class="language-plaintext highlighter-rouge">--model-metadata-file</code> is now <code class="language-plaintext highlighter-rouge">.llmcode.model.metadata.json</code>.</li>
  <li>Bugfix affecting launch with <code class="language-plaintext highlighter-rouge">--no-git</code>.</li>
  <li>Llmcode wrote 9% of the 424 lines edited in this release.</li>
</ul>
<h3 id="llmcode-v0420">
  
  
    <a href="#llmcode-v0420" class="anchor-heading" aria-labelledby="llmcode-v0420"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.42.0
  
  
</h3>
    

<ul>
  <li>Performance release:
    <ul>
      <li>5X faster launch!</li>
      <li>Faster auto-complete in large git repos (users report ~100X speedup)!</li>
    </ul>
  </li>
</ul>
<h3 id="llmcode-v0410">
  
  
    <a href="#llmcode-v0410" class="anchor-heading" aria-labelledby="llmcode-v0410"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.41.0
  
  
</h3>
    

<ul>
  <li><a href="https://llmcode.khulnasoft.com/2024/07/01/sonnet-not-lazy.html">Allow Claude 3.5 Sonnet to stream back &gt;4k tokens!</a>
    <ul>
      <li>It is the first model capable of writing such large coherent, useful code edits.</li>
      <li>Do large refactors or generate multiple files of new code in one go.</li>
    </ul>
  </li>
  <li>Llmcode now uses <code class="language-plaintext highlighter-rouge">claude-3-5-sonnet-20240620</code> by default if <code class="language-plaintext highlighter-rouge">ANTHROPIC_API_KEY</code> is set in the environment.</li>
  <li><a href="https://llmcode.khulnasoft.com/docs/usage/images-urls.html">Enabled image support</a> for 3.5 Sonnet and for GPT-4o &amp; 3.5 Sonnet via OpenRouter (by @yamitzky).</li>
  <li>Added <code class="language-plaintext highlighter-rouge">--attribute-commit-message</code> to prefix llmcode’s commit messages with “llmcode:”.</li>
  <li>Fixed regression in quality of one-line commit messages.</li>
  <li>Automatically retry on Anthropic <code class="language-plaintext highlighter-rouge">overloaded_error</code>.</li>
  <li>Bumped dependency versions.</li>
</ul>
<h3 id="llmcode-v0406">
  
  
    <a href="#llmcode-v0406" class="anchor-heading" aria-labelledby="llmcode-v0406"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.40.6
  
  
</h3>
    

<ul>
  <li>Fixed <code class="language-plaintext highlighter-rouge">/undo</code> so it works regardless of <code class="language-plaintext highlighter-rouge">--attribute</code> settings.</li>
</ul>
<h3 id="llmcode-v0405">
  
  
    <a href="#llmcode-v0405" class="anchor-heading" aria-labelledby="llmcode-v0405"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.40.5
  
  
</h3>
    

<ul>
  <li>Bump versions to pickup latest litellm to fix streaming issue with Gemini
    <ul>
      <li>https://github.com/BerriAI/litellm/issues/4408</li>
    </ul>
  </li>
</ul>
<h3 id="llmcode-v0401">
  
  
    <a href="#llmcode-v0401" class="anchor-heading" aria-labelledby="llmcode-v0401"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.40.1
  
  
</h3>
    

<ul>
  <li>Improved context awareness of repomap.</li>
  <li>Restored proper <code class="language-plaintext highlighter-rouge">--help</code> functionality.</li>
</ul>
<h3 id="llmcode-v0400">
  
  
    <a href="#llmcode-v0400" class="anchor-heading" aria-labelledby="llmcode-v0400"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.40.0
  
  
</h3>
    

<ul>
  <li>Improved prompting to discourage Sonnet from wasting tokens emitting unchanging code (#705).</li>
  <li>Improved error info for token limit errors.</li>
  <li>Options to suppress adding “(llmcode)” to the <a href="https://llmcode.khulnasoft.com/docs/git.html#commit-attribution">git author and committer names</a>.</li>
  <li>Use <code class="language-plaintext highlighter-rouge">--model-settings-file</code> to customize per-model settings, like use of repo-map (by @caseymcc).</li>
  <li>Improved invocation of flake8 linter for python code.</li>
</ul>
<h3 id="llmcode-v0390">
  
  
    <a href="#llmcode-v0390" class="anchor-heading" aria-labelledby="llmcode-v0390"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.39.0
  
  
</h3>
    

<ul>
  <li>Use <code class="language-plaintext highlighter-rouge">--sonnet</code> for Claude 3.5 Sonnet, which is the top model on <a href="https://llmcode.khulnasoft.com/docs/leaderboards/#claude-35-sonnet-takes-the-top-spot">llmcode’s LLM code editing leaderboard</a>.</li>
  <li>All <code class="language-plaintext highlighter-rouge">LLMCODE_xxx</code> environment variables can now be set in <code class="language-plaintext highlighter-rouge">.env</code> (by @jpshack-at-palomar).</li>
  <li>Use <code class="language-plaintext highlighter-rouge">--llm-history-file</code> to log raw messages sent to the LLM (by @daniel-vainsencher).</li>
  <li>Commit messages are no longer prefixed with “llmcode:”. Instead the git author and committer names have “(llmcode)” added.</li>
</ul>
<h3 id="llmcode-v0380">
  
  
    <a href="#llmcode-v0380" class="anchor-heading" aria-labelledby="llmcode-v0380"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.38.0
  
  
</h3>
    

<ul>
  <li>Use <code class="language-plaintext highlighter-rouge">--vim</code> for <a href="https://llmcode.khulnasoft.com/docs/usage/commands.html#vi">vim keybindings</a> in the chat.</li>
  <li><a href="https://llmcode.khulnasoft.com/docs/llms/warnings.html#specifying-context-window-size-and-token-costs">Add LLM metadata</a> via <code class="language-plaintext highlighter-rouge">.llmcode.models.json</code> file (by @caseymcc).</li>
  <li>More detailed <a href="https://llmcode.khulnasoft.com/docs/troubleshooting/token-limits.html">error messages on token limit errors</a>.</li>
  <li>Single line commit messages, without the recent chat messages.</li>
  <li>Ensure <code class="language-plaintext highlighter-rouge">--commit --dry-run</code> does nothing.</li>
  <li>Have playwright wait for idle network to better scrape js sites.</li>
  <li>Documentation updates, moved into website/ subdir.</li>
  <li>Moved tests/ into llmcode/tests/.</li>
</ul>
<h3 id="llmcode-v0370">
  
  
    <a href="#llmcode-v0370" class="anchor-heading" aria-labelledby="llmcode-v0370"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.37.0
  
  
</h3>
    

<ul>
  <li>Repo map is now optimized based on text of chat history as well as files added to chat.</li>
  <li>Improved prompts when no files have been added to chat to solicit LLM file suggestions.</li>
  <li>Llmcode will notice if you paste a URL into the chat, and offer to scrape it.</li>
  <li>Performance improvements the repo map, especially in large repos.</li>
  <li>Llmcode will not offer to add bare filenames like <code class="language-plaintext highlighter-rouge">make</code> or <code class="language-plaintext highlighter-rouge">run</code> which may just be words.</li>
  <li>Properly override <code class="language-plaintext highlighter-rouge">GIT_EDITOR</code> env for commits if it is already set.</li>
  <li>Detect supported audio sample rates for <code class="language-plaintext highlighter-rouge">/voice</code>.</li>
  <li>Other small bug fixes.</li>
</ul>
<h3 id="llmcode-v0360">
  
  
    <a href="#llmcode-v0360" class="anchor-heading" aria-labelledby="llmcode-v0360"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.36.0
  
  
</h3>
    

<ul>
  <li><a href="https://llmcode.khulnasoft.com/2024/05/22/linting.html">Llmcode can now lint your code and fix any errors</a>.
    <ul>
      <li>Llmcode automatically lints and fixes after every LLM edit.</li>
      <li>You can manually lint-and-fix files with <code class="language-plaintext highlighter-rouge">/lint</code> in the chat or <code class="language-plaintext highlighter-rouge">--lint</code> on the command line.</li>
      <li>Llmcode includes built in basic linters for all supported tree-sitter languages.</li>
      <li>You can also configure llmcode to use your preferred linter with <code class="language-plaintext highlighter-rouge">--lint-cmd</code>.</li>
    </ul>
  </li>
  <li>Llmcode has additional support for running tests and fixing problems.
    <ul>
      <li>Configure your testing command with <code class="language-plaintext highlighter-rouge">--test-cmd</code>.</li>
      <li>Run tests with <code class="language-plaintext highlighter-rouge">/test</code> or from the command line with <code class="language-plaintext highlighter-rouge">--test</code>.</li>
      <li>Llmcode will automatically attempt to fix any test failures.</li>
    </ul>
  </li>
</ul>
<h3 id="llmcode-v0350">
  
  
    <a href="#llmcode-v0350" class="anchor-heading" aria-labelledby="llmcode-v0350"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.35.0
  
  
</h3>
    

<ul>
  <li>Llmcode now uses GPT-4o by default.
    <ul>
      <li>GPT-4o tops the <a href="https://llmcode.khulnasoft.com/docs/leaderboards/">llmcode LLM code editing leaderboard</a> at 72.9%, versus 68.4% for Opus.</li>
      <li>GPT-4o takes second on <a href="https://llmcode.khulnasoft.com/docs/leaderboards/#code-refactoring-leaderboard">llmcode’s refactoring leaderboard</a> with 62.9%, versus Opus at 72.3%.</li>
    </ul>
  </li>
  <li>Added <code class="language-plaintext highlighter-rouge">--restore-chat-history</code> to restore prior chat history on launch, so you can continue the last conversation.</li>
  <li>Improved reflection feedback to LLMs using the diff edit format.</li>
  <li>Improved retries on <code class="language-plaintext highlighter-rouge">httpx</code> errors.</li>
</ul>
<h3 id="llmcode-v0340">
  
  
    <a href="#llmcode-v0340" class="anchor-heading" aria-labelledby="llmcode-v0340"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.34.0
  
  
</h3>
    

<ul>
  <li>Updated prompting to use more natural phrasing about files, the git repo, etc. Removed reliance on read-write/read-only terminology.</li>
  <li>Refactored prompting to unify some phrasing across edit formats.</li>
  <li>Enhanced the canned assistant responses used in prompts.</li>
  <li>Added explicit model settings for <code class="language-plaintext highlighter-rouge">openrouter/anthropic/claude-3-opus</code>, <code class="language-plaintext highlighter-rouge">gpt-3.5-turbo</code></li>
  <li>Added <code class="language-plaintext highlighter-rouge">--show-prompts</code> debug switch.</li>
  <li>Bugfix: catch and retry on all litellm exceptions.</li>
</ul>
<h3 id="llmcode-v0330">
  
  
    <a href="#llmcode-v0330" class="anchor-heading" aria-labelledby="llmcode-v0330"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.33.0
  
  
</h3>
    

<ul>
  <li>Added native support for <a href="https://llmcode.khulnasoft.com/docs/llms.html#deepseek">Deepseek models</a> using <code class="language-plaintext highlighter-rouge">DEEPSEEK_API_KEY</code> and <code class="language-plaintext highlighter-rouge">deepseek/deepseek-chat</code>, etc rather than as a generic OpenAI compatible API.</li>
</ul>
<h3 id="llmcode-v0320">
  
  
    <a href="#llmcode-v0320" class="anchor-heading" aria-labelledby="llmcode-v0320"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.32.0
  
  
</h3>
    

<ul>
  <li><a href="https://llmcode.khulnasoft.com/docs/leaderboards/">Llmcode LLM code editing leaderboards</a> that rank popular models according to their ability to edit code.
    <ul>
      <li>Leaderboards include GPT-3.5/4 Turbo, Opus, Sonnet, Gemini 1.5 Pro, Llama 3, Deepseek Coder &amp; Command-R+.</li>
    </ul>
  </li>
  <li>Gemini 1.5 Pro now defaults to a new diff-style edit format (diff-fenced), enabling it to work better with larger code bases.</li>
  <li>Support for Deepseek-V2, via more a flexible config of system messages in the diff edit format.</li>
  <li>Improved retry handling on errors from model APIs.</li>
  <li>Benchmark outputs results in YAML, compatible with leaderboard.</li>
</ul>
<h3 id="llmcode-v0310">
  
  
    <a href="#llmcode-v0310" class="anchor-heading" aria-labelledby="llmcode-v0310"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.31.0
  
  
</h3>
    

<ul>
  <li><a href="https://llmcode.khulnasoft.com/2024/05/02/browser.html">Llmcode is now also AI pair programming in your browser!</a> Use the <code class="language-plaintext highlighter-rouge">--browser</code> switch to launch an experimental browser based version of llmcode.</li>
  <li>Switch models during the chat with <code class="language-plaintext highlighter-rouge">/model &lt;name&gt;</code> and search the list of available models with <code class="language-plaintext highlighter-rouge">/models &lt;query&gt;</code>.</li>
</ul>
<h3 id="llmcode-v0301">
  
  
    <a href="#llmcode-v0301" class="anchor-heading" aria-labelledby="llmcode-v0301"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.30.1
  
  
</h3>
    

<ul>
  <li>Adding missing <code class="language-plaintext highlighter-rouge">google-generativeai</code> dependency</li>
</ul>
<h3 id="llmcode-v0300">
  
  
    <a href="#llmcode-v0300" class="anchor-heading" aria-labelledby="llmcode-v0300"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.30.0
  
  
</h3>
    

<ul>
  <li>Added <a href="https://llmcode.khulnasoft.com/docs/llms.html#free-models">Gemini 1.5 Pro</a> as a recommended free model.</li>
  <li>Allow repo map for “whole” edit format.</li>
  <li>Added <code class="language-plaintext highlighter-rouge">--models &lt;MODEL-NAME&gt;</code> to search the available models.</li>
  <li>Added <code class="language-plaintext highlighter-rouge">--no-show-model-warnings</code> to silence model warnings.</li>
</ul>
<h3 id="llmcode-v0292">
  
  
    <a href="#llmcode-v0292" class="anchor-heading" aria-labelledby="llmcode-v0292"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.29.2
  
  
</h3>
    

<ul>
  <li>Improved <a href="https://llmcode.khulnasoft.com/docs/llms.html#model-warnings">model warnings</a> for unknown or unfamiliar models</li>
</ul>
<h3 id="llmcode-v0291">
  
  
    <a href="#llmcode-v0291" class="anchor-heading" aria-labelledby="llmcode-v0291"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.29.1
  
  
</h3>
    

<ul>
  <li>Added better support for groq/llama3-70b-8192</li>
</ul>
<h3 id="llmcode-v0290">
  
  
    <a href="#llmcode-v0290" class="anchor-heading" aria-labelledby="llmcode-v0290"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.29.0
  
  
</h3>
    

<ul>
  <li>Added support for <a href="https://llmcode.khulnasoft.com/docs/llms.html">directly connecting to Anthropic, Cohere, Gemini and many other LLM providers</a>.</li>
  <li>Added <code class="language-plaintext highlighter-rouge">--weak-model &lt;model-name&gt;</code> which allows you to specify which model to use for commit messages and chat history summarization.</li>
  <li>New command line switches for working with popular models:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">--4-turbo-vision</code></li>
      <li><code class="language-plaintext highlighter-rouge">--opus</code></li>
      <li><code class="language-plaintext highlighter-rouge">--sonnet</code></li>
      <li><code class="language-plaintext highlighter-rouge">--anthropic-api-key</code></li>
    </ul>
  </li>
  <li>Improved “whole” and “diff” backends to better support <a href="https://llmcode.khulnasoft.com/docs/llms.html#cohere">Cohere’s free to use Command-R+ model</a>.</li>
  <li>Allow <code class="language-plaintext highlighter-rouge">/add</code> of images from anywhere in the filesystem.</li>
  <li>Fixed crash when operating in a repo in a detached HEAD state.</li>
  <li>Fix: Use the same default model in CLI and python scripting.</li>
</ul>
<h3 id="llmcode-v0280">
  
  
    <a href="#llmcode-v0280" class="anchor-heading" aria-labelledby="llmcode-v0280"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.28.0
  
  
</h3>
    

<ul>
  <li>Added support for new <code class="language-plaintext highlighter-rouge">gpt-4-turbo-2024-04-09</code> and <code class="language-plaintext highlighter-rouge">gpt-4-turbo</code> models.
    <ul>
      <li>Benchmarked at 61.7% on Exercism benchmark, comparable to <code class="language-plaintext highlighter-rouge">gpt-4-0613</code> and worse than the <code class="language-plaintext highlighter-rouge">gpt-4-preview-XXXX</code> models. See <a href="https://llmcode.khulnasoft.com/2024/03/08/claude-3.html">recent Exercism benchmark results</a>.</li>
      <li>Benchmarked at 34.1% on the refactoring/laziness benchmark, significantly worse than the <code class="language-plaintext highlighter-rouge">gpt-4-preview-XXXX</code> models. See <a href="https://llmcode.khulnasoft.com/2024/01/25/benchmarks-0125.html">recent refactor bencmark results</a>.</li>
      <li>Llmcode continues to default to <code class="language-plaintext highlighter-rouge">gpt-4-1106-preview</code> as it performs best on both benchmarks, and significantly better on the refactoring/laziness benchmark.</li>
    </ul>
  </li>
</ul>
<h3 id="llmcode-v0270">
  
  
    <a href="#llmcode-v0270" class="anchor-heading" aria-labelledby="llmcode-v0270"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.27.0
  
  
</h3>
    

<ul>
  <li>Improved repomap support for typescript, by @ryanfreckleton.</li>
  <li>Bugfix: Only /undo the files which were part of the last commit, don’t stomp other dirty files</li>
  <li>Bugfix: Show clear error message when OpenAI API key is not set.</li>
  <li>Bugfix: Catch error for obscure languages without tags.scm file.</li>
</ul>
<h3 id="llmcode-v0261">
  
  
    <a href="#llmcode-v0261" class="anchor-heading" aria-labelledby="llmcode-v0261"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.26.1
  
  
</h3>
    

<ul>
  <li>Fixed bug affecting parsing of git config in some environments.</li>
</ul>
<h3 id="llmcode-v0260">
  
  
    <a href="#llmcode-v0260" class="anchor-heading" aria-labelledby="llmcode-v0260"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.26.0
  
  
</h3>
    

<ul>
  <li>Use GPT-4 Turbo by default.</li>
  <li>Added <code class="language-plaintext highlighter-rouge">-3</code> and <code class="language-plaintext highlighter-rouge">-4</code> switches to use GPT 3.5 or GPT-4 (non-Turbo).</li>
  <li>Bug fix to avoid reflecting local git errors back to GPT.</li>
  <li>Improved logic for opening git repo on launch.</li>
</ul>
<h3 id="llmcode-v0250">
  
  
    <a href="#llmcode-v0250" class="anchor-heading" aria-labelledby="llmcode-v0250"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.25.0
  
  
</h3>
    

<ul>
  <li>Issue a warning if user adds too much code to the chat.
    <ul>
      <li>https://llmcode.khulnasoft.com/docs/faq.html#how-can-i-add-all-the-files-to-the-chat</li>
    </ul>
  </li>
  <li>Vocally refuse to add files to the chat that match <code class="language-plaintext highlighter-rouge">.llmcodeignore</code>
    <ul>
      <li>Prevents bug where subsequent git commit of those files will fail.</li>
    </ul>
  </li>
  <li>Added <code class="language-plaintext highlighter-rouge">--openai-organization-id</code> argument.</li>
  <li>Show the user a FAQ link if edits fail to apply.</li>
  <li>Made past articles part of https://llmcode.khulnasoft.com/blog/</li>
</ul>
<h3 id="llmcode-v0241">
  
  
    <a href="#llmcode-v0241" class="anchor-heading" aria-labelledby="llmcode-v0241"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.24.1
  
  
</h3>
    

<ul>
  <li>Fixed bug with cost computations when –no-steam in effect</li>
</ul>
<h3 id="llmcode-v0240">
  
  
    <a href="#llmcode-v0240" class="anchor-heading" aria-labelledby="llmcode-v0240"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.24.0
  
  
</h3>
    

<ul>
  <li>New <code class="language-plaintext highlighter-rouge">/web &lt;url&gt;</code> command which scrapes the url, turns it into fairly clean markdown and adds it to the chat.</li>
  <li>Updated all OpenAI model names, pricing info</li>
  <li>Default GPT 3.5 model is now <code class="language-plaintext highlighter-rouge">gpt-3.5-turbo-0125</code>.</li>
  <li>Bugfix to the <code class="language-plaintext highlighter-rouge">!</code> alias for <code class="language-plaintext highlighter-rouge">/run</code>.</li>
</ul>
<h3 id="llmcode-v0230">
  
  
    <a href="#llmcode-v0230" class="anchor-heading" aria-labelledby="llmcode-v0230"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.23.0
  
  
</h3>
    

<ul>
  <li>Added support for <code class="language-plaintext highlighter-rouge">--model gpt-4-0125-preview</code> and OpenAI’s alias <code class="language-plaintext highlighter-rouge">--model gpt-4-turbo-preview</code>. The <code class="language-plaintext highlighter-rouge">--4turbo</code> switch remains an alias for <code class="language-plaintext highlighter-rouge">--model gpt-4-1106-preview</code> at this time.</li>
  <li>New <code class="language-plaintext highlighter-rouge">/test</code> command that runs a command and adds the output to the chat on non-zero exit status.</li>
  <li>Improved streaming of markdown to the terminal.</li>
  <li>Added <code class="language-plaintext highlighter-rouge">/quit</code> as alias for <code class="language-plaintext highlighter-rouge">/exit</code>.</li>
  <li>Added <code class="language-plaintext highlighter-rouge">--skip-check-update</code> to skip checking for the update on launch.</li>
  <li>Added <code class="language-plaintext highlighter-rouge">--openrouter</code> as a shortcut for <code class="language-plaintext highlighter-rouge">--openai-api-base https://openrouter.ai/api/v1</code></li>
  <li>Fixed bug preventing use of env vars <code class="language-plaintext highlighter-rouge">OPENAI_API_BASE, OPENAI_API_TYPE, OPENAI_API_VERSION, OPENAI_API_DEPLOYMENT_ID</code>.</li>
</ul>
<h3 id="llmcode-v0220">
  
  
    <a href="#llmcode-v0220" class="anchor-heading" aria-labelledby="llmcode-v0220"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.22.0
  
  
</h3>
    

<ul>
  <li>Improvements for unified diff editing format.</li>
  <li>Added ! as an alias for /run.</li>
  <li>Autocomplete for /add and /drop now properly quotes filenames with spaces.</li>
  <li>The /undo command asks GPT not to just retry reverted edit.</li>
</ul>
<h3 id="llmcode-v0211">
  
  
    <a href="#llmcode-v0211" class="anchor-heading" aria-labelledby="llmcode-v0211"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.21.1
  
  
</h3>
    

<ul>
  <li>Bugfix for unified diff editing format.</li>
  <li>Added –4turbo and –4 aliases for –4-turbo.</li>
</ul>
<h3 id="llmcode-v0210">
  
  
    <a href="#llmcode-v0210" class="anchor-heading" aria-labelledby="llmcode-v0210"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.21.0
  
  
</h3>
    

<ul>
  <li>Support for python 3.12.</li>
  <li>Improvements to unified diff editing format.</li>
  <li>New <code class="language-plaintext highlighter-rouge">--check-update</code> arg to check if updates are available and exit with status code.</li>
</ul>
<h3 id="llmcode-v0200">
  
  
    <a href="#llmcode-v0200" class="anchor-heading" aria-labelledby="llmcode-v0200"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.20.0
  
  
</h3>
    

<ul>
  <li>
    <p>Add images to the chat to automatically use GPT-4 Vision, by @joshuavial</p>
  </li>
  <li>
    <p>Bugfixes:</p>
    <ul>
      <li>Improved unicode encoding for <code class="language-plaintext highlighter-rouge">/run</code> command output, by @ctoth</li>
      <li>Prevent false auto-commits on Windows, by @ctoth</li>
    </ul>
  </li>
</ul>
<h3 id="llmcode-v0191">
  
  
    <a href="#llmcode-v0191" class="anchor-heading" aria-labelledby="llmcode-v0191"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.19.1
  
  
</h3>
    

<ul>
  <li>Removed stray debug output.</li>
</ul>
<h3 id="llmcode-v0190">
  
  
    <a href="#llmcode-v0190" class="anchor-heading" aria-labelledby="llmcode-v0190"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.19.0
  
  
</h3>
    

<ul>
  <li><a href="https://llmcode.khulnasoft.com/docs/unified-diffs.html">Significantly reduced “lazy” coding from GPT-4 Turbo due to new unified diff edit format</a>
    <ul>
      <li>Score improves from 20% to 61% on new “laziness benchmark”.</li>
      <li>Llmcode now uses unified diffs by default for <code class="language-plaintext highlighter-rouge">gpt-4-1106-preview</code>.</li>
    </ul>
  </li>
  <li>New <code class="language-plaintext highlighter-rouge">--4-turbo</code> command line switch as a shortcut for <code class="language-plaintext highlighter-rouge">--model gpt-4-1106-preview</code>.</li>
</ul>
<h3 id="llmcode-v0181">
  
  
    <a href="#llmcode-v0181" class="anchor-heading" aria-labelledby="llmcode-v0181"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.18.1
  
  
</h3>
    

<ul>
  <li>Upgraded to new openai python client v1.3.7.</li>
</ul>
<h3 id="llmcode-v0180">
  
  
    <a href="#llmcode-v0180" class="anchor-heading" aria-labelledby="llmcode-v0180"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.18.0
  
  
</h3>
    

<ul>
  <li>Improved prompting for both GPT-4 and GPT-4 Turbo.
    <ul>
      <li>Far fewer edit errors from GPT-4 Turbo (<code class="language-plaintext highlighter-rouge">gpt-4-1106-preview</code>).</li>
      <li>Significantly better benchmark results from the June GPT-4 (<code class="language-plaintext highlighter-rouge">gpt-4-0613</code>). Performance leaps from 47%/64% up to 51%/71%.</li>
    </ul>
  </li>
  <li>Fixed bug where in-chat files were marked as both read-only and ready-write, sometimes confusing GPT.</li>
  <li>Fixed bug to properly handle repos with submodules.</li>
</ul>
<h3 id="llmcode-v0170">
  
  
    <a href="#llmcode-v0170" class="anchor-heading" aria-labelledby="llmcode-v0170"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.17.0
  
  
</h3>
    

<ul>
  <li>Support for OpenAI’s new 11/06 models:
    <ul>
      <li>gpt-4-1106-preview with 128k context window</li>
      <li>gpt-3.5-turbo-1106 with 16k context window</li>
    </ul>
  </li>
  <li><a href="https://llmcode.khulnasoft.com/docs/benchmarks-1106.html">Benchmarks for OpenAI’s new 11/06 models</a></li>
  <li>Streamlined <a href="https://llmcode.khulnasoft.com/docs/faq.html#can-i-script-llmcode">API for scripting llmcode, added docs</a></li>
  <li>Ask for more concise SEARCH/REPLACE blocks. <a href="https://llmcode.khulnasoft.com/docs/benchmarks.html">Benchmarked</a> at 63.9%, no regression.</li>
  <li>Improved repo-map support for elisp.</li>
  <li>Fixed crash bug when <code class="language-plaintext highlighter-rouge">/add</code> used on file matching <code class="language-plaintext highlighter-rouge">.gitignore</code></li>
  <li>Fixed misc bugs to catch and handle unicode decoding errors.</li>
</ul>
<h3 id="llmcode-v0163">
  
  
    <a href="#llmcode-v0163" class="anchor-heading" aria-labelledby="llmcode-v0163"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.16.3
  
  
</h3>
    

<ul>
  <li>Fixed repo-map support for C#.</li>
</ul>
<h3 id="llmcode-v0162">
  
  
    <a href="#llmcode-v0162" class="anchor-heading" aria-labelledby="llmcode-v0162"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.16.2
  
  
</h3>
    

<ul>
  <li>Fixed docker image.</li>
</ul>
<h3 id="llmcode-v0161">
  
  
    <a href="#llmcode-v0161" class="anchor-heading" aria-labelledby="llmcode-v0161"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.16.1
  
  
</h3>
    

<ul>
  <li>Updated tree-sitter dependencies to streamline the pip install process</li>
</ul>
<h3 id="llmcode-v0160">
  
  
    <a href="#llmcode-v0160" class="anchor-heading" aria-labelledby="llmcode-v0160"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.16.0
  
  
</h3>
    

<ul>
  <li><a href="https://llmcode.khulnasoft.com/docs/repomap.html">Improved repository map using tree-sitter</a></li>
  <li>Switched from “edit block” to “search/replace block”, which reduced malformed edit blocks. <a href="https://llmcode.khulnasoft.com/docs/benchmarks.html">Benchmarked</a> at 66.2%, no regression.</li>
  <li>Improved handling of malformed edit blocks targeting multiple edits to the same file. <a href="https://llmcode.khulnasoft.com/docs/benchmarks.html">Benchmarked</a> at 65.4%, no regression.</li>
  <li>Bugfix to properly handle malformed <code class="language-plaintext highlighter-rouge">/add</code> wildcards.</li>
</ul>
<h3 id="llmcode-v0150">
  
  
    <a href="#llmcode-v0150" class="anchor-heading" aria-labelledby="llmcode-v0150"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.15.0
  
  
</h3>
    

<ul>
  <li>Added support for <code class="language-plaintext highlighter-rouge">.llmcodeignore</code> file, which instructs llmcode to ignore parts of the git repo.</li>
  <li>New <code class="language-plaintext highlighter-rouge">--commit</code> cmd line arg, which just commits all pending changes with a sensible commit message generated by gpt-3.5.</li>
  <li>Added universal ctags and multiple architectures to the <a href="https://llmcode.khulnasoft.com/docs/install/docker.html">llmcode docker image</a></li>
  <li><code class="language-plaintext highlighter-rouge">/run</code> and <code class="language-plaintext highlighter-rouge">/git</code> now accept full shell commands, like: <code class="language-plaintext highlighter-rouge">/run (cd subdir; ls)</code></li>
  <li>Restored missing <code class="language-plaintext highlighter-rouge">--encoding</code> cmd line switch.</li>
</ul>
<h3 id="llmcode-v0142">
  
  
    <a href="#llmcode-v0142" class="anchor-heading" aria-labelledby="llmcode-v0142"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.14.2
  
  
</h3>
    

<ul>
  <li>Easily <a href="https://llmcode.khulnasoft.com/docs/install/docker.html">run llmcode from a docker image</a></li>
  <li>Fixed bug with chat history summarization.</li>
  <li>Fixed bug if <code class="language-plaintext highlighter-rouge">soundfile</code> package not available.</li>
</ul>
<h3 id="llmcode-v0141">
  
  
    <a href="#llmcode-v0141" class="anchor-heading" aria-labelledby="llmcode-v0141"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.14.1
  
  
</h3>
    

<ul>
  <li>/add and /drop handle absolute filenames and quoted filenames</li>
  <li>/add checks to be sure files are within the git repo (or root)</li>
  <li>If needed, warn users that in-chat file paths are all relative to the git repo</li>
  <li>Fixed /add bug in when llmcode launched in repo subdir</li>
  <li>Show models supported by api/key if requested model isn’t available</li>
</ul>
<h3 id="llmcode-v0140">
  
  
    <a href="#llmcode-v0140" class="anchor-heading" aria-labelledby="llmcode-v0140"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.14.0
  
  
</h3>
    

<ul>
  <li><a href="https://llmcode.khulnasoft.com/docs/faq.html#accessing-other-llms-with-openrouter">Support for Claude2 and other LLMs via OpenRouter</a> by @joshuavial</li>
  <li>Documentation for <a href="https://github.com/KhulnaSoft/llmcode/tree/main/benchmark">running the llmcode benchmarking suite</a></li>
  <li>Llmcode now requires Python &gt;= 3.9</li>
</ul>
<h3 id="llmcode-v0130">
  
  
    <a href="#llmcode-v0130" class="anchor-heading" aria-labelledby="llmcode-v0130"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.13.0
  
  
</h3>
    

<ul>
  <li><a href="https://llmcode.khulnasoft.com/docs/faq.html#how-did-v0130-change-git-usage">Only git commit dirty files that GPT tries to edit</a></li>
  <li>Send chat history as prompt/context for Whisper voice transcription</li>
  <li>Added <code class="language-plaintext highlighter-rouge">--voice-language</code> switch to constrain <code class="language-plaintext highlighter-rouge">/voice</code> to transcribe to a specific language</li>
  <li>Late-bind importing <code class="language-plaintext highlighter-rouge">sounddevice</code>, as it was slowing down llmcode startup</li>
  <li>Improved –foo/–no-foo switch handling for command line and yml config settings</li>
</ul>
<h3 id="llmcode-v0120">
  
  
    <a href="#llmcode-v0120" class="anchor-heading" aria-labelledby="llmcode-v0120"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.12.0
  
  
</h3>
    

<ul>
  <li><a href="https://llmcode.khulnasoft.com/docs/usage/voice.html">Voice-to-code</a> support, which allows you to code with your voice.</li>
  <li>Fixed bug where /diff was causing crash.</li>
  <li>Improved prompting for gpt-4, refactor of editblock coder.</li>
  <li><a href="https://llmcode.khulnasoft.com/docs/benchmarks.html">Benchmarked</a> at 63.2% for gpt-4/diff, no regression.</li>
</ul>
<h3 id="llmcode-v0111">
  
  
    <a href="#llmcode-v0111" class="anchor-heading" aria-labelledby="llmcode-v0111"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.11.1
  
  
</h3>
    

<ul>
  <li>Added a progress bar when initially creating a repo map.</li>
  <li>Fixed bad commit message when adding new file to empty repo.</li>
  <li>Fixed corner case of pending chat history summarization when dirty committing.</li>
  <li>Fixed corner case of undefined <code class="language-plaintext highlighter-rouge">text</code> when using <code class="language-plaintext highlighter-rouge">--no-pretty</code>.</li>
  <li>Fixed /commit bug from repo refactor, added test coverage.</li>
  <li><a href="https://llmcode.khulnasoft.com/docs/benchmarks.html">Benchmarked</a> at 53.4% for gpt-3.5/whole (no regression).</li>
</ul>
<h3 id="llmcode-v0110">
  
  
    <a href="#llmcode-v0110" class="anchor-heading" aria-labelledby="llmcode-v0110"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.11.0
  
  
</h3>
    

<ul>
  <li>Automatically summarize chat history to avoid exhausting context window.</li>
  <li>More detail on dollar costs when running with <code class="language-plaintext highlighter-rouge">--no-stream</code></li>
  <li>Stronger GPT-3.5 prompt against skipping/eliding code in replies (51.9% <a href="https://llmcode.khulnasoft.com/docs/benchmarks.html">benchmark</a>, no regression)</li>
  <li>Defend against GPT-3.5 or non-OpenAI models suggesting filenames surrounded by asterisks.</li>
  <li>Refactored GitRepo code out of the Coder class.</li>
</ul>
<h3 id="llmcode-v0101">
  
  
    <a href="#llmcode-v0101" class="anchor-heading" aria-labelledby="llmcode-v0101"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.10.1
  
  
</h3>
    

<ul>
  <li>/add and /drop always use paths relative to the git root</li>
  <li>Encourage GPT to use language like “add files to the chat” to ask users for permission to edit them.</li>
</ul>
<h3 id="llmcode-v0100">
  
  
    <a href="#llmcode-v0100" class="anchor-heading" aria-labelledby="llmcode-v0100"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.10.0
  
  
</h3>
    

<ul>
  <li>Added <code class="language-plaintext highlighter-rouge">/git</code> command to run git from inside llmcode chats.</li>
  <li>Use Meta-ENTER (Esc+ENTER in some environments) to enter multiline chat messages.</li>
  <li>Create a <code class="language-plaintext highlighter-rouge">.gitignore</code> with <code class="language-plaintext highlighter-rouge">.llmcode*</code> to prevent users from accidentally adding llmcode files to git.</li>
  <li>Check pypi for newer versions and notify user.</li>
  <li>Updated keyboard interrupt logic so that 2 ^C in 2 seconds always forces llmcode to exit.</li>
  <li>Provide GPT with detailed error if it makes a bad edit block, ask for a retry.</li>
  <li>Force <code class="language-plaintext highlighter-rouge">--no-pretty</code> if llmcode detects it is running inside a VSCode terminal.</li>
  <li><a href="https://llmcode.khulnasoft.com/docs/benchmarks.html">Benchmarked</a> at 64.7% for gpt-4/diff (no regression)</li>
</ul>
<h3 id="llmcode-v090">
  
  
    <a href="#llmcode-v090" class="anchor-heading" aria-labelledby="llmcode-v090"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.9.0
  
  
</h3>
    

<ul>
  <li>Support for the OpenAI models in <a href="https://llmcode.khulnasoft.com/docs/faq.html#azure">Azure</a></li>
  <li>Added <code class="language-plaintext highlighter-rouge">--show-repo-map</code></li>
  <li>Improved output when retrying connections to the OpenAI API</li>
  <li>Redacted api key from <code class="language-plaintext highlighter-rouge">--verbose</code> output</li>
  <li>Bugfix: recognize and add files in subdirectories mentioned by user or GPT</li>
  <li><a href="https://llmcode.khulnasoft.com/docs/benchmarks.html">Benchmarked</a> at 53.8% for gpt-3.5-turbo/whole (no regression)</li>
</ul>
<h3 id="llmcode-v083">
  
  
    <a href="#llmcode-v083" class="anchor-heading" aria-labelledby="llmcode-v083"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.8.3
  
  
</h3>
    

<ul>
  <li>Added <code class="language-plaintext highlighter-rouge">--dark-mode</code> and <code class="language-plaintext highlighter-rouge">--light-mode</code> to select colors optimized for terminal background</li>
  <li>Install docs link to <a href="https://github.com/joshuavial/llmcode.nvim">NeoVim plugin</a> by @joshuavial</li>
  <li>Reorganized the <code class="language-plaintext highlighter-rouge">--help</code> output</li>
  <li>Bugfix/improvement to whole edit format, may improve coding editing for GPT-3.5</li>
  <li>Bugfix and tests around git filenames with unicode characters</li>
  <li>Bugfix so that llmcode throws an exception when OpenAI returns InvalidRequest</li>
  <li>Bugfix/improvement to /add and /drop to recurse selected directories</li>
  <li>Bugfix for live diff output when using “whole” edit format</li>
</ul>
<h3 id="llmcode-v082">
  
  
    <a href="#llmcode-v082" class="anchor-heading" aria-labelledby="llmcode-v082"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.8.2
  
  
</h3>
    

<ul>
  <li>Disabled general availability of gpt-4 (it’s rolling out, not 100% available yet)</li>
</ul>
<h3 id="llmcode-v081">
  
  
    <a href="#llmcode-v081" class="anchor-heading" aria-labelledby="llmcode-v081"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.8.1
  
  
</h3>
    

<ul>
  <li>Ask to create a git repo if none found, to better track GPT’s code changes</li>
  <li>Glob wildcards are now supported in <code class="language-plaintext highlighter-rouge">/add</code> and <code class="language-plaintext highlighter-rouge">/drop</code> commands</li>
  <li>Pass <code class="language-plaintext highlighter-rouge">--encoding</code> into ctags, require it to return <code class="language-plaintext highlighter-rouge">utf-8</code></li>
  <li>More robust handling of filepaths, to avoid 8.3 windows filenames</li>
  <li>Added <a href="https://llmcode.khulnasoft.com/docs/faq.html">FAQ</a></li>
  <li>Marked GPT-4 as generally available</li>
  <li>Bugfix for live diffs of whole coder with missing filenames</li>
  <li>Bugfix for chats with multiple files</li>
  <li>Bugfix in editblock coder prompt</li>
</ul>
<h3 id="llmcode-v080">
  
  
    <a href="#llmcode-v080" class="anchor-heading" aria-labelledby="llmcode-v080"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.8.0
  
  
</h3>
    

<ul>
  <li><a href="https://llmcode.khulnasoft.com/docs/benchmarks.html">Benchmark comparing code editing in GPT-3.5 and GPT-4</a></li>
  <li>Improved Windows support:
    <ul>
      <li>Fixed bugs related to path separators in Windows</li>
      <li>Added a CI step to run all tests on Windows</li>
    </ul>
  </li>
  <li>Improved handling of Unicode encoding/decoding
    <ul>
      <li>Explicitly read/write text files with utf-8 encoding by default (mainly benefits Windows)</li>
      <li>Added <code class="language-plaintext highlighter-rouge">--encoding</code> switch to specify another encoding</li>
      <li>Gracefully handle decoding errors</li>
    </ul>
  </li>
  <li>Added <code class="language-plaintext highlighter-rouge">--code-theme</code> switch to control the pygments styling of code blocks (by @kwmiebach)</li>
  <li>Better status messages explaining the reason when ctags is disabled</li>
</ul>
<h3 id="llmcode-v072">
  
  
    <a href="#llmcode-v072" class="anchor-heading" aria-labelledby="llmcode-v072"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.7.2:
  
  
</h3>
    

<ul>
  <li>Fixed a bug to allow llmcode to edit files that contain triple backtick fences.</li>
</ul>
<h3 id="llmcode-v071">
  
  
    <a href="#llmcode-v071" class="anchor-heading" aria-labelledby="llmcode-v071"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.7.1:
  
  
</h3>
    

<ul>
  <li>Fixed a bug in the display of streaming diffs in GPT-3.5 chats</li>
</ul>
<h3 id="llmcode-v070">
  
  
    <a href="#llmcode-v070" class="anchor-heading" aria-labelledby="llmcode-v070"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.7.0:
  
  
</h3>
    

<ul>
  <li>Graceful handling of context window exhaustion, including helpful tips.</li>
  <li>Added <code class="language-plaintext highlighter-rouge">--message</code> to give GPT that one instruction and then exit after it replies and any edits are performed.</li>
  <li>Added <code class="language-plaintext highlighter-rouge">--no-stream</code> to disable streaming GPT responses.
    <ul>
      <li>Non-streaming responses include token usage info.</li>
      <li>Enables display of cost info based on OpenAI advertised pricing.</li>
    </ul>
  </li>
  <li>Coding competence benchmarking tool against suite of programming tasks based on Execism’s python repo.
    <ul>
      <li>https://github.com/exercism/python</li>
    </ul>
  </li>
  <li>Major refactor in preparation for supporting new function calls api.</li>
  <li>Initial implementation of a function based code editing backend for 3.5.
    <ul>
      <li>Initial experiments show that using functions makes 3.5 less competent at coding.</li>
    </ul>
  </li>
  <li>Limit automatic retries when GPT returns a malformed edit response.</li>
</ul>
<h3 id="llmcode-v062">
  
  
    <a href="#llmcode-v062" class="anchor-heading" aria-labelledby="llmcode-v062"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.6.2
  
  
</h3>
    

<ul>
  <li>Support for <code class="language-plaintext highlighter-rouge">gpt-3.5-turbo-16k</code>, and all OpenAI chat models</li>
  <li>Improved ability to correct when gpt-4 omits leading whitespace in code edits</li>
  <li>Added <code class="language-plaintext highlighter-rouge">--openai-api-base</code> to support API proxies, etc.</li>
</ul>
<h3 id="llmcode-v050">
  
  
    <a href="#llmcode-v050" class="anchor-heading" aria-labelledby="llmcode-v050"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Llmcode v0.5.0
  
  
</h3>
    

<ul>
  <li>Added support for <code class="language-plaintext highlighter-rouge">gpt-3.5-turbo</code> and <code class="language-plaintext highlighter-rouge">gpt-4-32k</code>.</li>
  <li>Added <code class="language-plaintext highlighter-rouge">--map-tokens</code> to set a token budget for the repo map, along with a PageRank based algorithm for prioritizing which files and identifiers to include in the map.</li>
  <li>Added in-chat command <code class="language-plaintext highlighter-rouge">/tokens</code> to report on context window token usage.</li>
  <li>Added in-chat command <code class="language-plaintext highlighter-rouge">/clear</code> to clear the conversation history.
<!--[[[end]]]--></li>
</ul>

          

          
        </main>
        


      </div>
    </div>
    
      

<div class="search-overlay"></div>

    
  </div>

  
</body>
</html>

