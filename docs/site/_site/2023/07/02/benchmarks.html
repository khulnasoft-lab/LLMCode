

<!DOCTYPE html>

<html lang="en-US">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge">

  <link rel="stylesheet" href="/assets/css/just-the-docs-default.css">

  <link rel="stylesheet" href="/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet">

  <style id="jtd-nav-activation">
  
.site-nav ul li a {
  background-image: none;
}

  </style>

  

  
    <script src="/assets/js/vendor/lunr.min.js"></script>
  

  <script src="/assets/js/just-the-docs.js"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1">

  



  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>GPT code editing benchmarks | llmcode</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="GPT code editing benchmarks" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Benchmarking GPT-3.5 and GPT-4 code editing skill using a new code editing benchmark suite based on the Exercism python exercises." />
<meta property="og:description" content="Benchmarking GPT-3.5 and GPT-4 code editing skill using a new code editing benchmark suite based on the Exercism python exercises." />
<link rel="canonical" href="http://0.0.0.0:4000/2023/07/02/benchmarks.html" />
<meta property="og:url" content="http://0.0.0.0:4000/2023/07/02/benchmarks.html" />
<meta property="og:site_name" content="llmcode" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-07-02T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="GPT code editing benchmarks" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-07-02T00:00:00+00:00","datePublished":"2023-07-02T00:00:00+00:00","description":"Benchmarking GPT-3.5 and GPT-4 code editing skill using a new code editing benchmark suite based on the Exercism python exercises.","headline":"GPT code editing benchmarks","mainEntityOfPage":{"@type":"WebPage","@id":"http://0.0.0.0:4000/2023/07/02/benchmarks.html"},"url":"http://0.0.0.0:4000/2023/07/02/benchmarks.html"}</script>
<!-- End Jekyll SEO tag -->


  
<meta property="og:image" content="http://0.0.0.0:4000/assets/benchmarks.jpg">
<meta property="twitter:image" content="http://0.0.0.0:4000/assets/benchmarks.jpg">

<link rel="alternate" type="application/rss+xml" title="RSS Feed" href="http://0.0.0.0:4000/feed.xml">
<link rel="preconnect" href="https://fonts.gstatic.com">
<link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style" type="text/css" crossorigin>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="theme-color" content="#157878">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/favicon-16x16.png">
<link rel="apple-touch-icon" sizes="180x180" href="/assets/icons/apple-touch-icon.png">
<link rel="manifest" href="/assets/icons/site.webmanifest">
<link rel="mask-icon" href="/assets/icons/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">




</head>

<body>
  <a class="skip-to-main" href="#main-content">Skip to main content</a>
  <svg xmlns="http://www.w3.org/2000/svg" class="d-none">
  <symbol id="svg-link" viewBox="0 0 24 24">
  <title>Link</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link">
    <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path>
  </svg>
</symbol>

  <symbol id="svg-menu" viewBox="0 0 24 24">
  <title>Menu</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu">
    <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line>
  </svg>
</symbol>

  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
  <title>Expand</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right">
    <polyline points="9 18 15 12 9 6"></polyline>
  </svg>
</symbol>

  <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE -->
<symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link">
  <title id="svg-external-link-title">(external link)</title>
  <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line>
</symbol>

  
    <symbol id="svg-doc" viewBox="0 0 24 24">
  <title>Document</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file">
    <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline>
  </svg>
</symbol>

    <symbol id="svg-search" viewBox="0 0 24 24">
  <title>Search</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search">
    <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line>
  </svg>
</symbol>

  
  
    <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md -->
<symbol id="svg-copy" viewBox="0 0 16 16">
  <title>Copy</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16">
    <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/>
    <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/>
  </svg>
</symbol>
<symbol id="svg-copied" viewBox="0 0 16 16">
  <title>Copied</title>
  <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16">
    <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/>
    <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/>
  </svg>
</symbol>

  
</svg>

  <div class="side-bar">
  <div class="site-header" role="banner">
    <a href="/" class="site-title lh-tight">
  llmcode

</a>
    <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false">
      <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg>
    </button>
  </div>

  <nav aria-label="Main" id="site-nav" class="site-nav">
  
  
    <ul class="nav-list"><li class="nav-list-item"><a href="/" class="nav-list-link">Home</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Installation category" aria-pressed="false">
      <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg>
    </button><a href="/docs/install.html" class="nav-list-link">Installation</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/install/optional.html" class="nav-list-link">Optional steps</a></li><li class="nav-list-item"><a href="/docs/install/docker.html" class="nav-list-link">Llmcode with docker</a></li><li class="nav-list-item"><a href="/docs/install/codespaces.html" class="nav-list-link">GitHub Codespaces</a></li><li class="nav-list-item"><a href="/docs/install/replit.html" class="nav-list-link">Replit</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Usage category" aria-pressed="false">
      <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg>
    </button><a href="/docs/usage.html" class="nav-list-link">Usage</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/usage/tips.html" class="nav-list-link">Tips</a></li><li class="nav-list-item"><a href="/docs/usage/commands.html" class="nav-list-link">In-chat commands</a></li><li class="nav-list-item"><a href="/docs/usage/modes.html" class="nav-list-link">Chat modes</a></li><li class="nav-list-item"><a href="/docs/usage/tutorials.html" class="nav-list-link">Tutorial videos</a></li><li class="nav-list-item"><a href="/docs/usage/voice.html" class="nav-list-link">Voice-to-code with llmcode</a></li><li class="nav-list-item"><a href="/docs/usage/images-urls.html" class="nav-list-link">Images &amp; web pages</a></li><li class="nav-list-item"><a href="/docs/usage/caching.html" class="nav-list-link">Prompt caching</a></li><li class="nav-list-item"><a href="/docs/usage/watch.html" class="nav-list-link">Llmcode in your IDE</a></li><li class="nav-list-item"><a href="/docs/usage/browser.html" class="nav-list-link">Llmcode in your browser</a></li><li class="nav-list-item"><a href="/docs/usage/conventions.html" class="nav-list-link">Specifying coding conventions</a></li><li class="nav-list-item"><a href="/docs/usage/copypaste.html" class="nav-list-link">Copy/paste with web chat</a></li><li class="nav-list-item"><a href="/docs/usage/lint-test.html" class="nav-list-link">Linting and testing</a></li><li class="nav-list-item"><a href="/docs/usage/not-code.html" class="nav-list-link">Editing config &amp; text files</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Connecting to LLMs category" aria-pressed="false">
      <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg>
    </button><a href="/docs/llms.html" class="nav-list-link">Connecting to LLMs</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/llms/openai.html" class="nav-list-link">OpenAI</a></li><li class="nav-list-item"><a href="/docs/llms/anthropic.html" class="nav-list-link">Anthropic</a></li><li class="nav-list-item"><a href="/docs/llms/gemini.html" class="nav-list-link">Gemini</a></li><li class="nav-list-item"><a href="/docs/llms/groq.html" class="nav-list-link">GROQ</a></li><li class="nav-list-item"><a href="/docs/llms/lm-studio.html" class="nav-list-link">LM Studio</a></li><li class="nav-list-item"><a href="/docs/llms/xai.html" class="nav-list-link">xAI</a></li><li class="nav-list-item"><a href="/docs/llms/azure.html" class="nav-list-link">Azure</a></li><li class="nav-list-item"><a href="/docs/llms/cohere.html" class="nav-list-link">Cohere</a></li><li class="nav-list-item"><a href="/docs/llms/deepseek.html" class="nav-list-link">DeepSeek</a></li><li class="nav-list-item"><a href="/docs/llms/ollama.html" class="nav-list-link">Ollama</a></li><li class="nav-list-item"><a href="/docs/llms/openai-compat.html" class="nav-list-link">OpenAI compatible APIs</a></li><li class="nav-list-item"><a href="/docs/llms/openrouter.html" class="nav-list-link">OpenRouter</a></li><li class="nav-list-item"><a href="/docs/llms/vertex.html" class="nav-list-link">Vertex AI</a></li><li class="nav-list-item"><a href="/docs/llms/bedrock.html" class="nav-list-link">Amazon Bedrock</a></li><li class="nav-list-item"><a href="/docs/llms/other.html" class="nav-list-link">Other LLMs</a></li><li class="nav-list-item"><a href="/docs/llms/warnings.html" class="nav-list-link">Model warnings</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Configuration category" aria-pressed="false">
      <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg>
    </button><a href="/docs/config.html" class="nav-list-link">Configuration</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/config/api-keys.html" class="nav-list-link">API Keys</a></li><li class="nav-list-item"><a href="/docs/config/options.html" class="nav-list-link">Options reference</a></li><li class="nav-list-item"><a href="/docs/config/llmcode_conf.html" class="nav-list-link">YAML config file</a></li><li class="nav-list-item"><a href="/docs/config/dotenv.html" class="nav-list-link">Config with .env</a></li><li class="nav-list-item"><a href="/docs/config/editor.html" class="nav-list-link">Editor configuration</a></li><li class="nav-list-item"><a href="/docs/config/reasoning.html" class="nav-list-link">Reasoning models</a></li><li class="nav-list-item"><a href="/docs/config/adv-model-settings.html" class="nav-list-link">Advanced model settings</a></li><li class="nav-list-item"><a href="/docs/config/model-aliases.html" class="nav-list-link">Model Aliases</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Troubleshooting category" aria-pressed="false">
      <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg>
    </button><a href="/docs/troubleshooting.html" class="nav-list-link">Troubleshooting</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/troubleshooting/edit-errors.html" class="nav-list-link">File editing problems</a></li><li class="nav-list-item"><a href="/docs/troubleshooting/warnings.html" class="nav-list-link">Model warnings</a></li><li class="nav-list-item"><a href="/docs/troubleshooting/token-limits.html" class="nav-list-link">Token limits</a></li><li class="nav-list-item"><a href="/docs/troubleshooting/imports.html" class="nav-list-link">Dependency versions</a></li><li class="nav-list-item"><a href="/docs/troubleshooting/llmcode-not-found.html" class="nav-list-link">Llmcode not found</a></li><li class="nav-list-item"><a href="/docs/troubleshooting/models-and-keys.html" class="nav-list-link">Models and API keys</a></li><li class="nav-list-item"><a href="/docs/troubleshooting/support.html" class="nav-list-link">Using /help</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Example chat transcripts category" aria-pressed="false">
      <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg>
    </button><a href="/examples/README.html" class="nav-list-link">Example chat transcripts</a><ul class="nav-list"><li class="nav-list-item"><a href="/examples/hello-world-flask.html" class="nav-list-link">Create a simple flask app with llmcode</a></li><li class="nav-list-item"><a href="/examples/2048-game.html" class="nav-list-link">Modify an open source 2048 game with llmcode</a></li><li class="nav-list-item"><a href="/examples/complex-change.html" class="nav-list-link">A complex multi-file change, with debugging</a></li><li class="nav-list-item"><a href="/examples/add-test.html" class="nav-list-link">Create a “black box” test case</a></li><li class="nav-list-item"><a href="/examples/update-docs.html" class="nav-list-link">Automatically update docs with llmcode</a></li><li class="nav-list-item"><a href="/examples/pong.html" class="nav-list-link">Build pong with llmcode and pygame.</a></li><li class="nav-list-item"><a href="/examples/css-exercises.html" class="nav-list-link">Complete a css exercise with llmcode</a></li><li class="nav-list-item"><a href="/examples/census.html" class="nav-list-link">Download, analyze and plot US Census data</a></li><li class="nav-list-item"><a href="/examples/asciinema.html" class="nav-list-link">Editing an asciinema cast file with llmcode</a></li><li class="nav-list-item"><a href="/examples/hello.html" class="nav-list-link">Hello llmcode!</a></li><li class="nav-list-item"><a href="/examples/no-color.html" class="nav-list-link">Honor the NO_COLOR environment variable</a></li><li class="nav-list-item"><a href="/examples/chat-transcript-css.html" class="nav-list-link">Improve css styling of chat transcripts</a></li><li class="nav-list-item"><a href="/examples/semantic-search-replace.html" class="nav-list-link">Semantic search &amp; replace code with llmcode</a></li></ul></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in More info category" aria-pressed="false">
      <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg>
    </button><a href="/docs/more-info.html" class="nav-list-link">More info</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/git.html" class="nav-list-link">Git integration</a></li><li class="nav-list-item"><a href="/docs/languages.html" class="nav-list-link">Supported languages</a></li><li class="nav-list-item"><a href="/docs/repomap.html" class="nav-list-link">Repository map</a></li><li class="nav-list-item"><a href="/docs/scripting.html" class="nav-list-link">Scripting llmcode</a></li><li class="nav-list-item"><a href="/docs/more/infinite-output.html" class="nav-list-link">Infinite output</a></li><li class="nav-list-item"><a href="/docs/more/edit-formats.html" class="nav-list-link">Edit formats</a></li><li class="nav-list-item"><a href="/docs/more/analytics.html" class="nav-list-link">Analytics</a></li><li class="nav-list-item"><a href="/docs/legal/privacy.html" class="nav-list-link">Privacy policy</a></li></ul></li><li class="nav-list-item"><a href="/docs/faq.html" class="nav-list-link">FAQ</a></li><li class="nav-list-item"><a href="/HISTORY.html" class="nav-list-link">Release history</a></li><li class="nav-list-item"><button class="nav-list-expander btn-reset" aria-label="toggle items in Llmcode LLM Leaderboards category" aria-pressed="false">
      <svg viewBox="0 0 24 24" aria-hidden="true"><use xlink:href="#svg-arrow-right"></use></svg>
    </button><a href="/docs/leaderboards/" class="nav-list-link">Llmcode LLM Leaderboards</a><ul class="nav-list"><li class="nav-list-item"><a href="/docs/leaderboards/edit.html" class="nav-list-link">Code editing leaderboard</a></li><li class="nav-list-item"><a href="/docs/leaderboards/refactor.html" class="nav-list-link">Refactoring leaderboard</a></li><li class="nav-list-item"><a href="/docs/leaderboards/by-release-date.html" class="nav-list-link">Scores by release date</a></li><li class="nav-list-item"><a href="/docs/leaderboards/notes.html" class="nav-list-link">Benchmark notes</a></li><li class="nav-list-item"><a href="/docs/leaderboards/contrib.html" class="nav-list-link">Contributing results</a></li></ul></li><li class="nav-list-item"><a href="/blog/" class="nav-list-link">Llmcode blog</a></li></ul>

  <ul class="nav-list"><li class="nav-list-item external">
          <a href="https://github.com/KhulnaSoft/llmcode" class="nav-list-link external"
            
          >
            GitHub
            <svg viewBox="0 0 24 24" aria-labelledby="svg-external-link-title"><use xlink:href="#svg-external-link"></use></svg>
          </a>
        </li><li class="nav-list-item external">
          <a href="https://discord.gg/Tv2uQnR88V" class="nav-list-link external"
            
          >
            Discord
            <svg viewBox="0 0 24 24" aria-labelledby="svg-external-link-title"><use xlink:href="#svg-external-link"></use></svg>
          </a>
        </li></ul>
</nav>


  
  
    <footer class="site-footer">
    Llmcode is AI pair programming in your terminal.
    Llmcode is on
    <a href="https://github.com/KhulnaSoft/llmcode">GitHub</a>
    and
    <a href="https://discord.gg/Tv2uQnR88V">Discord</a>.
</footer>

  
</div>

  <div class="main" id="top">
    <div id="main-header" class="main-header">
  
    

<div class="search" role="search">
  <div class="search-input-wrap">
    <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search llmcode" aria-label="Search llmcode" autocomplete="off">
    <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label>
  </div>
  <div id="search-results" class="search-results"></div>
</div>

  
  
  
    <nav aria-label="Auxiliary" class="aux-nav">
  <ul class="aux-nav-list">
    
      <li class="aux-nav-list-item">
        <a href="https://github.com/KhulnaSoft/llmcode" class="site-button"
          
        >
          GitHub
        </a>
      </li>
    
      <li class="aux-nav-list-item">
        <a href="https://discord.gg/Tv2uQnR88V" class="site-button"
          
        >
          Discord
        </a>
      </li>
    
      <li class="aux-nav-list-item">
        <a href="/blog/" class="site-button"
          
        >
          Blog
        </a>
      </li>
    
  </ul>
</nav>

  
</div>

    <div class="main-content-wrap">
      
      <div id="main-content" class="main-content">
        <main>
          
            <p class="post-date">July 02, 2023</p>
<h1 id="gpt-code-editing-benchmarks">
  
  
    <a href="#gpt-code-editing-benchmarks" class="anchor-heading" aria-labelledby="gpt-code-editing-benchmarks"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> GPT code editing benchmarks
  
  
</h1>
    

<p><a href="https://llmcode.khulnasoft.com/assets/benchmarks.svg"><img src="/assets/benchmarks.svg" alt="benchmark results" /></a></p>

<p>Llmcode is an open source command line chat tool that lets you work with GPT to edit
code in your local git repo.
To do this, llmcode needs to be able to reliably recognize when GPT wants to edit local files,
determine which files it wants to modify and what changes to save.
Such automated
code editing hinges on using the system prompt
to tell GPT how to structure code edits in its responses.</p>

<p>Llmcode currently asks GPT to use simple text based “edit formats”, but
<a href="https://openai.com/blog/function-calling-and-other-api-updates">OpenAI’s new function calling
API</a>
looks like a promising way to create more structured edit formats.
After implementing a couple of function based edit formats,
I wanted
to measure the potential benefits
of switching llmcode to use them by default.</p>

<p>With this in mind, I developed a
benchmark based on the <a href="https://github.com/exercism/python">Exercism
python</a> coding exercises.
This
benchmark evaluates how effectively llmcode and GPT can translate a
natural language coding request into executable code saved into
files that pass unit tests.
It provides an end-to-end evaluation of not just
GPT’s coding ability, but also its capacity to <em>edit existing code</em>
and <em>format those code edits</em> so that llmcode can save the
edits to the local source files.</p>

<p>I ran the benchmark
on all the ChatGPT models (except <code class="language-plaintext highlighter-rouge">gpt-4-32k</code>), using a variety of edit formats.
The results were interesting:</p>

<ul>
  <li><strong>Plain text edit formats worked best.</strong> Asking GPT to return an updated copy of the whole file in a standard markdown fenced code block proved to be the most reliable and effective edit format across all GPT-3.5 and GPT-4 models. The results for this <code class="language-plaintext highlighter-rouge">whole</code> edit format are shown in solid blue in the graph.</li>
  <li><strong>Function calls performed worse.</strong> Using the new functions API for edits performed worse than the above whole file method, for all the models. GPT-3.5 especially produced inferior code and frequently mangled this output format. This was surprising, as the functions API was introduced to enhance the reliability of structured outputs. The results for these <code class="language-plaintext highlighter-rouge">...-func</code> edit methods are shown as patterned bars in the graph (both green and blue).</li>
  <li><strong>The new June GPT-3.5 models did a bit worse than the old June model.</strong> The performance of the new June (<code class="language-plaintext highlighter-rouge">0613</code>) versions of GPT-3.5 appears to be a bit worse than the February (<code class="language-plaintext highlighter-rouge">0301</code>) version. This is visible if you look at the “first attempt” markers on the first three solid blue bars and also by comparing the first three solid green <code class="language-plaintext highlighter-rouge">diff</code> bars.</li>
  <li><strong>GPT-4 does better than GPT-3.5,</strong> as expected.</li>
</ul>

<p>The quantitative benchmark results agree with my intuitions
about prompting GPT for complex tasks like coding. It’s beneficial to
minimize the “cognitive overhead” of formatting the response, allowing
GPT to concentrate on the coding task at hand.</p>

<p>As a thought experiment, imagine a slack conversation with a editor developer where
you ask them to write the code to add some new feature to your app.
They’re going to type the response back to you by hand in the chat.
Should they type out the
code and wrap it in a normal markdown code block?
Or should they type up a properly escaped and
syntactically correct json data structure
that contains the text of the new code?</p>

<p>Using more complex output formats with GPT seems to cause two issues:</p>

<ul>
  <li>It makes GPT write worse code. Keeping the output format simple seems to allow GPT to devote more attention to the actual coding task.</li>
  <li>It reduces GPT’s adherence to the output format, making it more challenging for tools like llmcode to accurately identify and apply the edits GPT is attempting to make.</li>
</ul>

<p>I was expecting to start using function call based edits in llmcode for both GPT-3.5 and GPT-4.
But given these benchmark results, I won’t be adopting the functions API
at this time.
I will certainly plan to benchmark functions again with future versions of the models.</p>

<p>More details on the benchmark, edit formats and results are discussed below.</p>
<h2 id="the-benchmark">
  
  
    <a href="#the-benchmark" class="anchor-heading" aria-labelledby="the-benchmark"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The benchmark
  
  
</h2>
    

<p>The benchmark uses
<a href="https://github.com/exercism/python/tree/main/exercises/practice">133 practice exercises from the Exercism python repository</a>.
These
exercises were designed to help individuals learn Python and hone
their coding skills.</p>

<p>Each exercise includes:</p>

<ul>
  <li><a href="https://github.com/exercism/python/blob/main/exercises/practice/anagram/.docs/instructions.md">Instructions</a>, provided in markdown files.</li>
  <li><a href="https://github.com/exercism/python/blob/main/exercises/practice/anagram/anagram.py">Stub python code</a> in an <em>implementation file</em>, specifying the functions or classes that need to be implemented.</li>
  <li><a href="https://github.com/exercism/python/blob/main/exercises/practice/anagram/anagram_test.py">Unit tests</a> in a separate python file.</li>
</ul>

<p>The goal is for GPT to read the instructions, implement the provided function/class skeletons
and pass all the unit tests. The benchmark measures what percentage of
the 133 exercises are completed successfully, causing all the associated unit tests to pass.</p>

<p>To start each exercise, llmcode sends GPT
the initial contents of the implementation file,
the Exercism instructions
and a final instruction:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Use the above instructions to modify the supplied files: &lt;implementation file&gt;
Keep and implement the existing function or class stubs, they will be called from unit tests.
Only use standard python libraries, don't suggest installing any packages.
</code></pre></div></div>

<p>Llmcode updates the implementation file based on GPT’s reply and runs
the unit tests. If all tests pass, the exercise is considered
complete. If some tests fail, llmcode sends GPT a second message with
the test error output. It only sends the first 50 lines of test errors
to try and avoid exceeding the context window of the smaller models. Llmcode
also includes this final instruction:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>See the testing errors above.
The tests are correct.
Fix the code in &lt;implementation file&gt; to resolve the errors.
</code></pre></div></div>

<p>Requiring GPT to fix its first implementation in response to test failures
is another way in which this benchmark stresses code editing skill.
This second chance is also important because it
gives GPT the opportunity to adjust if the
instructions were imprecise with respect to the
specific requirements of the unit tests.
Many of the exercises have multiple paragraphs of instructions,
and most human coders would likely fail some tests on their
first try.</p>

<p>The bars in the graph show the percent of exercises that were completed by
each model and edit format combination. The full bar height represents
the final outcome following both coding attempts.
Each bar also has a horizontal mark that shows
the intermediate performance after the first coding attempt,
without the benefit of the second try that includes the test error output.</p>

<p>It’s worth noting that GPT never gets to see the source code of the
unit tests during the benchmark. It only sees the error output from
failed tests. Of course, all of this code was probably part of its
original training data!</p>

<p>In summary, passing an exercise means GPT was able to:</p>

<ul>
  <li>Write the required code (possibly after reviewing test error output),</li>
  <li>Correctly package all of the code edits into the edit format so that llmcode can process and save it to the implementation file.</li>
</ul>

<p>Conversely, failing an exercise only requires a breakdown in one of
those steps. In practice, GPT fails at different steps in different
exercises. Sometimes it simply writes the wrong code. Other times, it
fails to format the code edits in a way that conforms to the edit
format, resulting in the code not being saved correctly.</p>

<p>It’s worth keeping in mind that changing the edit format often affects
both aspects of GPT’s performance.
Complex edit formats often lead GPT to write worse code <em>and</em> make it less
successful at formatting the edits correctly.</p>
<h2 id="edit-formats">
  
  
    <a href="#edit-formats" class="anchor-heading" aria-labelledby="edit-formats"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Edit formats
  
  
</h2>
    

<p>I benchmarked 4 different edit formats, described below.
Each description includes a sample response that GPT might provide to a user who
requests:
“Change the print from hello to goodbye.”</p>
<h3 id="whole">
  
  
    <a href="#whole" class="anchor-heading" aria-labelledby="whole"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> whole
  
  
</h3>
    

<p>The
<a href="https://github.com/KhulnaSoft/llmcode/blob/main/llmcode/prompts/wholefile_prompts.py">whole</a>
format asks GPT to return an updated copy of the entire file, including any changes.
The file should be
formatted with normal markdown triple-backtick fences, inlined with the rest of its response text.</p>

<p>This format is very similar to how ChatGPT returns code snippets during normal chats, except with the addition of a filename right before the opening triple-backticks.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Here is the updated copy of your file demo.py:

demo.py
```python
def main():
    print("goodbye")
```
</code></pre></div></div>
<h3 id="diff">
  
  
    <a href="#diff" class="anchor-heading" aria-labelledby="diff"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> diff
  
  
</h3>
    

<p>The <a href="https://github.com/KhulnaSoft/llmcode/blob/main/llmcode/prompts/editblock_prompts.py">diff</a>
format also asks GPT to return edits as part of the normal response text,
in a simple diff format.
Each edit is a fenced code block that
specifies the filename and a chunk of ORIGINAL and UPDATED code.
GPT provides some original lines from the file and then a new updated set of lines.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Here are the changes you requested to demo.py:

```python
demo.py
&lt;&lt;&lt;&lt;&lt;&lt;&lt; ORIGINAL
    print("hello")
=======
    print("goodbye")
&gt;&gt;&gt;&gt;&gt;&gt;&gt; UPDATED
```
</code></pre></div></div>
<h3 id="whole-func">
  
  
    <a href="#whole-func" class="anchor-heading" aria-labelledby="whole-func"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> whole-func
  
  
</h3>
    

<p>The <a href="https://github.com/KhulnaSoft/llmcode/blob/main/llmcode/coders/wholefile_func_coder.py">whole-func</a>
format requests updated copies of whole files to be returned using the function call API.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
    "explanation": "Changed hello to goodbye.",
    "files": [
        {
            "path": "demo.py",
            "content": "def main():\n    print(\"goodbye\")\n"
        }
}
</code></pre></div></div>
<h3 id="diff-func">
  
  
    <a href="#diff-func" class="anchor-heading" aria-labelledby="diff-func"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> diff-func
  
  
</h3>
    

<p>The
<a href="https://github.com/KhulnaSoft/llmcode/blob/main/llmcode/coders/editblock_func_coder.py">diff-func</a>
format requests a list of
original/updated style edits to be returned using the function call API.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
    "explanation": "Changed hello to goodbye.",
    "edits": [
        {
            "path": "demo.py",
            "original_lines": [
                "    print(\"hello\")"
            ],
            "updated_lines": [
                "    print(\"goodbye\")"
            ],
        }
    ]
}
</code></pre></div></div>
<h2 id="gpt-35s-performance">
  
  
    <a href="#gpt-35s-performance" class="anchor-heading" aria-labelledby="gpt-35s-performance"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> GPT-3.5’s performance
  
  
</h2>
    
<h3 id="the-0613-models-seem-worse">
  
  
    <a href="#the-0613-models-seem-worse" class="anchor-heading" aria-labelledby="the-0613-models-seem-worse"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The <code class="language-plaintext highlighter-rouge">0613</code> models seem worse?
  
  
</h3>
    

<p>The GPT-3.5 benchmark results have me fairly convinced that the new
<code class="language-plaintext highlighter-rouge">gpt-3.5-turbo-0613</code> and <code class="language-plaintext highlighter-rouge">gpt-3.5-16k-0613</code> models
are a bit worse at code editing than
the older <code class="language-plaintext highlighter-rouge">gpt-3.5-turbo-0301</code> model.</p>

<p>This is visible in the “first attempt”
portion of each result, before GPT gets a second chance to edit the code.
Look at the horizontal white line in the middle of the first three blue bars.
Performance with the <code class="language-plaintext highlighter-rouge">whole</code> edit format was 46% for the
February model and only 39% for the June models.</p>

<p>But also note how much the solid green <code class="language-plaintext highlighter-rouge">diff</code> bars
degrade between the February and June GPT-3.5 models.
They drop from 30% down to about 19%.</p>

<p>I saw other signs of this degraded performance
in earlier versions of the
benchmark as well.</p>
<h3 id="pathological-use-of-diff">
  
  
    <a href="#pathological-use-of-diff" class="anchor-heading" aria-labelledby="pathological-use-of-diff"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Pathological use of <code class="language-plaintext highlighter-rouge">diff</code>
  
  
</h3>
    

<p>When GPT-3.5 is able to correctly generate the <code class="language-plaintext highlighter-rouge">diff</code> edit format,
it often uses it in a pathological manner. It places the <em>entire</em>
original source file in the ORIGINAL block and the entire updated file
in the UPDATED block. This is strictly worse than just using the
<code class="language-plaintext highlighter-rouge">whole</code> edit format, as GPT is sending two full copies of the file.</p>
<h3 id="hallucinated-function-calls">
  
  
    <a href="#hallucinated-function-calls" class="anchor-heading" aria-labelledby="hallucinated-function-calls"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Hallucinated function calls
  
  
</h3>
    

<p>When GPT-3.5 uses the functions API
it is prone to ignoring the JSON Schema that specifies valid functions.
It often returns a completely novel and semantically
invalid <code class="language-plaintext highlighter-rouge">function_call</code> fragment with <code class="language-plaintext highlighter-rouge">"name": "python"</code>.</p>

<p>The <code class="language-plaintext highlighter-rouge">arguments</code> attribute is supposed to be a set of key/value pairs
with the arguments to the function specified in the <code class="language-plaintext highlighter-rouge">name</code> field.
Instead, GPT-3.5 frequently just stuffs an entire python
file into that field.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>        "function_call": {
          "name": "python",
          "arguments": "def main():\n    print(\"hello\")\n"
        },
</code></pre></div></div>

<p>It seems like it might be getting confused by fine-tuning that was
done for the ChatGPT code interpreter plugin?</p>
<h2 id="randomness">
  
  
    <a href="#randomness" class="anchor-heading" aria-labelledby="randomness"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Randomness
  
  
</h2>
    

<p>The benchmark attempts to be deterministic, always sending identical
requests for each exercise on repeated runs.
As part of this effort,
when sending test error output to GPT,
it removes the wall-clock timing information that
is normally included by the <code class="language-plaintext highlighter-rouge">unittest</code> module.</p>

<p>The benchmark harness also logs SHA hashes of
all the OpenAI API requests and replies.
This makes it possible to
detect randomness or nondeterminism
in the benchmarking process.</p>

<p>It turns out that the OpenAI chat APIs are not deterministic, even at
<code class="language-plaintext highlighter-rouge">temperature=0</code>.  The same identical request will produce multiple
distinct responses, usually less than 5-10 variations.  This suggests
that OpenAI may be load balancing their API across a number of
slightly different instances of the model?</p>

<p>For certain exercises, some of these variable responses pass the unit tests while
other variants do not. Results for exercises like this, which are
“on the bubble”,
are therefore a bit random, depending on which variant OpenAI returns.</p>

<p>Given that, it would be ideal to run all 133 exercises many times for each
model/edit-format combination and report an average performance.
This would average away the effect of the API variance.
It would also significantly increase the cost of this sort of benchmarking.
So I didn’t do that.</p>

<p>Benchmarking against 133 exercises already provides some robustness, since
we are measuring the performance across many exercises.</p>

<p>But to get a sense of how much the API variance impacts the benchmark outcomes,
I ran all 133 exercises 10 times each
against <code class="language-plaintext highlighter-rouge">gpt-3.5-turbo-0613</code> with the <code class="language-plaintext highlighter-rouge">whole</code> edit format.
You’ll see one set of error bars in the graph, which show
the range of results from those 10 runs.</p>

<p>The OpenAI API randomness doesn’t seem to
cause a large variance in the overall benchmark results.</p>
<h2 id="conclusions">
  
  
    <a href="#conclusions" class="anchor-heading" aria-labelledby="conclusions"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Conclusions
  
  
</h2>
    

<p>Based on these benchmark results, llmcode will continue to use
the <code class="language-plaintext highlighter-rouge">whole</code> edit format for GPT-3.5, and <code class="language-plaintext highlighter-rouge">diff</code> for GPT-4.</p>

<p>GPT-4 gets comparable results with the <code class="language-plaintext highlighter-rouge">whole</code> and <code class="language-plaintext highlighter-rouge">diff</code> edit formats,
but using <code class="language-plaintext highlighter-rouge">whole</code> significantly increases costs and latency compared to <code class="language-plaintext highlighter-rouge">diff</code>.</p>

<p>The latency of streaming back the entire updated copy of each edited file
is a real challenge with the <code class="language-plaintext highlighter-rouge">whole</code> format.
The GPT-3.5 models are quite responsive, and can
stream back entire files at reasonable speed.
Llmcode displays a progress bar and
live diffs of the files as they stream in,
which helps pass the time.</p>

<p>The GPT-4 models are much slower, and waiting for even small files
to be completely “retyped” on each request is probably unacceptable.</p>

          

          
        </main>
        


      </div>
    </div>
    
      

<div class="search-overlay"></div>

    
  </div>

  
</body>
</html>

