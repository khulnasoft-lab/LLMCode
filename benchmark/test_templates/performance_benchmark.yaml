name: performance_benchmark
description: Template for performance benchmarking tests
framework: pytest
language: python
template_code: |
    import pytest
    import time
    import sys
    import os
    import psutil
    import gc
    from unittest.mock import Mock, patch
    
    # Add project root to path
    sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    
    from {file_name} import {function_or_class_name}
    
    @pytest.fixture
    def large_dataset():
        """Create large dataset for performance testing"""
        return {{large_dataset_generator}}
    
    @pytest.fixture
    def benchmark_setup():
        """Setup for benchmark tests"""
        # Clear memory before test
        gc.collect()
        initial_memory = psutil.Process().memory_info().rss
        
        yield
        
        # Check memory after test
        final_memory = psutil.Process().memory_info().rss
        memory_increase = final_memory - initial_memory
        
        # Assert memory increase is reasonable
        assert memory_increase < {{max_memory_increase}}, f"Memory increase too large: {memory_increase} bytes"
    
    def test_performance_basic(benchmark, benchmark_setup):
        """Test basic performance of {function_or_class_name}"""
        def benchmark_function():
            return {function_or_class_name}({{benchmark_params}})
        
        # Run benchmark
        result = benchmark(benchmark_function)
        
        # Verify result is correct
        assert result == {{expected_result}}
    
    def test_performance_large_dataset(benchmark, benchmark_setup, large_dataset):
        """Test performance with large dataset"""
        def benchmark_function():
            return {function_or_class_name}(large_dataset)
        
        # Run benchmark
        result = benchmark(benchmark_function)
        
        # Verify result is correct
        assert len(result) == {{expected_result_length}}
    
    def test_performance_memory_usage(benchmark_setup):
        """Test memory usage of {function_or_class_name}"""
        # Get initial memory
        process = psutil.Process()
        initial_memory = process.memory_info().rss
        
        # Run function multiple times
        for _ in range({{iterations}}):
            result = {function_or_class_name}({{memory_test_params}})
        
        # Get final memory
        final_memory = process.memory_info().rss
        memory_increase = final_memory - initial_memory
        
        # Assert memory usage is reasonable
        assert memory_increase < {{max_memory_increase}}, f"Memory increase too large: {memory_increase} bytes"
    
    def test_performance_concurrent_access():
        """Test concurrent access performance"""
        import threading
        import queue
        
        results = queue.Queue()
        errors = queue.Queue()
        
        def worker():
            try:
                result = {function_or_class_name}({{concurrent_params}})
                results.put(result)
            except Exception as e:
                errors.put(e)
        
        # Create and start threads
        threads = []
        for _ in range({{num_threads}}):
            thread = threading.Thread(target=worker)
            threads.append(thread)
            thread.start()
        
        # Wait for threads to complete
        for thread in threads:
            thread.join()
        
        # Check for errors
        assert errors.empty(), f"Concurrent access errors: {[errors.get() for _ in range(errors.qsize())]}"
        
        # Verify results
        assert results.qsize() == {{num_threads}}
    
    def test_performance_scalability():
        """Test performance scalability with different input sizes"""
        input_sizes = [{{small_size}}, {{medium_size}}, {{large_size}}]
        execution_times = []
        
        for size in input_sizes:
            # Create input of specified size
            test_input = {{scalable_input_generator}}.format(size=size)
            
            # Measure execution time
            start_time = time.time()
            result = {function_or_class_name}(test_input)
            end_time = time.time()
            
            execution_time = end_time - start_time
            execution_times.append(execution_time)
            
            # Verify result
            assert len(result) == size
        
        # Check that execution time scales reasonably
        # (should not be exponential growth)
        for i in range(1, len(execution_times)):
            ratio = execution_times[i] / execution_times[i-1]
            size_ratio = input_sizes[i] / input_sizes[i-1]
            
            # Time should scale linearly or better
            assert ratio <= size_ratio * {{max_time_ratio_factor}}, f"Performance scales poorly: {ratio} vs {size_ratio}"
    
    def test_performance_cold_start():
        """Test cold start performance"""
        # Clear any caches
        if hasattr({function_or_class_name}, 'clear_cache'):
            {function_or_class_name}.clear_cache()
        
        # Measure cold start time
        start_time = time.time()
        result = {function_or_class_name}({{cold_start_params}})
        end_time = time.time()
        
        cold_start_time = end_time - start_time
        
        # Measure warm start time
        start_time = time.time()
        result = {function_or_class_name}({{cold_start_params}})
        end_time = time.time()
        
        warm_start_time = end_time - start_time
        
        # Verify warm start is faster
        assert warm_start_time < cold_start_time, "Warm start should be faster than cold start"
        
        # Verify cold start is within acceptable limits
        assert cold_start_time < {{max_cold_start_time}}, f"Cold start time too long: {cold_start_time}s"
    
    def test_performance_under_load():
        """Test performance under heavy load"""
        import concurrent.futures
        
        def load_test_worker():
            start_time = time.time()
            result = {function_or_class_name}({{load_test_params}})
            end_time = time.time()
            return end_time - start_time, result
        
        # Run concurrent load tests
        execution_times = []
        with concurrent.futures.ThreadPoolExecutor(max_workers={{max_workers}}) as executor:
            futures = [executor.submit(load_test_worker) for _ in range({{num_load_tests}})]
            
            for future in concurrent.futures.as_completed(futures):
                exec_time, result = future.result()
                execution_times.append(exec_time)
                
                # Verify result
                assert result == {{expected_load_result}}
        
        # Calculate statistics
        avg_time = sum(execution_times) / len(execution_times)
        max_time = max(execution_times)
        min_time = min(execution_times)
        
        # Assert performance under load
        assert avg_time < {{max_avg_time}}, f"Average time under load too high: {avg_time}s"
        assert max_time < {{max_single_time}}, f"Maximum time under load too high: {max_time}s"
        
        # Print performance stats
        print(f"Load test stats - Avg: {avg_time:.3f}s, Min: {min_time:.3f}s, Max: {max_time:.3f}s")
variables:
  - function_or_class_name
  - large_dataset_generator
  - max_memory_increase
  - benchmark_params
  - expected_result
  - expected_result_length
  - memory_test_params
  - iterations
  - concurrent_params
  - num_threads
  - small_size
  - medium_size
  - large_size
  - scalable_input_generator
  - max_time_ratio_factor
  - cold_start_params
  - max_cold_start_time
  - load_test_params
  - max_workers
  - num_load_tests
  - expected_load_result
  - max_avg_time
  - max_single_time
dependencies:
  - pytest
  - pytest-benchmark
  - psutil
  - gc
  - threading
  - queue
  - concurrent.futures
tags:
  - performance
  - benchmark
  - memory
  - concurrency
  - scalability
